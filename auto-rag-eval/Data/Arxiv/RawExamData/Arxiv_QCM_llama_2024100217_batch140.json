{"0": {"documentation": {"title": "Opto-Acoustic Biosensing with Optomechanofluidic Resonators", "source": "Kaiyuan Zhu, Kewen Han, Tal Carmon, Xudong Fan and Gaurav Bahl", "docs_id": "1405.5282", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Opto-Acoustic Biosensing with Optomechanofluidic Resonators. Opto-mechano-fluidic resonators (OMFRs) are a unique optofluidics platform that can measure the acoustic properties of fluids and bioanalytes in a fully-contained microfluidic system. By confining light in ultra-high-Q whispering gallery modes of OMFRs, optical forces such as radiation pressure and electrostriction can be used to actuate and sense structural mechanical vibrations spanning MHz to GHz frequencies. These vibrations are hybrid fluid-shell modes that entrain any bioanalyte present inside. As a result, bioanalytes can now reflect their acoustic properties on the optomechanical vibrational spectrum of the device, in addition to optical property measurements with existing optofluidics techniques. In this work, we investigate acoustic sensing capabilities of OMFRs using computational eigenfrequency analysis. We analyze the OMFR eigenfrequency sensitivity to bulk fluid-phase materials as well as nanoparticles, and propose methods to extract multiple acoustic parameters from multiple vibrational modes. The new informational degrees-of-freedom provided by such opto-acoustic measurements could lead to surprising new sensor applications in the near future."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Opto-mechano-fluidic resonators (OMFRs) are a unique optofluidics platform that can measure the acoustic properties of fluids and bioanalytes in a fully-contained microfluidic system. What is the primary mechanism by which OMFRs can actuate and sense structural mechanical vibrations, and what are the advantages of using this mechanism?\n\n**A)** OMFRs use electrostriction to actuate and sense vibrations, which allows for precise control over the vibrational modes. This mechanism is advantageous because it enables the measurement of acoustic properties of fluids and bioanalytes with high sensitivity.\n\n**B)** OMFRs use radiation pressure to actuate and sense vibrations, which allows for the entrainment of bioanalytes and the measurement of their acoustic properties. This mechanism is advantageous because it enables the measurement of acoustic properties of fluids and bioanalytes in a fully-contained microfluidic system.\n\n**C)** OMFRs use a combination of electrostriction and radiation pressure to actuate and sense vibrations, which allows for the precise control over the vibrational modes and the measurement of acoustic properties of fluids and bioanalytes. This mechanism is advantageous because it enables the measurement of acoustic properties of fluids and bioanalytes with high sensitivity and precision.\n\n**D)** OMFRs use a piezoelectric material to actuate and sense vibrations, which allows for the measurement of acoustic properties of fluids and bioanalytes with high sensitivity. This mechanism is advantageous because it enables the measurement of acoustic properties of fluids and bioanalytes in a fully-contained microfluidic system.\n\n**Correct Answer:** B) OMFRs use radiation pressure to actuate and sense vibrations, which allows for the entrainment of bioanalytes and the measurement of their acoustic properties. This mechanism is advantageous because it enables the measurement of acoustic properties of fluids and bioanalytes in a fully-contained microfluidic system.\n\n**Explanation:** The correct answer is B) OMFRs use radiation pressure to actuate and sense vibrations. This is because the documentation states that OMFRs can confine light in ultra-high-Q whispering gallery modes, which allows for the use of optical forces such as radiation pressure to actuate and sense structural mechanical vibrations. Additionally, the documentation states that these vibrations are hybrid fluid-shell modes that entrain any bioanalyte present inside, which is a direct result of the use of radiation pressure."}, "1": {"documentation": {"title": "Electroweak phase transition and Higgs boson couplings in the model\n  based on supersymmetric strong dynamics", "source": "Shinya Kanemura, Eibun Senaha, Tetsuo Shindou, Toshifumi Yamada", "docs_id": "1211.5883", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electroweak phase transition and Higgs boson couplings in the model\n  based on supersymmetric strong dynamics. We discuss a strongly-coupled extended Higgs sector with the 126 GeV Higgs boson, which is a low-energy effective theory of the supersymmetric SU(2)$_H$ gauge thoery that causes confinement. In this effective theory, we study the parameter region where electroweak phase transition is of strongly first order, as required for successful electroweak baryogenesis. In such a parameter region, the model has a Landau pole at the order of 10 TeV, which corresponds to the confinement scale of the SU(2)$_H$ gauge theory. We find that the large coupling constant which blows up at the Landau pole results in large non-decoupling loop effects on low-energy observables, such as the Higgs-photon-photon vertex and the triple Higgs boson vertex. As phenomenological consequences of electroweak baryogenesis in our model, the Higgs-to-diphoton branching ratio is about 20% smaller while the triple Higgs boson coupling is more than about 20% larger than the standard model predictions. Such deviations may be detectable in future collider experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the model based on supersymmetric strong dynamics, what is the expected impact on the Higgs-to-diphoton branching ratio due to the strongly first-order electroweak phase transition, and how does it compare to the standard model predictions?\n\nA) The Higgs-to-diphoton branching ratio is expected to be 10% larger than the standard model predictions.\nB) The Higgs-to-diphoton branching ratio is expected to be 20% smaller than the standard model predictions.\nC) The Higgs-to-diphoton branching ratio is expected to be 5% larger than the standard model predictions.\nD) The Higgs-to-diphoton branching ratio is expected to be 30% smaller than the standard model predictions.\n\nCorrect Answer: B) The Higgs-to-diphoton branching ratio is expected to be 20% smaller than the standard model predictions.\n\nExplanation: According to the documentation, the model predicts that the Higgs-to-diphoton branching ratio is about 20% smaller than the standard model predictions due to the strongly first-order electroweak phase transition. This is a consequence of the large coupling constant that blows up at the Landau pole, resulting in large non-decoupling loop effects on low-energy observables."}, "2": {"documentation": {"title": "The excluded area of two-dimensional hard particles", "source": "Thomas Geigenfeind and Daniel de las Heras", "docs_id": "1902.05961", "section": ["cond-mat.soft", "physics.data-an", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The excluded area of two-dimensional hard particles. The excluded area between a pair of two-dimensional hard particles with given relative orientation is the region in which one particle cannot be located due to the presence of the other particle. The magnitude of the excluded area as a function of the relative particle orientation plays a major role in the determination of the bulk phase behaviour of hard particles. We use principal component analysis to identify the different types of excluded area corresponding to randomly generated two-dimensional hard particles modeled as non-self-intersecting polygons and star lines (line segments radiating from a common origin). Only three principal components are required to have an excellent representation of the value of the excluded area as a function of the relative particle orientation. Independently of the particle shape, the minimum value of the excluded area is always achieved when the particles are antiparallel to each other. The property that affects the value of the excluded area most strongly is the elongation of the particle shape. Principal component analysis identifies four limiting cases of excluded areas with one to four global minima at equispaced relative orientations. We study selected particle shapes using Monte Carlo simulations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary factor that determines the magnitude of the excluded area between two-dimensional hard particles, and how does it affect the bulk phase behavior of hard particles?\n\n**A)** The relative orientation of the particles, with the minimum excluded area achieved when the particles are antiparallel to each other.\n\n**B)** The size of the particles, with larger particles resulting in a larger excluded area.\n\n**C)** The shape of the particles, with elongated particles resulting in a larger excluded area.\n\n**D)** The temperature of the system, with higher temperatures resulting in a larger excluded area.\n\n**Correct Answer:** **A)** The relative orientation of the particles, with the minimum excluded area achieved when the particles are antiparallel to each other.\n\n**Explanation:** According to the documentation, the minimum value of the excluded area is always achieved when the particles are antiparallel to each other, regardless of the particle shape. This is because the excluded area is a function of the relative orientation of the particles, and antiparallel orientation results in the smallest possible excluded area. The other options are incorrect because the size of the particles, shape of the particles, and temperature of the system do not directly affect the magnitude of the excluded area."}, "3": {"documentation": {"title": "Stellar haloes in Milky-Way mass galaxies: From the inner to the outer\n  haloes", "source": "P. Tissera, T. Beers, D. Carollo, C. Scannapieco", "docs_id": "1309.3609", "section": ["astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stellar haloes in Milky-Way mass galaxies: From the inner to the outer\n  haloes. We present a comprehensive study of the chemical properties of the stellar haloes of Milky-Way mass galaxies, analysing the transition between the inner to the outer haloes. We find the transition radius between the relative dominance of the inner-halo and outer-halo stellar populations to be ~15-20 kpc for most of our haloes, similar to that inferred for the Milky Way from recent observations. While the number density of stars in the simulated inner-halo populations decreases rapidly with distance, the outer-halo populations contribute about 20-40 per cent in the fiducial solar neighborhood, in particular at the lowest metallicities. We have determined [Fe/H] profiles for our simulated haloes; they exhibit flat or mild gradients, in the range [-0.002, -0.01 ] dex/kpc. The metallicity distribution functions exhibit different features, reflecting the different assembly history of the individual stellar haloes. We find that stellar haloes formed with larger contributions from massive subgalactic systems have steeper metallicity gradients. Very metal-poor stars are mainly contributed to the halo systems by lower-mass satellites. There is a clear trend among the predicted metallicity distribution functions that a higher fraction of low-metallicity stars are found with increasing radius. These properties are consistent with the range of behaviours observed for stellar haloes of nearby galaxies."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the chemical properties of stellar haloes in Milky-Way mass galaxies, specifically the transition between the inner and outer haloes. What is the characteristic radius at which the inner-halo and outer-halo stellar populations dominate, and what implications does this have for the metallicity distribution functions of these haloes?\n\n**A)** The transition radius is approximately 5-10 kpc, and this corresponds to a sharp increase in the number density of stars in the outer halo, leading to a steeper metallicity gradient.\n\n**B)** The transition radius is approximately 15-20 kpc, and this corresponds to a gradual transition between the inner and outer haloes, with the outer halo contributing about 20-40% of the total stellar population in the solar neighborhood.\n\n**C)** The transition radius is approximately 30-40 kpc, and this corresponds to a significant decrease in the number density of stars in the inner halo, with the outer halo exhibiting a flat or mild metallicity gradient.\n\n**D)** The transition radius is approximately 10-15 kpc, and this corresponds to a rapid increase in the number density of stars in the outer halo, leading to a decrease in the metallicity gradient.\n\n**Correct Answer:** B) The transition radius is approximately 15-20 kpc, and this corresponds to a gradual transition between the inner and outer haloes, with the outer halo contributing about 20-40% of the total stellar population in the solar neighborhood.\n\n**Explanation:** The correct answer is based on the text, which states that the transition radius between the relative dominance of the inner-halo and outer-halo stellar populations is approximately 15-20 kpc. This corresponds to a gradual transition between the inner and outer haloes, with the outer halo contributing about 20-40% of the total stellar population in the solar neighborhood. The other options are incorrect because they either provide an incorrect transition radius or imply a more rapid or gradual transition than described in the text."}, "4": {"documentation": {"title": "Can billiard eigenstates be approximated by superpositions of plane\n  waves?", "source": "Boris Gutkin", "docs_id": "nlin/0301031", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Can billiard eigenstates be approximated by superpositions of plane\n  waves?. The plane wave decomposition method (PWDM) is one of the most popular strategies for numerical solution of the quantum billiard problem. The method is based on the assumption that each eigenstate in a billiard can be approximated by a superposition of plane waves at a given energy. By the classical results on the theory of differential operators this can indeed be justified for billiards in convex domains. On the contrary, in the present work we demonstrate that eigenstates of non-convex billiards, in general, cannot be approximated by any solution of the Helmholtz equation regular everywhere in $\\R^2$ (in particular, by linear combinations of a finite number of plane waves having the same energy). From this we infer that PWDM cannot be applied to billiards in non-convex domains. Furthermore, it follows from our results that unlike the properties of integrable billiards, where each eigenstate can be extended into the billiard exterior as a regular solution of the Helmholtz equation, the eigenstates of non-convex billiards, in general, do not admit such an extension."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nCan the eigenstates of non-convex billiards be approximated by superpositions of plane waves, and if not, what are the implications of this result for the plane wave decomposition method (PWDM)?\n\nA) Yes, but only for billiards with a finite number of corners.\nB) No, and this result implies that PWDM cannot be applied to non-convex billiards, but it does not preclude the possibility of approximating eigenstates with a finite number of plane waves.\nC) Yes, and this result implies that PWDM can be applied to non-convex billiards, but only for billiards with a finite number of corners.\nD) No, and this result implies that PWDM cannot be applied to non-convex billiards, and the eigenstates of non-convex billiards do not admit an extension into the billiard exterior as a regular solution of the Helmholtz equation.\n\nCorrect Answer: D) No, and this result implies that PWDM cannot be applied to non-convex billiards, and the eigenstates of non-convex billiards do not admit an extension into the billiard exterior as a regular solution of the Helmholtz equation.\n\nExplanation: The correct answer is D) because the documentation states that \"eigenstates of non-convex billiards, in general, cannot be approximated by any solution of the Helmholtz equation regular everywhere in $\\R^2$\". This implies that PWDM cannot be applied to non-convex billiards, and the eigenstates of non-convex billiards do not admit an extension into the billiard exterior as a regular solution of the Helmholtz equation. The other options are incorrect because they either contradict the documentation or introduce additional assumptions that are not supported by the text."}, "5": {"documentation": {"title": "Frequency-Selective Beamforming Cancellation Design for Millimeter-Wave\n  Full-Duplex", "source": "Ian P. Roberts, Hardik B. Jain, and Sriram Vishwanath", "docs_id": "1910.11983", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Frequency-Selective Beamforming Cancellation Design for Millimeter-Wave\n  Full-Duplex. The wide bandwidths offered at millimeter-wave (mmWave) frequencies have made them an attractive choice for future wireless communication systems. Recent works have presented beamforming strategies for enabling in-band full-duplex (FD) capability at mmWave even under the constraints of hybrid beamforming, extending the exciting possibilities of next-generation wireless. Existing mmWave FD designs, however, do not consider frequency-selective mmWave channels. Wideband communication at mmWave suggests that frequency-selectivity will likely be of concern since communication channels will be on the order of hundreds of megahertz or more. This has motivated the work of this paper, in which we present a frequency-selective beamforming design to enable practical wideband mmWave FD applications. In our designs, we account for the challenges associated with hybrid analog/digital beamforming such as phase shifter resolution, a desirably low number of radio frequency (RF) chains, and the frequency-flat nature of analog beamformers. We use simulation to validate our work, which indicates that spectral efficiency gains can be achieved with our design by enabling simultaneous transmission and reception in-band."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of frequency-selective beamforming design for millimeter-wave full-duplex (FD) systems, and how does it address the challenges associated with hybrid analog/digital beamforming?\n\n**A)** The primary motivation is to improve the spectral efficiency of mmWave FD systems by enabling simultaneous transmission and reception in-band, while addressing the challenges of phase shifter resolution and RF chain limitations.\n\n**B)** The primary motivation is to reduce the interference between transmission and reception in mmWave FD systems, while addressing the challenges of frequency-flat analog beamformers.\n\n**C)** The primary motivation is to increase the bandwidth of mmWave FD systems, while addressing the challenges of hybrid analog/digital beamforming.\n\n**D)** The primary motivation is to improve the signal-to-noise ratio (SNR) of mmWave FD systems, while addressing the challenges of phase shifter resolution and RF chain limitations.\n\n**Correct Answer:** A) The primary motivation is to improve the spectral efficiency of mmWave FD systems by enabling simultaneous transmission and reception in-band, while addressing the challenges of phase shifter resolution and RF chain limitations.\n\n**Explanation:** The correct answer is A) because the motivation behind the development of frequency-selective beamforming design is to improve the spectral efficiency of mmWave FD systems by enabling simultaneous transmission and reception in-band, which is a key challenge in hybrid analog/digital beamforming. The design addresses the challenges of phase shifter resolution and RF chain limitations, which are mentioned in the documentation as concerns in hybrid beamforming."}, "6": {"documentation": {"title": "Internal Feedback in Biological Control: Locality and System Level\n  Synthesis", "source": "Jing Shuang Li", "docs_id": "2109.11757", "section": ["eess.SY", "cs.SY", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Internal Feedback in Biological Control: Locality and System Level\n  Synthesis. The presence of internal feedback pathways (IFPs) is an ubiquitous yet unexplained phenomenon in the brain. Motivated by experimental observations on 1) motor-related signals in visual areas, and 2) massively distributed processing in the brain, we approach this problem from a sensorimotor standpoint and make use of distributed optimal controllers to explain IFPs. We use the System Level Synthesis (SLS) controller to model neuronal phenomena such as signaling delay, local processing, and local reaction. Based on the SLS controller, we make qualitative theoretical predictions about IFPs that has strong alignment with experimental and imaging studies. In particular, we introduce a necessary `mesocircuit' for optimal performance with distributed and local processing, and local disturbance rejection; this `mesocircuit' requires extreme amounts of IFPs and memory for proper function. This is the first theory that can replicate the massive amounts of IFPs in the brain purely from a priori principles, providing a new and promising theoretical basis upon which we can build to better understand the inner workings of the brain."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Internal Feedback in Biological Control: Locality and System Level Synthesis\" propose a necessary `mesocircuit' for optimal performance in distributed and local processing. What is the primary function of this `mesocircuit', according to the authors?\n\nA) To regulate the strength of motor-related signals in visual areas\nB) To facilitate massively distributed processing in the brain\nC) To provide a mechanism for local disturbance rejection and extreme amounts of internal feedback pathways\nD) To model neuronal phenomena such as signaling delay and local processing\n\n**Correct Answer:** C) To provide a mechanism for local disturbance rejection and extreme amounts of internal feedback pathways\n\n**Explanation:** The authors propose that the `mesocircuit' is necessary for optimal performance in distributed and local processing, and that it requires extreme amounts of internal feedback pathways and memory for proper function. This suggests that the primary function of the `mesocircuit' is to provide a mechanism for local disturbance rejection, which is a critical aspect of maintaining optimal performance in complex systems. The other options are incorrect because they do not accurately capture the primary function of the `mesocircuit' as proposed by the authors."}, "7": {"documentation": {"title": "The role of global economic policy uncertainty in predicting crude oil\n  futures volatility: Evidence from a two-factor GARCH-MIDAS model", "source": "Peng-Fei Dai (TJU), Xiong Xiong (TJU), Wei-Xing Zhou (ECUST)", "docs_id": "2007.12838", "section": ["q-fin.ST", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The role of global economic policy uncertainty in predicting crude oil\n  futures volatility: Evidence from a two-factor GARCH-MIDAS model. This paper aims to examine whether the global economic policy uncertainty (GEPU) and uncertainty changes have different impacts on crude oil futures volatility. We establish single-factor and two-factor models under the GARCH-MIDAS framework to investigate the predictive power of GEPU and GEPU changes excluding and including realized volatility. The findings show that the models with rolling-window specification perform better than those with fixed-span specification. For single-factor models, the GEPU index and its changes, as well as realized volatility, are consistent effective factors in predicting the volatility of crude oil futures. Specially, GEPU changes have stronger predictive power than the GEPU index. For two-factor models, GEPU is not an effective forecast factor for the volatility of WTI crude oil futures or Brent crude oil futures. The two-factor model with GEPU changes contains more information and exhibits stronger forecasting ability for crude oil futures market volatility than the single-factor models. The GEPU changes are indeed the main source of long-term volatility of the crude oil futures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary finding regarding the predictive power of Global Economic Policy Uncertainty (GEPU) changes versus the GEPU index in predicting crude oil futures volatility, according to the study?\n\n**A)** GEPU changes have weaker predictive power than the GEPU index.\n**B)** GEPU changes have stronger predictive power than the GEPU index, and are the main source of long-term volatility of crude oil futures.\n**C)** GEPU is not an effective forecast factor for the volatility of crude oil futures, regardless of whether it is included or excluded from the model.\n**D)** The two-factor model with GEPU changes exhibits weaker forecasting ability for crude oil futures market volatility compared to the single-factor models.\n\n**Correct Answer:** B) GEPU changes have stronger predictive power than the GEPU index, and are the main source of long-term volatility of crude oil futures.\n\n**Explanation:** The study finds that for single-factor models, GEPU changes have stronger predictive power than the GEPU index, and that GEPU changes are indeed the main source of long-term volatility of crude oil futures. This suggests that the changes in GEPU have a more significant impact on predicting crude oil futures volatility than the GEPU index itself."}, "8": {"documentation": {"title": "Flow and interferometry results from Au+Au collisions at\n  $\\sqrt{\\textit{s}_{NN}}$ = 4.5 GeV", "source": "STAR Collaboration: J. Adam, L. Adamczyk, J. R. Adams, J. K. Adkins,\n  G. Agakishiev, M. M. Aggarwal, Z. Ahammed, I. Alekseev, D. M. Anderson, A.\n  Aparin, E. C. Aschenauer, M. U. Ashraf, F. G. Atetalla, A. Attri, G. S.\n  Averichev, V. Bairathi, K. Barish, A. Behera, R. Bellwied, A. Bhasin, J.\n  Bielcik, J. Bielcikova, L. C. Bland, I. G. Bordyuzhin, J. D. Brandenburg, A.\n  V. Brandin, J. Butterworth, H. Caines, M. Calder\\'on de la Barca S\\'anchez,\n  J. M. Campbell, D. Cebra, I. Chakaberia, P. Chaloupka, B. K. Chan, F-H.\n  Chang, Z. Chang, N. Chankova-Bunzarova, A. Chatterjee, D. Chen, J. H. Chen,\n  X. Chen, Z. Chen, J. Cheng, M. Cherney, M. Chevalier, S. Choudhury, W.\n  Christie, X. Chu, H. J. Crawford, M. Csan\\'ad, M. Daugherity, T. G. Dedovich,\n  I. M. Deppner, A. A. Derevschikov, L. Didenko, X. Dong, J. L. Drachenberg, J.\n  C. Dunlop, T. Edmonds, N. Elsey, J. Engelage, G. Eppley, R. Esha, S. Esumi,\n  O. Evdokimov, A. Ewigleben, O. Eyser, R. Fatemi, S. Fazio, P. Federic, J.\n  Fedorisin, C. J. Feng, Y. Feng, P. Filip, E. Finch, Y. Fisyak, A. Francisco,\n  L. Fulek, C. A. Gagliardi, T. Galatyuk, F. Geurts, A. Gibson, K. Gopal, D.\n  Grosnick, W. Guryn, A. I. Hamad, A. Hamed, S. Harabasz, J. W. Harris, S. He,\n  W. He, X. H. He, S. Heppelmann, S. Heppelmann, N. Herrmann, E. Hoffman, L.\n  Holub, Y. Hong, S. Horvat, Y. Hu, H. Z. Huang, S. L. Huang, T. Huang, X.\n  Huang, T. J. Humanic, P. Huo, G. Igo, D. Isenhower, W. W. Jacobs, C. Jena, A.\n  Jentsch, Y. JI, J. Jia, K. Jiang, S. Jowzaee, X. Ju, E. G. Judd, S. Kabana,\n  M. L. Kabir, S. Kagamaster, D. Kalinkin, K. Kang, D. Kapukchyan, K. Kauder,\n  H. W. Ke, D. Keane, A. Kechechyan, M. Kelsey, Y. V. Khyzhniak, D. P.\n  Kiko{\\l}a, C. Kim, B. Kimelman, D. Kincses, T. A. Kinghorn, I. Kisel, A.\n  Kiselev, M. Kocan, L. Kochenda, L. K. Kosarzewski, L. Kozyra, L. Kramarik, P.\n  Kravtsov, K. Krueger, N. Kulathunga Mudiyanselage, L. Kumar, R. Kunnawalkam\n  Elayavalli, J. H. Kwasizur, R. Lacey, S. Lan, J. M. Landgraf, J. Lauret, A.\n  Lebedev, R. Lednicky, J. H. Lee, Y. H. Leung, C. Li, W. Li, W. Li, X. Li, Y.\n  Li, Y. Liang, R. Licenik, T. Lin, Y. Lin, M. A. Lisa, F. Liu, H. Liu, P. Liu,\n  P. Liu, T. Liu, X. Liu, Y. Liu, Z. Liu, T. Ljubicic, W. J. Llope, R. S.\n  Longacre, N. S. Lukow, S. Luo, X. Luo, G. L. Ma, L. Ma, R. Ma, Y. G. Ma, N.\n  Magdy, R. Majka, D. Mallick, S. Margetis, C. Markert, H. S. Matis, J. A.\n  Mazer, K. Meehan, N. G. Minaev, S. Mioduszewski, B. Mohanty, M. M. Mondal, I.\n  Mooney, Z. Moravcova, D. A. Morozov, M. Nagy, J. D. Nam, Md. Nasim, K. Nayak,\n  D. Neff, J. M. Nelson, D. B. Nemes, M. Nie, G. Nigmatkulov, T. Niida, L. V.\n  Nogach, T. Nonaka, A. S. Nunes, G. Odyniec, A. Ogawa, S. Oh, V. A. Okorokov,\n  B. S. Page, R. Pak, A. Pandav, Y. Panebratsev, B. Pawlik, D. Pawlowska, H.\n  Pei, C. Perkins, L. Pinsky, R. L. Pint\\'er, J. Pluta, J. Porter, M. Posik, N.\n  K. Pruthi, M. Przybycien, J. Putschke, H. Qiu, A. Quintero, S. K.\n  Radhakrishnan, S. Ramachandran, R. L. Ray, R. Reed, H. G. Ritter, J. B.\n  Roberts, O. V. Rogachevskiy, J. L. Romero, L. Ruan, J. Rusnak, N. R. Sahoo,\n  H. Sako, S. Salur, J. Sandweiss, S. Sato, W. B. Schmidke, N. Schmitz, B. R.\n  Schweid, F. Seck, J. Seger, M. Sergeeva, R. Seto, P. Seyboth, N. Shah, E.\n  Shahaliev, P. V. Shanmuganathan, M. Shao, F. Shen, W. Q. Shen, S. S. Shi, Q.\n  Y. Shou, E. P. Sichtermann, R. Sikora, M. Simko, J. Singh, S. Singha, N.\n  Smirnov, W. Solyst, P. Sorensen, H. M. Spinka, B. Srivastava, T. D. S.\n  Stanislaus, M. Stefaniak, D. J. Stewart, M. Strikhanov, B. Stringfellow, A.\n  A. P. Suaide, M. Sumbera, B. Summa, X. M. Sun, X. Sun, Y. Sun, Y. Sun, B.\n  Surrow, D. N. Svirida, P. Szymanski, A. H. Tang, Z. Tang, A. Taranenko, T.\n  Tarnowsky, J. H. Thomas, A. R. Timmins, D. Tlusty, M. Tokarev, C. A. Tomkiel,\n  S. Trentalange, R. E. Tribble, P. Tribedy, S. K. Tripathy, O. D. Tsai, Z. Tu,\n  T. Ullrich, D. G. Underwood, I. Upsal, G. Van Buren, J. Vanek, A. N.\n  Vasiliev, I. Vassiliev, F. Videb{\\ae}k, S. Vokal, S. A. Voloshin, F. Wang, G.\n  Wang, J. S. Wang, P. Wang, Y. Wang, Y. Wang, Z. Wang, J. C. Webb, P. C.\n  Weidenkaff, L. Wen, G. D. Westfall, H. Wieman, S. W. Wissink, R. Witt, Y. Wu,\n  Z. G. Xiao, G. Xie, W. Xie, H. Xu, N. Xu, Q. H. Xu, Y. F. Xu, Y. Xu, Z. Xu,\n  Z. Xu, C. Yang, Q. Yang, S. Yang, Y. Yang, Z. Yang, Z. Ye, Z. Ye, L. Yi, K.\n  Yip, H. Zbroszczyk, W. Zha, C. Zhang, D. Zhang, S. Zhang, S. Zhang, X. P.\n  Zhang, Y. Zhang, Y. Zhang, Z. J. Zhang, Z. Zhang, Z. Zhang, J. Zhao, C.\n  Zhong, C. Zhou, X. Zhu, Z. Zhu, M. Zurek, M. Zyzak", "docs_id": "2007.14005", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Flow and interferometry results from Au+Au collisions at\n  $\\sqrt{\\textit{s}_{NN}}$ = 4.5 GeV. The Beam Energy Scan (BES) program at the Relativistic Heavy Ion Collider (RHIC) was extended to energies below $\\sqrt{\\textit{s}_{NN}}$ = 7.7 GeV in 2015 by successful implementation of the fixed-target mode of operation in the STAR (Solenoidal Track At RHIC) experiment. In the fixed-target mode, ions circulate in one ring of the collider and interact with a stationary target at the entrance of the STAR Time Projection Chamber. The first results for Au+Au collisions at $\\sqrt{\\textit{s}_{NN}}$ = 4.5 GeV are presented, including directed and elliptic flow of identified hadrons, and radii from pion femtoscopy. The proton flow and pion femtoscopy results agree quantitatively with earlier measurements by Alternating Gradient Synchrotron experiments at similar energies. This validates running the STAR experiment in the fixed-target configuration. Pion directed and elliptic flow are presented for the first time at this beam energy. Pion and proton elliptic flow show behavior which hints at constituent quark scaling, but large error bars preclude reliable conclusions. The ongoing second phase of BES (BES-II) will provide fixed-target data sets with 100 times more events at each of several energies down to $\\sqrt{\\textit{s}_{NN}}$ = 3.0 GeV."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the STAR experiment's fixed-target configuration, based on the agreement between the proton flow and pion femtoscopy results with earlier measurements by Alternating Gradient Synchrotron experiments?\n\nA) The STAR experiment's fixed-target configuration is not suitable for studying hadronic interactions.\nB) The STAR experiment's fixed-target configuration is not capable of producing sufficient particle yields for reliable measurements.\nC) The STAR experiment's fixed-target configuration is validated by the quantitative agreement with earlier measurements, indicating that it is a suitable configuration for studying hadronic interactions at low energies.\nD) The STAR experiment's fixed-target configuration is only suitable for studying proton interactions, not hadronic interactions.\n\nCorrect Answer: C) The STAR experiment's fixed-target configuration is validated by the quantitative agreement with earlier measurements, indicating that it is a suitable configuration for studying hadronic interactions at low energies.\n\nExplanation: The correct answer can be inferred from the text, which states that the proton flow and pion femtoscopy results agree quantitatively with earlier measurements by Alternating Gradient Synchrotron experiments at similar energies. This suggests that the STAR experiment's fixed-target configuration is capable of producing reliable measurements, and therefore is a suitable configuration for studying hadronic interactions at low energies."}, "9": {"documentation": {"title": "FedFog: Network-Aware Optimization of Federated Learning over Wireless\n  Fog-Cloud Systems", "source": "Van-Dinh Nguyen, Symeon Chatzinotas, Bjorn Ottersten, and Trung Q.\n  Duong", "docs_id": "2107.02755", "section": ["cs.LG", "cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "FedFog: Network-Aware Optimization of Federated Learning over Wireless\n  Fog-Cloud Systems. Federated learning (FL) is capable of performing large distributed machine learning tasks across multiple edge users by periodically aggregating trained local parameters. To address key challenges of enabling FL over a wireless fog-cloud system (e.g., non-i.i.d. data, users' heterogeneity), we first propose an efficient FL algorithm based on Federated Averaging (called FedFog) to perform the local aggregation of gradient parameters at fog servers and global training update at the cloud. Next, we employ FedFog in wireless fog-cloud systems by investigating a novel network-aware FL optimization problem that strikes the balance between the global loss and completion time. An iterative algorithm is then developed to obtain a precise measurement of the system performance, which helps design an efficient stopping criteria to output an appropriate number of global rounds. To mitigate the straggler effect, we propose a flexible user aggregation strategy that trains fast users first to obtain a certain level of accuracy before allowing slow users to join the global training updates. Extensive numerical results using several real-world FL tasks are provided to verify the theoretical convergence of FedFog. We also show that the proposed co-design of FL and communication is essential to substantially improve resource utilization while achieving comparable accuracy of the learning model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Federated Learning (FL) over wireless fog-cloud systems, what is the primary motivation behind the proposed network-aware optimization problem in FedFog, and how does it address the challenges of non-i.i.d. data and user heterogeneity?\n\n**A)** To reduce the communication overhead between fog servers and cloud, and to improve the accuracy of local models by aggregating gradients from fast users first.\n\n**B)** To mitigate the straggler effect by training slow users first and then aggregating their gradients with those from fast users, while also optimizing the global loss and completion time.\n\n**C)** To enhance the robustness of FL models by incorporating domain knowledge from fog servers and cloud, and to improve the efficiency of local aggregation by using a novel network-aware optimization problem.\n\n**D)** To reduce the computational complexity of FL by parallelizing the training process across multiple fog servers and cloud, and to improve the accuracy of global updates by aggregating gradients from all users.\n\n**Correct Answer:** B) To mitigate the straggler effect by training slow users first and then aggregating their gradients with those from fast users, while also optimizing the global loss and completion time.\n\n**Explanation:** The correct answer is B) because the proposed network-aware optimization problem in FedFog aims to strike a balance between the global loss and completion time, which is essential to mitigate the straggler effect in FL. By training slow users first and then aggregating their gradients with those from fast users, FedFog can improve the efficiency of local aggregation and reduce the overall completion time. This approach also optimizes the global loss by ensuring that all users contribute to the training process, regardless of their speed."}, "10": {"documentation": {"title": "Synchronized and mixed outbreaks of coupled recurrent epidemics", "source": "Muhua Zheng, Ming Zhao, Byungjoon Min, and Zonghua Liu", "docs_id": "1610.02528", "section": ["physics.soc-ph", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Synchronized and mixed outbreaks of coupled recurrent epidemics. Epidemic spreading has been studied for a long time and most of them are focused on the growing aspect of a single epidemic outbreak. Recently, we extended the study to the case of recurrent epidemics (Sci. Rep. {\\bf 5}, 16010 (2015)) but limited only to a single network. We here report from the real data of coupled regions or cities that the recurrent epidemics in two coupled networks are closely related to each other and can show either synchronized outbreak phase where outbreaks occur simultaneously in both networks or mixed outbreak phase where outbreaks occur in one network but do not in another one. To reveal the underlying mechanism, we present a two-layered network model of coupled recurrent epidemics to reproduce the synchronized and mixed outbreak phases. We show that the synchronized outbreak phase is preferred to be triggered in two coupled networks with the same average degree while the mixed outbreak phase is preferred for the case with different average degrees. Further, we show that the coupling between the two layers is preferred to suppress the mixed outbreak phase but enhance the synchronized outbreak phase. A theoretical analysis based on microscopic Markov-chain approach is presented to explain the numerical results. This finding opens a new window for studying the recurrent epidemics in multi-layered networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of synchronized and mixed outbreaks of coupled recurrent epidemics, what is the primary mechanism that favors the synchronized outbreak phase in two coupled networks with the same average degree?\n\n**A)** The coupling between the two layers enhances the synchronized outbreak phase.\n**B)** The coupling between the two layers suppresses the synchronized outbreak phase.\n**C)** The average degree of the two networks is the primary factor that determines the outbreak phase.\n**D)** The average degree of the two networks is not a determining factor, and the coupling between the two layers is the primary mechanism.\n\n**Correct Answer:** A) The coupling between the two layers enhances the synchronized outbreak phase.\n\n**Explanation:** According to the documentation, the study found that the coupling between the two layers is preferred to enhance the synchronized outbreak phase. This suggests that the coupling between the two layers plays a crucial role in facilitating the synchronized outbreak phase in two coupled networks with the same average degree.\n\n**Note:** The other options are incorrect because:\n\n* B) The coupling between the two layers is actually preferred to suppress the mixed outbreak phase, not enhance the synchronized outbreak phase.\n* C) The average degree of the two networks is not the primary factor that determines the outbreak phase, but rather the coupling between the two layers.\n* D) The average degree of the two networks is indeed a determining factor, but the correct answer is that the coupling between the two layers enhances the synchronized outbreak phase, not that the average degree is not a determining factor."}, "11": {"documentation": {"title": "Uncertainty Quantification in Medical Image Segmentation with\n  Multi-decoder U-Net", "source": "Yanwu Yang, Xutao Guo, Yiwei Pan, Pengcheng Shi, Haiyan Lv, Ting Ma", "docs_id": "2109.07045", "section": ["eess.IV", "cs.AI", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Uncertainty Quantification in Medical Image Segmentation with\n  Multi-decoder U-Net. Accurate medical image segmentation is crucial for diagnosis and analysis. However, the models without calibrated uncertainty estimates might lead to errors in downstream analysis and exhibit low levels of robustness. Estimating the uncertainty in the measurement is vital to making definite, informed conclusions. Especially, it is difficult to make accurate predictions on ambiguous areas and focus boundaries for both models and radiologists, even harder to reach a consensus with multiple annotations. In this work, the uncertainty under these areas is studied, which introduces significant information with anatomical structure and is as important as segmentation performance. We exploit the medical image segmentation uncertainty quantification by measuring segmentation performance with multiple annotations in a supervised learning manner and propose a U-Net based architecture with multiple decoders, where the image representation is encoded with the same encoder, and segmentation referring to each annotation is estimated with multiple decoders. Nevertheless, a cross-loss function is proposed for bridging the gap between different branches. The proposed architecture is trained in an end-to-end manner and able to improve predictive uncertainty estimates. The model achieves comparable performance with fewer parameters to the integrated training model that ranked the runner-up in the MICCAI-QUBIQ 2020 challenge."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind estimating uncertainty in medical image segmentation, and how does it relate to the challenges faced in downstream analysis and diagnosis?\n\n**A)** To improve the accuracy of segmentation performance by reducing the impact of ambiguous areas and focus boundaries.\n**B)** To provide radiologists with more confidence in their annotations and improve the consistency of their work.\n**C)** To enable the development of more robust models that can accurately predict outcomes in complex medical scenarios.\n**D)** To quantify the uncertainty in medical image segmentation and provide a more comprehensive understanding of the limitations of current models.\n\n**Correct Answer:** C) To enable the development of more robust models that can accurately predict outcomes in complex medical scenarios.\n\n**Explanation:** The correct answer is C) because the primary motivation behind estimating uncertainty in medical image segmentation is to improve the robustness of models in complex medical scenarios. The documentation highlights the challenges of making accurate predictions on ambiguous areas and focus boundaries, and the need for models to be able to handle these uncertainties. By estimating uncertainty, models can better handle these complexities and provide more accurate predictions. The other options, while related to the topic, do not capture the primary motivation behind uncertainty estimation in medical image segmentation."}, "12": {"documentation": {"title": "Sectoral co-movements in the Indian stock market: A mesoscopic network\n  analysis", "source": "Kiran Sharma, Shreyansh Shah, Anindya S. Chakrabarti and Anirban\n  Chakraborti", "docs_id": "1607.05514", "section": ["q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sectoral co-movements in the Indian stock market: A mesoscopic network\n  analysis. In this article we review several techniques to extract information from stock market data. We discuss recurrence analysis of time series, decomposition of aggregate correlation matrices to study co-movements in financial data, stock level partial correlations with market indices, multidimensional scaling and minimum spanning tree. We apply these techniques to daily return time series from the Indian stock market. The analysis allows us to construct networks based on correlation matrices of individual stocks in one hand and on the other, we discuss dynamics of market indices. Thus both micro level and macro level dynamics can be analyzed using such tools. We use the multi-dimensional scaling methods to visualize the sectoral structure of the stock market, and analyze the comovements among the sectoral stocks. Finally, we construct a mesoscopic network based on sectoral indices. Minimum spanning tree technique is seen to be extremely useful in order to separate technologically related sectors and the mapping corresponds to actual production relationship to a reasonable extent."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the article \"Sectoral co-movements in the Indian stock market: A mesoscopic network analysis\" employ various techniques to extract information from stock market data. Which of the following techniques is NOT mentioned in the article as a method for analyzing co-movements in financial data?\n\nA) Recurrence analysis of time series\nB) Decomposition of aggregate correlation matrices\nC) Stock level partial correlations with market indices\nD) Principal Component Analysis (PCA)\n\n**A)**{candidate A}\n**B)**{candidate B}\n**C)**{candidate C}\n**D)**{candidate D} Correct Answer: **D)**{candidate D}\n\n**Explanation:** The article mentions recurrence analysis of time series, decomposition of aggregate correlation matrices, and stock level partial correlations with market indices as methods for analyzing co-movements in financial data. However, Principal Component Analysis (PCA) is not mentioned in the article as one of the techniques used. PCA is a widely used technique in finance for dimensionality reduction and extracting patterns from large datasets, but it is not explicitly mentioned in the article."}, "13": {"documentation": {"title": "Extending the applicability of Thermal Dynamics to Evolutionary Biology", "source": "Tommaso Brotto, Guy Bunin, Jorge Kurchan", "docs_id": "1507.07453", "section": ["cond-mat.stat-mech", "physics.bio-ph", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Extending the applicability of Thermal Dynamics to Evolutionary Biology. In the past years, a remarkable mapping has been found between the dynamics of a population of M individuals undergoing random mutations and selection, and that of a single system in contact with a thermal bath with temperature 1/M. This correspondence holds under the somewhat restrictive condition that the population is dominated by a single type at almost all times, punctuated by rare successive mutations. Here we argue that such thermal dynamics will hold more generally, specifically in systems with rugged fitness landscapes. This includes cases with strong clonal interference, where a number of concurrent mutants dominate the population. The problem becomes closely analogous to the experimental situation of glasses subjected to controlled variations of parameters such as temperature, pressure or magnetic fields. Non-trivial suggestions from the field of glasses may be thus proposed for evolutionary systems - including a large part of the numerical simulation procedures - that in many cases would have been counter intuitive without this background."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Extending the applicability of Thermal Dynamics to Evolutionary Biology\" argue that the correspondence between the dynamics of a population of M individuals undergoing random mutations and selection, and that of a single system in contact with a thermal bath with temperature 1/M, can be generalized to systems with rugged fitness landscapes. However, they also acknowledge that this correspondence is restricted to the condition that the population is dominated by a single type at almost all times, punctuated by rare successive mutations.\n\n**What are the main assumptions underlying the authors' argument, and how do they relate to the concept of \"rugged fitness landscapes\"?**\n\nA) The authors assume that the population is in a state of equilibrium, where the frequency of each genotype is constant over time. This assumption is relevant to the concept of rugged fitness landscapes because it implies that the population is not subject to strong selective pressures.\n\nB) The authors assume that the population is in a state of non-equilibrium, where the frequency of each genotype is changing over time due to random mutations and selection. This assumption is relevant to the concept of rugged fitness landscapes because it implies that the population is subject to strong selective pressures that can lead to the formation of fitness valleys and peaks.\n\nC) The authors assume that the population is in a state of steady-state, where the frequency of each genotype is constant over time, but with occasional rare events of successive mutations. This assumption is relevant to the concept of rugged fitness landscapes because it implies that the population is subject to both strong selective pressures and rare events that can lead to the formation of new fitness valleys and peaks.\n\nD) The authors assume that the population is in a state of dynamic equilibrium, where the frequency of each genotype is constantly changing over time due to random mutations and selection, but with a bias towards the most fit genotypes. This assumption is relevant to the concept of rugged fitness landscapes because it implies that the population is subject to strong selective pressures that can lead to the formation of fitness valleys and peaks.\n\n**Correct answer:** C) The authors assume that the population is in a state of steady-state, where the frequency of each genotype is constant over time, but with occasional rare events of successive mutations. This assumption is relevant to the concept of rugged fitness landscapes because it implies that the population is subject to both strong selective pressures and rare events that can lead to the formation of new fitness valleys and peaks.\n\n**Explanation:** The authors' argument is based on the idea that the dynamics of a population of M individuals undergoing random mutations and selection can be mapped to the dynamics of a single system in contact with a thermal bath with temperature 1/M. This mapping is only valid under the assumption that the population is in a state of steady-state, where the frequency of each genotype is constant over time, but with occasional rare events of successive mutations. This assumption is relevant to the concept of rugged fitness landscapes because it implies that the population is subject to both strong selective pressures and rare events that can lead to the formation of new fitness valleys and peaks. The other options do not accurately capture the authors' assumptions and the relationship to rugged fitness landscapes."}, "14": {"documentation": {"title": "Anisotropic Stark shift, field-induced dissociation, and\n  electroabsorption of excitons in phosphorene", "source": "H{\\o}gni C. Kamban, Thomas G. Pedersen, Nuno M. R. Peres", "docs_id": "2006.12908", "section": ["cond-mat.mes-hall", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Anisotropic Stark shift, field-induced dissociation, and\n  electroabsorption of excitons in phosphorene. We compute binding energies, Stark shifts, electric-field-induced dissociation rates, and the Franz-Keldysh effect for excitons in phosphorene in various dielectric surroundings. All three effects show a pronounced dependence on the direction of the in-plane electric field, with the dissociation rates in particular decreasing by several orders of magnitude upon rotating the electric field from the armchair to the zigzag axis. To better understand the numerical dissociation rates, we derive an analytical approximation to the anisotropic rates induced by weak electric fields, thereby generalizing the previously obtained result for isotropic two-dimensional semiconductors. This approximation is shown to be valid in the weak-field limit by comparing it to the exact rates. The anisotropy is also apparent in the large difference between armchair and zigzag components of the exciton polarizability tensor, which we compute for the five lowest lying states. As expected, we also find much more pronounced Stark shifts in either the armchair or zigzag direction, depending on the symmetry of the state in question. Finally, an isotropic interaction potential is shown to be an excellent approximation to a more accurate anisotropic interaction derived from the Poisson equation, confirming that the anisotropy of phosphorene is largely due to the direction dependence of the effective masses."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary reason for the anisotropic Stark shift, field-induced dissociation, and electroabsorption of excitons in phosphorene, as described in the given Arxiv documentation?\n\nA) The direction of the in-plane electric field affects the exciton polarizability tensor.\nB) The anisotropy of phosphorene is due to the direction dependence of the effective masses.\nC) The dissociation rates are influenced by the symmetry of the state in question.\nD) The anisotropy is a result of the difference in the exciton binding energies in the armchair and zigzag directions.\n\nCorrect Answer: B) The anisotropy of phosphorene is due to the direction dependence of the effective masses.\n\nExplanation: The correct answer is B) The anisotropy of phosphorene is due to the direction dependence of the effective masses. This is stated in the documentation as \"Finally, an isotropic interaction potential is shown to be an excellent approximation to a more accurate anisotropic interaction derived from the Poisson equation, confirming that the anisotropy of phosphorene is largely due to the direction dependence of the effective masses.\" This indicates that the direction dependence of the effective masses is the primary reason for the anisotropic Stark shift, field-induced dissociation, and electroabsorption of excitons in phosphorene.\n\nThe other options are incorrect because:\n\nA) The direction of the in-plane electric field affects the exciton polarizability tensor, but this is not the primary reason for the anisotropy.\n\nC) The symmetry of the state in question influences the Stark shift, but it is not the primary reason for the anisotropy.\n\nD) The difference in exciton binding energies in the armchair and zigzag directions is related to the anisotropy, but it is not the primary reason for the anisotropy."}, "15": {"documentation": {"title": "Joint Sensor Node Selection and State Estimation for Nonlinear Networks\n  and Systems", "source": "Aleksandar Haber", "docs_id": "2006.04342", "section": ["eess.SY", "cs.SY", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Joint Sensor Node Selection and State Estimation for Nonlinear Networks\n  and Systems. State estimation and sensor selection problems for nonlinear networks and systems are ubiquitous problems that are important for the control, monitoring, analysis, and prediction of a large number of engineered and physical systems. Sensor selection problems are extensively studied for linear networks. However, less attention has been dedicated to networks with nonlinear dynamics. Furthermore, widely used sensor selection methods relying on structural (graph-based) observability approaches might produce far from optimal results when applied to nonlinear network dynamics. In addition, state estimation and sensor selection problems are often treated separately, and this might decrease the overall estimation performance. To address these challenges, we develop a novel methodology for selecting sensor nodes for networks with nonlinear dynamics. Our main idea is to incorporate the sensor selection problem into an initial state estimation problem. The resulting mixed-integer nonlinear optimization problem is approximately solved using three methods. The good numerical performance of our approach is demonstrated by testing the algorithms on prototypical Duffing oscillator, associative memory, and chemical reaction networks. The developed codes are available online."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in developing a sensor selection methodology for nonlinear networks and systems, and how does the proposed approach address this challenge?\n\nA) The primary challenge is the high dimensionality of nonlinear dynamics, and the proposed approach addresses this by using a reduced-order model. However, this approach may not capture the full complexity of the system.\n\nB) The primary challenge is the lack of structural observability in nonlinear networks, and the proposed approach addresses this by incorporating the sensor selection problem into an initial state estimation problem, resulting in a mixed-integer nonlinear optimization problem.\n\nC) The primary challenge is the computational complexity of solving nonlinear optimization problems, and the proposed approach addresses this by using three approximation methods to solve the mixed-integer nonlinear optimization problem.\n\nD) The primary challenge is the difficulty in modeling nonlinear dynamics, and the proposed approach addresses this by using a linearization of the nonlinear dynamics, which may not accurately represent the system's behavior.\n\n**Correct Answer:** B) The primary challenge is the lack of structural observability in nonlinear networks, and the proposed approach addresses this by incorporating the sensor selection problem into an initial state estimation problem, resulting in a mixed-integer nonlinear optimization problem.\n\n**Explanation:** The correct answer is B) because the documentation states that \"widely used sensor selection methods relying on structural (graph-based) observability approaches might produce far from optimal results when applied to nonlinear network dynamics.\" This suggests that the primary challenge is indeed the lack of structural observability in nonlinear networks. The proposed approach addresses this challenge by incorporating the sensor selection problem into an initial state estimation problem, resulting in a mixed-integer nonlinear optimization problem. This approach is designed to overcome the limitations of traditional sensor selection methods for nonlinear networks."}, "16": {"documentation": {"title": "Hydrogen magnetic reaction gene regulation", "source": "Yeon Sook Kim, Dae Gwan Lee, Suk Keun Lee", "docs_id": "1210.7091", "section": ["q-bio.OT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hydrogen magnetic reaction gene regulation. A new gene regulation system using weak magnetic field can induce the hydrogen magnetic reaction (HMR) in hydrogen atoms, and subsequently affect the electrostatic polarity of hydrogen bonds in DNA base pairs. The HMR can sequentially activate the DNA base pair polarities of target DNA. With the characteristic base pair polarities of DNA duplex the (pyrimidine)m-(purine)n DNA segment is a basic unit to maintain and to accumulate the electrostatic energy of DNA duplex (1). To enhance the polarities of objective DNA this HMR gene regulation (HMR-GR) uses the polarized magnetic field with optimal nucleotide exposure time for T:A and C:G base pairs (50 msec and 80 msec, respectively). The targeting efficiency of HMR-GR to the objective DNA is theoretically up to 25%. In the present study, the HMR-GR expanded the conformation of oligo-dsDNA in vitro, implicating the active state of DNA, and also enhanced the restriction endonuclease digestion, in vitro RNA transcription, and the production of green fluorescence protein and {\\beta}-galactosidase by using each RE site sequence and relevant promoter sequence, respectively. Taken together, it is assumed that the HMR-GR is an effective and safe method to control the multiple genes sequentially by activating their specific DNA motifs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism by which the Hydrogen Magnetic Reaction Gene Regulation (HMR-GR) system induces the electrostatic polarity of hydrogen bonds in DNA base pairs, and what are the optimal exposure times for T:A and C:G base pairs?\n\n**A)** The HMR-GR system induces the electrostatic polarity of hydrogen bonds in DNA base pairs through the application of a strong magnetic field, with optimal exposure times of 50 msec for T:A and 80 msec for C:G base pairs.\n\n**B)** The HMR-GR system induces the electrostatic polarity of hydrogen bonds in DNA base pairs through the application of a weak magnetic field, with optimal exposure times of 50 msec for T:A and 80 msec for C:G base pairs.\n\n**C)** The HMR-GR system induces the electrostatic polarity of hydrogen bonds in DNA base pairs through the application of a thermal gradient, with optimal exposure times of 50 msec for T:A and 80 msec for C:G base pairs.\n\n**D)** The HMR-GR system induces the electrostatic polarity of hydrogen bonds in DNA base pairs through the application of a light-induced conformational change, with optimal exposure times of 50 msec for T:A and 80 msec for C:G base pairs.\n\n**Correct Answer:** B) The HMR-GR system induces the electrostatic polarity of hydrogen bonds in DNA base pairs through the application of a weak magnetic field, with optimal exposure times of 50 msec for T:A and 80 msec for C:G base pairs.\n\n**Explanation:** The correct answer is based on the information provided in the documentation, which states that the HMR-GR system uses a \"polarized magnetic field\" to induce the electrostatic polarity of hydrogen bonds in DNA base pairs. Additionally, the documentation specifies that the optimal exposure times for T:A and C:G base pairs are 50 msec and 80 msec, respectively. Therefore, option B is the correct answer."}, "17": {"documentation": {"title": "Cavity-enhanced optical Hall effect in two-dimensional free charge\n  carrier gases detected at terahertz frequencies", "source": "S. Knight, S. Sch\\\"oche, V. Darakchieva, P. K\\\"uhne, J.-F. Carlin, N.\n  Grandjean, C.M. Herzinger, M. Schubert and T. Hofmann", "docs_id": "1504.00705", "section": ["cond-mat.mes-hall", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cavity-enhanced optical Hall effect in two-dimensional free charge\n  carrier gases detected at terahertz frequencies. The effect of a tunable, externally coupled Fabry-P\\'{e}rot cavity to resonantly enhance the optical Hall effect signatures at terahertz frequencies produced by a traditional Drude-like two-dimensional electron gas is shown and discussed in this communication. As a result, the detection of optical Hall effect signatures at conveniently obtainable magnetic fields, for example by neodymium permanent magnets, is demonstrated. An AlInN/GaN-based high electron mobility transistor structure grown on a sapphire substrate is used for the experiment. The optical Hall effect signatures and their dispersions, which are governed by the frequency and the reflectance minima and maxima of the externally coupled Fabry-P\\'{e}rot cavity, are presented and discussed. Tuning the externally coupled Fabry-P\\'{e}rot cavity strongly modifies the optical Hall effect signatures, which provides a new degree of freedom for optical Hall effect experiments in addition to frequency, angle of incidence and magnetic field direction and strength."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of the externally coupled Fabry-P\u00e9rot cavity on the optical Hall effect signatures in the experiment described in the communication?\n\nA) It reduces the reflectance minima and maxima, resulting in a decrease in the optical Hall effect signatures.\nB) It modifies the frequency of the optical Hall effect signatures, but does not affect the reflectance minima and maxima.\nC) It enhances the reflectance minima and maxima, resulting in a strong modification of the optical Hall effect signatures.\nD) It does not affect the optical Hall effect signatures, as the cavity is not coupled to the experiment.\n\nCorrect Answer: C) It enhances the reflectance minima and maxima, resulting in a strong modification of the optical Hall effect signatures.\n\nExplanation: The communication states that the effect of the tunable, externally coupled Fabry-P\u00e9rot cavity is to \"resonantly enhance the optical Hall effect signatures at terahertz frequencies\". This implies that the cavity modifies the reflectance minima and maxima, which in turn enhances the optical Hall effect signatures. This is a key aspect of the experiment, as it allows for the detection of optical Hall effect signatures at conveniently obtainable magnetic fields."}, "18": {"documentation": {"title": "Solubilization kinetics determines the pulsatory dynamics of lipid\n  vesicles exposed to surfactant", "source": "Morgan Chabanon and Padmini Rangamani", "docs_id": "1802.00472", "section": ["physics.bio-ph", "cond-mat.soft", "q-bio.SC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Solubilization kinetics determines the pulsatory dynamics of lipid\n  vesicles exposed to surfactant. We establish a biophysical model for the dynamics of lipid vesicles exposed to surfactants. The solubilization of the lipid membrane due to the insertion of surfactant molecules induces a reduction of membrane surface area at almost constant vesicle volume. This results in a rate-dependent increase of membrane tension and leads to the opening of a micron-sized pore. We show that solubilization kinetics due to surfactants can determine the regimes of pore dynamics: either the pores open and reseal within a second (short-lived pore), or the pore stays open up to a few minutes (long-lived pore). First, we validate our model with previously published experimental measurements of pore dynamics. Then, we investigate how the solubilization kinetics and membrane properties affect the dynamics of the pore and construct a phase diagram for short and long-lived pores. Finally, we examine the dynamics of sequential pore openings and show that cyclic short-lived pores occur at a period inversely proportional to the solubilization rate. By deriving a theoretical expression for the cycle period, we provide an analytic tool to measure the solubilization rate of lipid vesicles by surfactants. Our findings shed light on some fundamental biophysical mechanisms that allow simple cell-like structures to sustain their integrity against environmental stresses, and have the potential to aid the design of vesicle-based drug delivery systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of surfactant insertion on lipid vesicles, and how does it relate to the dynamics of pore formation?\n\nA) The insertion of surfactant molecules increases the membrane surface area, leading to a decrease in membrane tension and pore formation.\nB) The insertion of surfactant molecules reduces the membrane surface area, leading to an increase in membrane tension and pore formation.\nC) The insertion of surfactant molecules has no effect on the membrane surface area, and pore formation is solely dependent on the vesicle volume.\nD) The insertion of surfactant molecules increases the vesicle volume, leading to a decrease in membrane tension and pore formation.\n\nCorrect Answer: B) The insertion of surfactant molecules reduces the membrane surface area, leading to an increase in membrane tension and pore formation.\n\nExplanation: According to the documentation, the insertion of surfactant molecules reduces the membrane surface area, leading to an increase in membrane tension. This increase in tension eventually leads to the opening of a micron-sized pore. The correct answer is B, as it accurately describes the primary effect of surfactant insertion on lipid vesicles.\n\nNote: The other options are incorrect because:\n\n* A) The insertion of surfactant molecules actually decreases the membrane surface area, not increases it.\n* C) The insertion of surfactant molecules does have an effect on the membrane surface area, and pore formation is indeed related to the vesicle volume.\n* D) The insertion of surfactant molecules does not increase the vesicle volume, and pore formation is not solely dependent on the vesicle volume."}, "19": {"documentation": {"title": "Variance estimation and asymptotic confidence bands for the mean\n  estimator of sampled functional data with high entropy unequal probability\n  sampling designs", "source": "Herv\\'e Cardot and Camelia Goga and Pauline Lardin", "docs_id": "1209.6503", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Variance estimation and asymptotic confidence bands for the mean\n  estimator of sampled functional data with high entropy unequal probability\n  sampling designs. For fixed size sampling designs with high entropy it is well known that the variance of the Horvitz-Thompson estimator can be approximated by the H\\'ajek formula. The interest of this asymptotic variance approximation is that it only involves the first order inclusion probabilities of the statistical units. We extend this variance formula when the variable under study is functional and we prove, under general conditions on the regularity of the individual trajectories and the sampling design, that we can get a uniformly convergent estimator of the variance function of the Horvitz-Thompson estimator of the mean function. Rates of convergence to the true variance function are given for the rejective sampling. We deduce, under conditions on the entropy of the sampling design, that it is possible to build confidence bands whose coverage is asymptotically the desired one via simulation of Gaussian processes with variance function given by the H\\'ajek formula. Finally, the accuracy of the proposed variance estimator is evaluated on samples of electricity consumption data measured every half an hour over a period of one week."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a sampling design with high entropy, where the variable under study is a functional data, and the sampling units have unequal probability of inclusion. Prove that the H\u00e1jek formula for the variance of the Horvitz-Thompson estimator can be extended to estimate the variance function of the mean function of the functional data.\n\n**A)** The H\u00e1jek formula can be extended by replacing the first-order inclusion probabilities with the second-order inclusion probabilities, which are defined as the probabilities of two units being included in the sample together.\n\n**B)** The H\u00e1jek formula can be extended by using the concept of entropy of the sampling design, which allows us to approximate the variance function of the Horvitz-Thompson estimator.\n\n**C)** The H\u00e1jek formula can be extended by assuming that the individual trajectories of the functional data are regular, meaning that they satisfy certain smoothness conditions, and using this assumption to derive a uniformly convergent estimator of the variance function.\n\n**D)** The H\u00e1jek formula cannot be extended to estimate the variance function of the mean function of the functional data, as it only involves the first-order inclusion probabilities and does not account for the functional nature of the data.\n\n**Correct Answer:** C) The H\u00e1jek formula can be extended by assuming that the individual trajectories of the functional data are regular, meaning that they satisfy certain smoothness conditions, and using this assumption to derive a uniformly convergent estimator of the variance function.\n\n**Explanation:** The correct answer is C) because the documentation states that the interest of the asymptotic variance approximation is that it only involves the first-order inclusion probabilities of the statistical units, and that the extension of the H\u00e1jek formula to estimate the variance function of the mean function of the functional data is possible under general conditions on the regularity of the individual trajectories and the sampling design. The assumption of regularity of the individual trajectories is crucial in deriving a uniformly convergent estimator of the variance function."}, "20": {"documentation": {"title": "The economics of minority language use: theory and empirical evidence\n  for a language game model", "source": "Stefan Sperlich and Jose-Ramon Uriarte", "docs_id": "1908.11604", "section": ["econ.EM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The economics of minority language use: theory and empirical evidence\n  for a language game model. Language and cultural diversity is a fundamental aspect of the present world. We study three modern multilingual societies -- the Basque Country, Ireland and Wales -- which are endowed with two, linguistically distant, official languages: $A$, spoken by all individuals, and $B$, spoken by a bilingual minority. In the three cases it is observed a decay in the use of minoritarian $B$, a sign of diversity loss. However, for the \"Council of Europe\" the key factor to avoid the shift of $B$ is its use in all domains. Thus, we investigate the language choices of the bilinguals by means of an evolutionary game theoretic model. We show that the language population dynamics has reached an evolutionary stable equilibrium where a fraction of bilinguals have shifted to speak $A$. Thus, this equilibrium captures the decline in the use of $B$. To test the theory we build empirical models that predict the use of $B$ for each proportion of bilinguals. We show that model-based predictions fit very well the observed use of Basque, Irish, and Welsh."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of the language game model for minority language use, what is the key factor that the Council of Europe aims to promote to avoid the shift of minority language B, according to the study on the Basque Country, Ireland, and Wales?\n\n**A)** The use of minority language B in formal settings\n**B)** The use of minority language B in all domains\n**C)** The promotion of bilingualism among the minority population\n**D)** The suppression of minority language B in education\n\n**Correct Answer:** **B)** The use of minority language B in all domains\n\n**Explanation:** According to the study, the key factor to avoid the shift of minority language B is its use in all domains, as observed in the Council of Europe. This is because the study shows that the language population dynamics has reached an evolutionary stable equilibrium where a fraction of bilinguals have shifted to speak A, and the use of B in all domains is crucial to maintain its use."}, "21": {"documentation": {"title": "Amplitude mediated spiral chimera pattern in a nonlinear\n  reaction-diffusion system", "source": "Srilena Kundu, Paulsamy Muruganandam, Dibakar Ghosh and M. Lakshmanan", "docs_id": "2105.10701", "section": ["nlin.AO", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Amplitude mediated spiral chimera pattern in a nonlinear\n  reaction-diffusion system. Formation of diverse patterns in spatially extended reaction-diffusion systems is an important aspect of study which is pertinent to many chemical and biological processes. Of special interest is the peculiar phenomenon of chimera state having spatial coexistence of coherent and incoherent dynamics in a system of identically interacting individuals. In the present article, we report the emergence of various collective dynamical patterns while considering a system of prey-predator dynamics in presence of a two-dimensional diffusive environment. Particularly, we explore the observance of four distinct categories of spatial arrangements among the species, namely spiral wave, spiral chimera, completely synchronized oscillations, and oscillation death states in a broad region of the diffusion-driven parameter space. Emergence of amplitude mediated spiral chimera states displaying drifted amplitudes and phases in the incoherent subpopulation is detected for parameter values beyond both Turing and Hopf bifurcations. Transition scenarios among all these distinguishable patterns are numerically demonstrated for a wide range of the diffusion coefficients which reveal that the chimera states arise during the transition from oscillatory to steady state dynamics. Furthermore, we characterize the occurrence of each of the recognizable patterns by estimating the strength of incoherent subpopulations in the two-dimensional space."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Amplitude Mediated Spiral Chimera Pattern in a Nonlinear Reaction-Diffusion System, what is the significance of the emergence of amplitude-mediated spiral chimera states, and how do these states arise in the system?\n\n**A)** These states represent a stable oscillatory regime that emerges in the system due to the presence of a two-dimensional diffusive environment. They are characterized by a synchronized oscillation of the prey and predator populations.\n\n**B)** These states represent a transition from oscillatory to steady-state dynamics, where the incoherent subpopulation exhibits drifted amplitudes and phases, leading to the emergence of spiral wave, spiral chimera, completely synchronized oscillations, and oscillation death states.\n\n**C)** These states are a result of the Turing and Hopf bifurcations, where the system undergoes a phase transition from a stable to an unstable state, leading to the emergence of complex patterns in the spatially extended reaction-diffusion system.\n\n**D)** These states are a manifestation of the system's ability to exhibit diverse patterns in response to changes in the diffusion coefficients, leading to the emergence of spiral wave, spiral chimera, completely synchronized oscillations, and oscillation death states.\n\n**Correct Answer:** B) These states represent a transition from oscillatory to steady-state dynamics, where the incoherent subpopulation exhibits drifted amplitudes and phases, leading to the emergence of spiral wave, spiral chimera, completely synchronized oscillations, and oscillation death states.\n\n**Explanation:** The correct answer is B) because the emergence of amplitude-mediated spiral chimera states is a result of the transition from oscillatory to steady-state dynamics, where the incoherent subpopulation exhibits drifted amplitudes and phases. This transition is characterized by the emergence of different patterns in the spatially extended reaction-diffusion system, including spiral wave, spiral chimera, completely synchronized oscillations, and oscillation death states. The other options are incorrect because they do not accurately describe the significance of the emergence of amplitude-mediated spiral chimera states. Option A is incorrect because the emergence of amplitude-mediated spiral chimera states is not a stable oscillatory regime. Option C is incorrect because the Turing and Hopf bifurcations are not directly related to the emergence of amplitude-mediated spiral chimera states. Option D is incorrect because the emergence of amplitude-mediated spiral chimera states is not a result of changes in the diffusion coefficients."}, "22": {"documentation": {"title": "Large Language Models Can Be Strong Differentially Private Learners", "source": "Xuechen Li, Florian Tram\\`er, Percy Liang, Tatsunori Hashimoto", "docs_id": "2110.05679", "section": ["cs.LG", "cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Large Language Models Can Be Strong Differentially Private Learners. Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and attempts at straightforwardly applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead. We show that this performance drop can be mitigated with (1) the use of large pretrained models; (2) hyperparameters that suit DP optimization; and (3) fine-tuning objectives aligned with the pretraining procedure. With these factors set right, we obtain private NLP models that outperform state-of-the-art private training approaches and strong non-private baselines -- by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained models tends to not suffer from dimension-dependent performance degradation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in applying Differentially Private Stochastic Gradient Descent (DP-SGD) to large deep learning models of text, and how can it be mitigated?\n\n**A)** The primary challenge is the high computational overhead, and it can be mitigated by using smaller pre-trained models and adjusting hyperparameters.\n\n**B)** The primary challenge is the limited success of DP learning for building large deep learning models of text, and it can be mitigated by using large pre-trained models, hyperparameters that suit DP optimization, and fine-tuning objectives aligned with the pretraining procedure.\n\n**C)** The primary challenge is the dimension-dependent performance degradation, and it can be mitigated by using noise-reducing techniques and increasing the model size.\n\n**D)** The primary challenge is the lack of memory efficiency, and it can be mitigated by using techniques such as model pruning and knowledge distillation.\n\n**Correct Answer:** B) The primary challenge is the limited success of DP learning for building large deep learning models of text, and it can be mitigated by using large pre-trained models, hyperparameters that suit DP optimization, and fine-tuning objectives aligned with the pretraining procedure.\n\n**Explanation:** The correct answer is based on the first sentence of the provided documentation, which states that \"Differentially Private (DP) learning has seen limited success for building large deep learning models of text...\" The subsequent paragraphs discuss the challenges and limitations of applying DP-SGD to large deep learning models of text, and propose solutions to mitigate these challenges. Candidate B accurately summarizes the primary challenge and the proposed solutions."}, "23": {"documentation": {"title": "D mesons in isospin asymmetric strange hadronic matter", "source": "Arvind Kumar and Amruta Mishra", "docs_id": "1010.0403", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "D mesons in isospin asymmetric strange hadronic matter. We study the in-medium properties of $D$ and $\\bar{D}$ mesons in isospin asymmetric hyperonic matter arising due to their interactions with the light hadrons. The interactions of $D$ and $\\bar{D}$ mesons with these light hadrons are derived by generalizing the chiral SU(3) model used for the study of hyperonic matter to SU(4). The nucleons, the scalar isoscalar meson, $\\sigma$ and the scalar-isovector meson, $\\delta$ as modified in the strange hadronic matter, modify the masses of $D$ and $\\bar{D}$ mesons. It is found that as compared to the $\\bar{D}$ mesons, the $D$ meson properties are more sensitive to the isospin asymmetry at high densities. The effects of strangeness in the medium on the properties of $D$ and $\\bar{D}$ mesons are studied in the present investigation. The $D$ mesons ($D^0$,$D^+$) are found to undergo larger medium modifications as compared to $\\bar{D}$ mesons ($\\bar {D^0}$, $D^-$) with the strangeness fraction, $f_s$ and these modifications are observed to be more appreciable at high densities. The present study of the in-medium properties of $D$ and $\\bar{D}$ mesons will be of relevance for the experiments in the future Facility for Antiproton and Ion Research, GSI, where the baryonic matter at high densities will be produced. The isospin asymmetric effects in the doublet $D = (D^{0}, D^{+})$ in the strange hadronic matter should show in observables like their production and flow in asymmetric heavy-ion collisions as well as in $J/\\psi$ suppression."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of isospin asymmetric strange hadronic matter, what is the primary effect on the properties of $D$ mesons compared to $\\bar{D}$ mesons, and how does this effect change with increasing density?\n\n**A)** The $D$ mesons are less sensitive to the isospin asymmetry at high densities, while the $\\bar{D}$ mesons are more sensitive.\n\n**B)** The $D$ mesons are more sensitive to the isospin asymmetry at high densities, while the $\\bar{D}$ mesons are less sensitive.\n\n**C)** The effects of strangeness in the medium on the properties of $D$ and $\\bar{D}$ mesons are the same, and do not depend on density.\n\n**D)** The $D$ mesons undergo larger medium modifications as compared to $\\bar{D}$ mesons, but these modifications are only significant at low densities.\n\n**Correct Answer:** B) The $D$ mesons are more sensitive to the isospin asymmetry at high densities, while the $\\bar{D}$ mesons are less sensitive.\n\n**Explanation:** According to the documentation, the $D$ mesons are found to undergo larger medium modifications as compared to $\\bar{D}$ mesons, and these modifications are observed to be more appreciable at high densities. This suggests that the $D$ mesons are more sensitive to the isospin asymmetry at high densities, while the $\\bar{D}$ mesons are less sensitive."}, "24": {"documentation": {"title": "Contagious McKean-Vlasov systems with heterogeneous impact and exposure", "source": "Zachary Feinstein and Andreas Sojmark", "docs_id": "2104.06776", "section": ["math.PR", "q-fin.MF", "q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Contagious McKean-Vlasov systems with heterogeneous impact and exposure. We introduce a heterogeneous formulation of a contagious McKean-Vlasov system, whose inherent heterogeneity comes from asymmetric interactions with a natural and highly tractable structure. It is shown that this formulation characterises the limit points of a finite particle system, deriving from a balance sheet based model of solvency contagion in interbank markets, where banks have heterogeneous exposure to and impact on the distress within the system. We also provide a simple result on global uniqueness for the full problem with common noise under a smallness condition on the strength of interactions, and we show that, in the problem without common noise, there is a unique differentiable solution up to an explosion time. Finally, we identify an intuitive and consistent way of specifying how the system should jump to resolve an instability when the contagious pressures become too large. This is known to happen even in the homogeneous version of the problem, where jumps are specified by a 'physical' notion of solution, but no such notion currently exists for a heterogeneous formulation of the system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a contagious McKean-Vlasov system with heterogeneous impact and exposure, as introduced in the paper \"Contagious McKean-Vlasov systems with heterogeneous impact and exposure\". Suppose we have a finite particle system that arises from a balance sheet based model of solvency contagion in interbank markets, where banks have heterogeneous exposure to and impact on the distress within the system.\n\n**A)** Show that the limit points of the finite particle system can be characterized by the proposed heterogeneous formulation of the McKean-Vlasov system.\n\n**B)** Prove that the full problem with common noise has a global unique solution under a smallness condition on the strength of interactions.\n\n**C)** Derive an expression for the explosion time of the system without common noise, assuming that the system jumps to resolve an instability when the contagious pressures become too large.\n\n**D)** Explain the intuitive and consistent way of specifying how the system should jump to resolve an instability, and discuss the implications of this approach for the homogeneous version of the problem.\n\n**Correct Answer:** C) Derive an expression for the explosion time of the system without common noise, assuming that the system jumps to resolve an instability when the contagious pressures become too large.\n\n**Explanation:**\n\nThe correct answer is C) because the paper explicitly states that the system jumps to resolve an instability when the contagious pressures become too large, and the authors provide a simple result on global uniqueness for the full problem with common noise under a smallness condition on the strength of interactions. However, the paper does not provide an explicit expression for the explosion time of the system without common noise.\n\nThe other options are incorrect because:\n\n* A) The limit points of the finite particle system are characterized by the proposed heterogeneous formulation of the McKean-Vlasov system, but this is not a direct answer to the question.\n* B) The full problem with common noise has a global unique solution under a smallness condition on the strength of interactions, but this is not directly related to the explosion time of the system without common noise.\n* D) The intuitive and consistent way of specifying how the system should jump to resolve an instability is discussed in the paper, but the correct answer is not this option."}, "25": {"documentation": {"title": "ParPaRaw: Massively Parallel Parsing of Delimiter-Separated Raw Data", "source": "Elias Stehle and Hans-Arno Jacobsen", "docs_id": "1905.13415", "section": ["cs.DB", "cs.DC", "cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "ParPaRaw: Massively Parallel Parsing of Delimiter-Separated Raw Data. Parsing is essential for a wide range of use cases, such as stream processing, bulk loading, and in-situ querying of raw data. Yet, the compute-intense step often constitutes a major bottleneck in the data ingestion pipeline, since parsing of inputs that require more involved parsing rules is challenging to parallelise. This work proposes a massively parallel algorithm for parsing delimiter-separated data formats on GPUs. Other than the state-of-the-art, the proposed approach does not require an initial sequential pass over the input to determine a thread's parsing context. That is, how a thread, beginning somewhere in the middle of the input, should interpret a certain symbol (e.g., whether to interpret a comma as a delimiter or as part of a larger string enclosed in double-quotes). Instead of tailoring the approach to a single format, we are able to perform a massively parallel FSM simulation, which is more flexible and powerful, supporting more expressive parsing rules with general applicability. Achieving a parsing rate of as much as 14.2 GB/s, our experimental evaluation on a GPU with 3584 cores shows that the presented approach is able to scale to thousands of cores and beyond. With an end-to-end streaming approach, we are able to exploit the full-duplex capabilities of the PCIe bus and hide latency from data transfers. Considering the end-to-end performance, the algorithm parses 4.8 GB in as little as 0.44 seconds, including data transfers."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is a key advantage of the proposed massively parallel parsing algorithm in ParPaRaw, which enables it to scale to thousands of cores and beyond?\n\nA) It requires an initial sequential pass over the input to determine a thread's parsing context.\nB) It can only parse delimiter-separated data formats that require simple parsing rules.\nC) It performs a massively parallel FSM simulation, supporting more expressive parsing rules with general applicability.\nD) It is limited to parsing data formats that are specific to a single hardware platform.\n\nCorrect Answer: C) It performs a massively parallel FSM simulation, supporting more expressive parsing rules with general applicability.\n\nExplanation: The correct answer is C) because the proposed algorithm in ParPaRaw performs a massively parallel FSM simulation, which allows it to support more expressive parsing rules and has general applicability. This is a key advantage of the algorithm, as it enables it to handle a wide range of parsing tasks and formats. The other options are incorrect because they do not accurately describe the algorithm's capabilities or limitations. Option A is incorrect because the algorithm does not require an initial sequential pass over the input to determine a thread's parsing context. Option B is incorrect because the algorithm can parse a wide range of delimiter-separated data formats, not just those with simple parsing rules. Option D is incorrect because the algorithm is not limited to parsing data formats specific to a single hardware platform."}, "26": {"documentation": {"title": "Hard pomeron enhancement of ultrahigh-energy neutrino-nucleon\n  cross-sections", "source": "A. Z. Gazizov and S. I. Yanush", "docs_id": "astro-ph/0105368", "section": ["astro-ph", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hard pomeron enhancement of ultrahigh-energy neutrino-nucleon\n  cross-sections. An unknown small-x behavior of nucleon structure functions gives appreciable uncertainties to high-energy neutrino-nucleon cross-sections. We construct structure functions using at small x Regge inspired description by A. Donnachie and P. V. Landshoff with soft and hard pomerons, and employing at larger x the perturbative QCD expressions. The smooth interpolation between two regimes for each Q^2 is provided with the help of simple polynomial functions. To obtain low-x neutrino-nucleon structure functions $F_2^{\\nu N, \\bar \\nu N}(x,Q^2)$ and singlet part of $F_{3}^{\\nu N,\\bar \\nu N}(x,Q^2)$ from Donnachie-Landshoff function $F_2^{ep}(x,Q^2)$, we use the Q^2-dependent ratios R_2(Q^2) and R_3(Q^2) derived from perturbative QCD calculations. Non-singlet part of F_3 at low x, which is very small, is taken as power-law extrapolation of perturbative function at larger x. This procedure gives a full set of smooth neutrino-nucleon structure functions in the whole range of x and Q^2 at interest. Using these structure functions, we have calculated the neutrino-nucleon cross-sections and compared them with some other cross-sections known in literature. Our cross-sections turn out to be the highest among them at the highest energies, which is explained by contribution of the hard pomeron."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary contribution to the highest-energy neutrino-nucleon cross-sections, according to the study mentioned in the Arxiv documentation?\n\nA) The soft pomeron\nB) The hard pomeron\nC) The non-singlet part of F3 at low x\nD) The perturbative QCD expressions\n\nCorrect Answer: B) The hard pomeron\n\nExplanation: The study mentions that the hard pomeron contribution is responsible for the highest-energy neutrino-nucleon cross-sections, which is a key finding of the research. The correct answer is B) The hard pomeron. The other options are incorrect because the soft pomeron is not mentioned as a primary contributor, the non-singlet part of F3 at low x is a small contribution, and the perturbative QCD expressions are used to derive the structure functions, but not as the primary contributor to the cross-sections."}, "27": {"documentation": {"title": "Measurement of the underlying event in jet events from 7 TeV\n  proton-proton collisions with the ATLAS detector", "source": "ATLAS Collaboration", "docs_id": "1406.0392", "section": ["hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurement of the underlying event in jet events from 7 TeV\n  proton-proton collisions with the ATLAS detector. Distributions sensitive to the underlying event in QCD jet events have been measured with the ATLAS detector at the LHC, based on 37/pb of proton-proton collision data collected at a centre-of-mass energy of 7 TeV. Charged-particle mean $p_T$ and densities of all-particle $E_T$ and charged-particle multiplicity and $p_T$ have been measured in regions azimuthally transverse to the hardest jet in each event. These are presented both as one-dimensional distributions and with their mean values as functions of the leading-jet transverse momentum from 20 GeV to 800 GeV. The correlation of charged-particle mean $p_T$ with charged-particle multiplicity is also studied, and the $E_T$ densities include the forward rapidity region; these features provide extra data constraints for Monte Carlo modelling of colour reconnection and beam-remnant effects respectively. For the first time, underlying event observables have been computed separately for inclusive jet and exclusive dijet event selections, allowing more detailed study of the interplay of multiple partonic scattering and QCD radiation contributions to the underlying event. Comparisons to the predictions of different Monte Carlo models show a need for further model tuning, but the standard approach is found to generally reproduce the features of the underlying event in both types of event selection."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the implications of the ATLAS detector's measurement of the underlying event in QCD jet events from 7 TeV proton-proton collisions on the tuning of Monte Carlo models.\n\nA) The results suggest that the standard approach to modelling the underlying event is sufficient, and further tuning is not necessary.\nB) The correlation between charged-particle mean $p_T$ and charged-particle multiplicity provides strong evidence for the importance of colour reconnection in the underlying event.\nC) The study of inclusive jet and exclusive dijet event selections reveals that multiple partonic scattering and QCD radiation contributions to the underlying event are equally important.\nD) The inclusion of the forward rapidity region in the $E_T$ density measurements provides a crucial constraint on the modelling of beam-remnant effects, but does not impact the overall understanding of the underlying event.\n\nCorrect Answer: D) The inclusion of the forward rapidity region in the $E_T$ density measurements provides a crucial constraint on the modelling of beam-remnant effects, but does not impact the overall understanding of the underlying event.\n\nExplanation: The correct answer is D) because the documentation states that the forward rapidity region is included in the $E_T$ density measurements to provide extra data constraints for Monte Carlo modelling of beam-remnant effects. However, it also notes that the standard approach is found to generally reproduce the features of the underlying event in both inclusive jet and exclusive dijet event selections, suggesting that the inclusion of the forward rapidity region does not impact the overall understanding of the underlying event."}, "28": {"documentation": {"title": "Soft spectator scattering in the nucleon form factors at large $Q^2$\n  within the SCET approach", "source": "Nikolai Kivel and Marc Vanderhaeghen", "docs_id": "1010.5314", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Soft spectator scattering in the nucleon form factors at large $Q^2$\n  within the SCET approach. The proton form factors at large momentum transfer are dominated by two contributions which are associated with the hard and soft rescattering respectively. Motivated by a very active experimental form factor program at intermediate values of momentum transfers, $Q^{2}\\sim 5-15 \\text{GeV}^{2}$, where an understanding in terms of only a hard rescattering mechanism cannot yet be expected, we investigate in this work the soft rescattering contribution using soft collinear effective theory (SCET). Within such description, the form factor is characterized, besides the hard scale $Q^2$, by a semi-hard scale $Q \\Lambda$, which arises due to presence of soft spectators, with virtuality $\\Lambda^2$ ($\\Lambda \\sim 0.5$ GeV), such that $Q^{2}\\gg Q\\Lambda\\gg \\Lambda^{2}$. We show that in this case a two-step factorization can be successfully carried out using the SCET approach. In a first step (SCET$_I$), we perform the leading order matching of the QCD electromagnetic current onto the relevant SCET$_I$ operators and perform a resummation of large logarithms using renormalization group equations. We then discuss the further matching onto a SCET$_{II}$ framework, and propose the complete factorization formula for the Dirac form factor, accounting for both hard and soft contributions. We also present a qualitative discussion of the phenomenological consequences of this new framework."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the Soft Collinear Effective Theory (SCET) approach, what is the significance of the semi-hard scale $Q\\Lambda$ in describing the proton form factors at large momentum transfer, and how does it relate to the hard and soft rescattering contributions?\n\nA) The semi-hard scale $Q\\Lambda$ is a measure of the hard scale $Q^2$, and its presence indicates that the form factor is dominated by a hard rescattering mechanism.\n\nB) The semi-hard scale $Q\\Lambda$ arises due to the presence of soft spectators with virtuality $\\Lambda^2$, and it is a characteristic of the soft rescattering contribution.\n\nC) The semi-hard scale $Q\\Lambda$ is a result of the leading order matching of the QCD electromagnetic current onto the relevant SCET operators, and it is used to resum large logarithms.\n\nD) The semi-hard scale $Q\\Lambda$ is a phenomenological parameter that is introduced to account for the soft rescattering contribution, and its value is determined by experimental data.\n\nCorrect Answer: B) The semi-hard scale $Q\\Lambda$ arises due to the presence of soft spectators with virtuality $\\Lambda^2$, and it is a characteristic of the soft rescattering contribution.\n\nExplanation: The semi-hard scale $Q\\Lambda$ is a key concept in the SCET approach, and it arises due to the presence of soft spectators with virtuality $\\Lambda^2$. This scale is a characteristic of the soft rescattering contribution, and it is used to describe the proton form factors at large momentum transfer. The correct answer, B, highlights the significance of the semi-hard scale $Q\\Lambda$ in describing the soft rescattering contribution, which is a crucial aspect of the SCET approach. The other options are incorrect because they either misinterpret the significance of the semi-hard scale $Q\\Lambda$ or introduce it as a phenomenological parameter without explaining its underlying physical significance."}, "29": {"documentation": {"title": "Symmetry Aware Evaluation of 3D Object Detection and Pose Estimation in\n  Scenes of Many Parts in Bulk", "source": "Romain Br\\'egier (Inria), Fr\\'ed\\'eric Devernay (PRIMA, IMAGINE),\n  Laetitia Leyrit (LASMEA), James Crowley", "docs_id": "1806.08129", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Symmetry Aware Evaluation of 3D Object Detection and Pose Estimation in\n  Scenes of Many Parts in Bulk. While 3D object detection and pose estimation has been studied for a long time, its evaluation is not yet completely satisfactory. Indeed, existing datasets typically consist in numerous acquisitions of only a few scenes because of the tediousness of pose annotation, and existing evaluation protocols cannot handle properly objects with symmetries. This work aims at addressing those two points. We first present automatic techniques to produce fully annotated RGBD data of many object instances in arbitrary poses, with which we produce a dataset of thousands of independent scenes of bulk parts composed of both real and synthetic images. We then propose a consistent evaluation methodology suitable for any rigid object, regardless of its symmetries. We illustrate it with two reference object detection and pose estimation methods on different objects, and show that incorporating symmetry considerations into pose estimation methods themselves can lead to significant performance gains. The proposed dataset is available at http://rbregier.github.io/dataset2017."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a new dataset for 3D object detection and pose estimation, as described in the paper \"Symmetry Aware Evaluation of 3D Object Detection and Pose Estimation in Scenes of Many Parts in Bulk\"?\n\n**A)** To improve the accuracy of pose estimation methods for objects with symmetries.\n**B)** To address the tediousness of pose annotation in existing datasets.\n**C)** To provide a comprehensive evaluation framework for 3D object detection and pose estimation methods.\n**D)** To develop automatic techniques for producing fully annotated RGBD data of many object instances in arbitrary poses.\n\n**Correct Answer:** D) To develop automatic techniques for producing fully annotated RGBD data of many object instances in arbitrary poses.\n\n**Explanation:** The correct answer is D) because the paper aims to address the two main issues with existing datasets and evaluation protocols: the tediousness of pose annotation and the inability to handle objects with symmetries. The development of automatic techniques for producing fully annotated RGBD data is a key step towards addressing these issues, as it enables the creation of a large and diverse dataset that can be used to evaluate 3D object detection and pose estimation methods."}, "30": {"documentation": {"title": "New Random Ordered Phase in Isotropic Models with Many-body Interactions", "source": "Yoichiro Hashizume and Masuo Suzuki", "docs_id": "1009.3718", "section": ["cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "New Random Ordered Phase in Isotropic Models with Many-body Interactions. In this study, we have found a new random ordered phase in isotropic models with many-body interactions. Spin correlations between neighboring planes are rigorously shown to form a long-range order, namely coplanar order, using a unitary transformation, and the phase transition of this new order has been analyzed on the bases of the mean-field theory and correlation identities. In the systems with regular 4-body interactions, the transition temperature $T_{\\text{c}}$ is obtained as $T_{\\text{c}}=(z-2)J/k_{\\text{B}}$, and the field conjugate to this new order parameter is found to be $H^2$. In contrast, the corresponding physical quantities in the systems with random 4-body interactions are given by $T_{\\text{c}}=\\sqrt{z-2}J/k_{\\text{B}}$ and $H^4$, respectively. Scaling forms of order parameters for regular or random 4-body interactions are expressed by the same scaling functions in the systems with regular or random 2-body interactions, respectively. Furthermore, we have obtained the nonlinear susceptibilities in the regular and random systems, where the coefficient $\\chi_{\\text{nl}}$ of $H^3$ in the magnetization shows positive divergence in the regular model, while the coefficient $\\chi_{7}$ of $H^7$ in the magnetization shows negative divergence in the random model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the study \"New Random Ordered Phase in Isotropic Models with Many-body Interactions\", what is the difference in the scaling forms of the order parameters between systems with regular and random 4-body interactions, and how does this relate to the corresponding physical quantities in systems with regular and random 2-body interactions?\n\n**A)** The scaling forms of the order parameters are the same for both regular and random 4-body interactions, and the corresponding physical quantities in systems with regular and random 2-body interactions are also the same.\n\n**B)** The scaling forms of the order parameters differ between regular and random 4-body interactions, but the corresponding physical quantities in systems with regular and random 2-body interactions are the same.\n\n**C)** The scaling forms of the order parameters are the same for both regular and random 4-body interactions, but the corresponding physical quantities in systems with regular and random 2-body interactions differ.\n\n**D)** The scaling forms of the order parameters differ between regular and random 4-body interactions, and the corresponding physical quantities in systems with regular and random 2-body interactions also differ, with the former being related to $H^4$ and the latter being related to $H^3$.\n\n**Correct Answer:** D) The scaling forms of the order parameters differ between regular and random 4-body interactions, and the corresponding physical quantities in systems with regular and random 2-body interactions also differ, with the former being related to $H^4$ and the latter being related to $H^3$.\n\n**Explanation:** The study shows that the scaling forms of the order parameters for regular and random 4-body interactions are different, with the former being related to $H^4$ and the latter being related to $H^3$. Additionally, the corresponding physical quantities in systems with regular and random 2-body interactions are also related to $H^4$ and $H^3$, respectively. Therefore, option D is the correct answer."}, "31": {"documentation": {"title": "A Differential Model of the Complex Cell", "source": "Miles Hansard and Radu Horaud", "docs_id": "2012.09027", "section": ["q-bio.NC", "cs.CV", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Differential Model of the Complex Cell. The receptive fields of simple cells in the visual cortex can be understood as linear filters. These filters can be modelled by Gabor functions, or by Gaussian derivatives. Gabor functions can also be combined in an `energy model' of the complex cell response. This paper proposes an alternative model of the complex cell, based on Gaussian derivatives. It is most important to account for the insensitivity of the complex response to small shifts of the image. The new model uses a linear combination of the first few derivative filters, at a single position, to approximate the first derivative filter, at a series of adjacent positions. The maximum response, over all positions, gives a signal that is insensitive to small shifts of the image. This model, unlike previous approaches, is based on the scale space theory of visual processing. In particular, the complex cell is built from filters that respond to the \\twod\\ differential structure of the image. The computational aspects of the new model are studied in one and two dimensions, using the steerability of the Gaussian derivatives. The response of the model to basic images, such as edges and gratings, is derived formally. The response to natural images is also evaluated, using statistical measures of shift insensitivity. The relevance of the new model to the cortical image representation is discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed alternative model of the complex cell, based on Gaussian derivatives, in terms of its insensitivity to small shifts of the image?\n\n**A)** It uses a linear combination of Gabor functions to approximate the first derivative filter at multiple positions.\n**B)** It employs a single position to approximate the first derivative filter at a series of adjacent positions, resulting in a signal insensitive to small shifts.\n**C)** It utilizes the scale space theory of visual processing to build filters that respond to the two-dimensional differential structure of the image.\n**D)** It combines Gaussian derivatives in an \"energy model\" to simulate the complex cell response.\n\n**Correct Answer:** B) It employs a single position to approximate the first derivative filter at a series of adjacent positions, resulting in a signal insensitive to small shifts.\n\n**Explanation:** The correct answer, B, highlights the key innovation of the proposed model, which uses a single position to approximate the first derivative filter at multiple adjacent positions. This approach allows the model to capture the insensitivity of the complex cell response to small shifts of the image. The other options are incorrect because they either describe alternative approaches (A and D) or a related concept (C), but not the primary advantage of the proposed model."}, "32": {"documentation": {"title": "Clique Minors in Cartesian Products of Graphs", "source": "David R. Wood", "docs_id": "0711.1189", "section": ["math.CO", "cs.DM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Clique Minors in Cartesian Products of Graphs. A \"clique minor\" in a graph G can be thought of as a set of connected subgraphs in G that are pairwise disjoint and pairwise adjacent. The \"Hadwiger number\" h(G) is the maximum cardinality of a clique minor in G. This paper studies clique minors in the Cartesian product G*H. Our main result is a rough structural characterisation theorem for Cartesian products with bounded Hadwiger number. It implies that if the product of two sufficiently large graphs has bounded Hadwiger number then it is one of the following graphs: - a planar grid with a vortex of bounded width in the outerface, - a cylindrical grid with a vortex of bounded width in each of the two `big' faces, or - a toroidal grid. Motivation for studying the Hadwiger number of a graph includes Hadwiger's Conjecture, which states that the chromatic number chi(G) <= h(G). It is open whether Hadwiger's Conjecture holds for every Cartesian product. We prove that if |V(H)|-1 >= chi(G) >= chi(H) then Hadwiger's Conjecture holds for G*H. On the other hand, we prove that Hadwiger's Conjecture holds for all Cartesian products if and only if it holds for all G * K_2. We then show that h(G * K_2) is tied to the treewidth of G. We also develop connections with pseudoachromatic colourings and connected dominating sets that imply near-tight bounds on the Hadwiger number of grid graphs (Cartesian products of paths) and Hamming graphs (Cartesian products of cliques)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the relationship between the Hadwiger number of a graph G and the Hadwiger number of its Cartesian product with a complete graph K2, as proven in the paper \"Clique Minors in Cartesian Products of Graphs\"?\n\nA) If the Hadwiger number of G is greater than the Hadwiger number of K2, then the Hadwiger number of G*H is strictly greater than the Hadwiger number of G.\nB) If the Hadwiger number of G*H is greater than the Hadwiger number of G, then the Hadwiger number of K2 is strictly greater than the Hadwiger number of K2.\nC) Hadwiger's Conjecture holds for G*H if and only if Hadwiger's Conjecture holds for G*H and K2.\nD) Hadwiger's Conjecture holds for G*H if and only if the Hadwiger number of G*H is equal to the Hadwiger number of K2.\n\nCorrect Answer: C) Hadwiger's Conjecture holds for G*H if and only if Hadwiger's Conjecture holds for G*H and K2.\n\nExplanation: The paper states that Hadwiger's Conjecture holds for G*H if and only if it holds for G*H and K2. This means that the relationship between the Hadwiger number of G and the Hadwiger number of K2 is crucial in determining whether Hadwiger's Conjecture holds for the Cartesian product G*H. The correct answer reflects this relationship."}, "33": {"documentation": {"title": "Feedback Network Models for Quantum Transport", "source": "John E. Gough", "docs_id": "1408.6991", "section": ["quant-ph", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Feedback Network Models for Quantum Transport. Quantum feedback networks have been introduced in quantum optics as a set of rules for constructing arbitrary networks of quantum mechanical systems connected by uni-directional quantum optical fields, and has allowed for a system theoretic approach to open quantum optics systems. Our aim here is to establish a network theory for quantum transport systems where typically the mediating fields between systems are bi-directional. Mathematically this leads us to study quantum feedback networks where fields arrive at ports in input-output pairs, which is then just a specially case of the uni-directional theory. However, it is conceptually important to develop this theory in the context of quantum transport theory, and the resulting theory extends traditional approaches which tends to view the components in quantum transport as scatterers for the various fields, in the process allows us to consider emission and absorption of field quanta by these components. The quantum feedback network theory is applicable to both Bose and Fermi fields, moreover it applies to nonlinear dynamics for the component systems. In this first paper on the subject, we advance the general theory, but study the case of linear passive quantum components in some detail."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of establishing a network theory for quantum transport systems, and how does it differ from traditional approaches to quantum optics?\n\nA) To develop a theory that views components as scatterers for fields, and to study the case of nonlinear dynamics for component systems.\nB) To create a theory that considers emission and absorption of field quanta by components, and to apply it to both Bose and Fermi fields.\nC) To establish a system theoretic approach to open quantum optics systems, and to study the case of linear passive quantum components in detail.\nD) To develop a theory that views components as emitters and absorbers of fields, and to apply it to nonlinear dynamics for component systems.\n\nCorrect Answer: C) To establish a system theoretic approach to open quantum optics systems, and to study the case of linear passive quantum components in detail.\n\nExplanation: The correct answer is C) because the primary goal of establishing a network theory for quantum transport systems is to develop a system theoretic approach to open quantum optics systems, as stated in the introduction. This approach differs from traditional views of components as scatterers for fields. The correct answer also highlights the focus on linear passive quantum components in the first paper on the subject."}, "34": {"documentation": {"title": "Delta(1232) and the Polarizabilities of the Nucleon", "source": "Thomas R. Hemmert, Barry R. Holstein and Joachim Kambor", "docs_id": "hep-ph/9612374", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Delta(1232) and the Polarizabilities of the Nucleon. Previous calculations of the polarizabilities of the nucleon within the framework of heavy baryon chiral perturbation theory have included the contribution of the $\\Delta$(1232) only its effect on various contact terms or have been performed in chiral SU(3) where systematic errors are difficult to control. Herein we perfrom a corresponding calculation in chiral SU(2) wherein $\\Delta$(1232) is treated as an explicit degree of freedom and the expansion is taken to third order in soft momenta, the pion mass and the quantity $M_\\Delta-M_N$, collectively denoted by $\\epsilon$. We present the results of a systematic $O(\\epsilon^3)$ calculation of forward Compton scattering off the nucleon, extract the electric polarizability $\\bar{\\alpha}_E$, the magnetic polarizability $\\bar{\\beta}_M$ and the spin polarizability $\\gamma$ and compare with available information from experiments and from previous calculations. Concluding with a critical discussion of our results, we point out the necessity of a future $O(\\epsilon^4)$ calculation."}, "answer": "Here is the difficult multi-form question based on the provided documentation:\n\nWhat is the primary motivation behind the authors' calculation of the polarizabilities of the nucleon in the framework of chiral SU(2), and what is the significance of treating the $\\Delta$(1232) as an explicit degree of freedom?\n\nA) To improve the accuracy of the calculation by including the effect of the $\\Delta$(1232) on various contact terms, and to control systematic errors in the chiral SU(3) framework.\nB) To perform a systematic $O(\\epsilon^3)$ calculation of forward Compton scattering off the nucleon, extract the electric polarizability $\\bar{\\alpha}_E$, the magnetic polarizability $\\bar{\\beta}_M$ and the spin polarizability $\\gamma$, and to compare the results with available information from experiments and previous calculations.\nC) To demonstrate the necessity of a future $O(\\epsilon^4)$ calculation, and to point out the limitations of the current $O(\\epsilon^3)$ calculation.\nD) To provide a critical discussion of the results and to highlight the importance of including the $\\Delta$(1232) as an explicit degree of freedom in the chiral SU(2) framework.\n\nCorrect Answer: B) To perform a systematic $O(\\epsilon^3)$ calculation of forward Compton scattering off the nucleon, extract the electric polarizability $\\bar{\\alpha}_E$, the magnetic polarizability $\\bar{\\beta}_M$ and the spin polarizability $\\gamma$, and to compare the results with available information from experiments and previous calculations.\n\nExplanation: The correct answer is B) because the primary motivation behind the authors' calculation is to perform a systematic $O(\\epsilon^3)$ calculation of forward Compton scattering off the nucleon, extract the electric polarizability $\\bar{\\alpha}_E$, the magnetic polarizability $\\bar{\\beta}_M$ and the spin polarizability $\\gamma$, and to compare the results with available information from experiments and previous calculations. The other options are incorrect because they either focus on the limitations of the current calculation (C) or the importance of including the $\\Delta$(1232) as an explicit degree of freedom (D), but do not accurately capture the primary motivation behind the calculation. Option A is also incorrect because it mentions the effect of the $\\Delta$(1232) on various contact terms, but this is not the primary motivation behind the calculation."}, "35": {"documentation": {"title": "Disentangling Identifiable Features from Noisy Data with Structured\n  Nonlinear ICA", "source": "Hermanni H\\\"alv\\\"a, Sylvain Le Corff, Luc Leh\\'ericy, Jonathan So,\n  Yongjie Zhu, Elisabeth Gassiat, Aapo Hyvarinen", "docs_id": "2106.09620", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Disentangling Identifiable Features from Noisy Data with Structured\n  Nonlinear ICA. We introduce a new general identifiable framework for principled disentanglement referred to as Structured Nonlinear Independent Component Analysis (SNICA). Our contribution is to extend the identifiability theory of deep generative models for a very broad class of structured models. While previous works have shown identifiability for specific classes of time-series models, our theorems extend this to more general temporal structures as well as to models with more complex structures such as spatial dependencies. In particular, we establish the major result that identifiability for this framework holds even in the presence of noise of unknown distribution. Finally, as an example of our framework's flexibility, we introduce the first nonlinear ICA model for time-series that combines the following very useful properties: it accounts for both nonstationarity and autocorrelation in a fully unsupervised setting; performs dimensionality reduction; models hidden states; and enables principled estimation and inference by variational maximum-likelihood."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the Structured Nonlinear Independent Component Analysis (SNICA) framework, and how does it extend the identifiability theory of deep generative models?\n\nA) SNICA introduces a new method for disentangling identifiable features from noisy data, but it does not extend the identifiability theory of deep generative models.\nB) SNICA extends the identifiability theory of deep generative models to a very broad class of structured models, including time-series models with complex structures such as spatial dependencies.\nC) SNICA is a new framework for disentangling identifiable features from noisy data, but it does not establish the major result that identifiability holds even in the presence of noise of unknown distribution.\nD) SNICA is a new framework for disentangling identifiable features from noisy data, but it only extends the identifiability theory of deep generative models to specific classes of time-series models.\n\n**Correct Answer:** B) SNICA extends the identifiability theory of deep generative models to a very broad class of structured models, including time-series models with complex structures such as spatial dependencies.\n\n**Explanation:** The correct answer is B because the documentation states that the SNICA framework extends the identifiability theory of deep generative models to a very broad class of structured models, including time-series models with complex structures such as spatial dependencies. This is a key contribution of the SNICA framework, as it allows for principled disentanglement of identifiable features from noisy data in a wide range of applications."}, "36": {"documentation": {"title": "Semantic Labeling of Large-Area Geographic Regions Using Multi-View and\n  Multi-Date Satellite Images and Noisy OSM Training Labels", "source": "Bharath Comandur and Avinash C. Kak", "docs_id": "2008.10271", "section": ["cs.CV", "cs.DC", "cs.LG", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semantic Labeling of Large-Area Geographic Regions Using Multi-View and\n  Multi-Date Satellite Images and Noisy OSM Training Labels. We present a novel multi-view training framework and CNN architecture for combining information from multiple overlapping satellite images and noisy training labels derived from OpenStreetMap (OSM) to semantically label buildings and roads across large geographic regions (100 km$^2$). Our approach to multi-view semantic segmentation yields a 4-7% improvement in the per-class IoU scores compared to the traditional approaches that use the views independently of one another. A unique (and, perhaps, surprising) property of our system is that modifications that are added to the tail-end of the CNN for learning from the multi-view data can be discarded at the time of inference with a relatively small penalty in the overall performance. This implies that the benefits of training using multiple views are absorbed by all the layers of the network. Additionally, our approach only adds a small overhead in terms of the GPU-memory consumption even when training with as many as 32 views per scene. The system we present is end-to-end automated, which facilitates comparing the classifiers trained directly on true orthophotos vis-a-vis first training them on the off-nadir images and subsequently translating the predicted labels to geographical coordinates. With no human supervision, our IoU scores for the buildings and roads classes are 0.8 and 0.64 respectively which are better than state-of-the-art approaches that use OSM labels and that are not completely automated."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is a key advantage of the proposed multi-view training framework and CNN architecture in the Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images and Noisy OSM Training Labels paper?\n\nA) It requires a large amount of human supervision to train the network.\nB) It can only be used for small geographic regions.\nC) It can absorb the benefits of training using multiple views by all layers of the network, with a relatively small penalty in overall performance.\nD) It is limited to using only one type of satellite image.\n\nCorrect Answer: C) It can absorb the benefits of training using multiple views by all layers of the network, with a relatively small penalty in overall performance.\n\nExplanation: The correct answer is C) because the paper states that the proposed system has a unique property where modifications to the tail-end of the CNN for learning from multi-view data can be discarded at the time of inference with a relatively small penalty in overall performance. This implies that the benefits of training using multiple views are absorbed by all layers of the network."}, "37": {"documentation": {"title": "Peculiar Glitch of PSR J1119-6127 and Extension of the Vortex Creep\n  Model", "source": "O. Akbal, E. G\\\"ugercino\\u{g}lu, S. \\c{S}a\\c{s}maz Mu\\c{s}, M.A. Alpar", "docs_id": "1502.03786", "section": ["astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Peculiar Glitch of PSR J1119-6127 and Extension of the Vortex Creep\n  Model. Glitches are sudden changes in rotation frequency and spin-down rate, observed from pulsars of all ages. Standard glitches are characterized by a positive step in angular velocity ($\\Delta\\Omega$ $ > $ $0$) and a negative step in the spin-down rate ($\\Delta \\dot \\Omega$ $ < $ $0$) of the pulsar. There are no glitch-associated changes in the electromagnetic signature of rotation-powered pulsars in all cases so far. For the first time, in the last glitch of PSR J1119-6127, there is clear evidence for changing emission properties coincident with the glitch. This glitch is also unusual in its signature. Further, the absolute value of the spin-down rate actually decreases in the long term. This is in contrast to usual glitch behaviour. In this paper we extend the vortex creep model in order to take into account these peculiarities. We propose that a starquake with crustal plate movement towards the rotational poles of the star induces inward vortex motion which causes the unusual glitch signature. The component of the magnetic field perpendicular to the rotation axis will decrease, giving rise to a permanent change in the pulsar external torque."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism proposed by the authors to explain the peculiar glitch signature of PSR J1119-6127, and how does it differ from the standard glitch behavior?\n\nA) A sudden increase in the magnetic field component perpendicular to the rotation axis, leading to an increase in the pulsar's external torque.\nB) A starquake with crustal plate movement towards the rotational poles of the star induces inward vortex motion, causing a decrease in the component of the magnetic field perpendicular to the rotation axis.\nC) The authors suggest that the glitch is caused by a change in the pulsar's internal magnetic field, leading to a decrease in the spin-down rate.\nD) The authors propose that the glitch is a result of a change in the pulsar's rotation axis, causing a change in the component of the magnetic field perpendicular to the rotation axis.\n\n**Correct Answer:** B) A starquake with crustal plate movement towards the rotational poles of the star induces inward vortex motion, causing a decrease in the component of the magnetic field perpendicular to the rotation axis.\n\n**Explanation:** The correct answer is B) because the authors propose that a starquake with crustal plate movement towards the rotational poles of the star induces inward vortex motion, which causes a decrease in the component of the magnetic field perpendicular to the rotation axis. This is in contrast to the standard glitch behavior, where the spin-down rate decreases, but the absolute value of the spin-down rate actually decreases in the long term. The other options are incorrect because they do not accurately describe the proposed mechanism for the peculiar glitch signature of PSR J1119-6127."}, "38": {"documentation": {"title": "Three-body properties of low-lying $^{12}$Be resonances", "source": "E. Garrido, A. S. Jensen, D. V. Fedorov, J. G. Johansen", "docs_id": "1207.7191", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Three-body properties of low-lying $^{12}$Be resonances. We compute the three-body structure of the lowest resonances of $^{12}$Be considered as two neutrons around an inert $^{10}$Be core. This is an extension of the bound state calculations of $^{12}$Be into the continuum spectrum. We investigate the lowest resonances of angular momenta and parities, $0^{\\pm}$, $1^{-}$ and $2^{+}$. Surprisingly enough, they all are naturally occurring in the three-body model. We calculate bulk structure dominated by small distance properties as well as decays determined by the asymptotic large-distance structure. Both $0^{+}$ and $2^{+}$ have two-body $^{10}$Be-neutron d-wave structure, while $1^{-}$ has an even mixture of $p$ and d-waves. The corresponding relative neutron-neutron partial waves are distributed among $s$, $p$, and d-waves. The branching ratios show different mixtures of one-neutron emission, three-body direct, and sequential decays. We argue for spin and parities, $0^{+}$, $1^{-}$ and $2^{+}$, to the resonances at 0.89, 2.03, 5.13, respectively. The computed structures are in agreement with existing reaction measurements."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the three-body structure calculations of the lowest resonances of $^{12}$Be, and how do the computed structures compare to existing reaction measurements?\n\nA) The three-body model predicts that all resonances have a dominant s-wave structure, which contradicts existing measurements.\nB) The three-body model successfully reproduces the branching ratios and decay modes of the resonances, with a good agreement between the computed and measured structures.\nC) The three-body model predicts that the $0^{+}$ and $2^{+}$ resonances have a dominant d-wave structure, while the $1^{-}$ resonance has a dominant p-wave structure, which is consistent with existing measurements.\nD) The three-body model fails to predict the existence of the $1^{-}$ resonance, which is observed in reaction measurements.\n\nCorrect Answer: C) The three-body model predicts that the $0^{+}$ and $2^{+}$ resonances have a dominant d-wave structure, while the $1^{-}$ resonance has a dominant p-wave structure, which is consistent with existing measurements.\n\nExplanation: The correct answer is based on the text, which states that both $0^{+}$ and $2^{+}$ have two-body $^{10}$Be-neutron d-wave structure, while $1^{-}$ has an even mixture of p and d-waves. This is consistent with the existing reaction measurements, which are mentioned in the text as being in agreement with the computed structures."}, "39": {"documentation": {"title": "Robust path-following control design of heavy vehicles based on\n  multiobjective evolutionary optimization", "source": "Gustavo Alves Prudencio de Morais, Lucas Barbosa Marcos, Filipe\n  Marques Barbosa, Bruno Henrique Groenner Barbosa, Marco Henrique Terra,\n  Valdir Grassi Jr", "docs_id": "2010.07255", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust path-following control design of heavy vehicles based on\n  multiobjective evolutionary optimization. The ability to deal with systems parametric uncertainties is an essential issue for heavy self-driving vehicles in unconfined environments. In this sense, robust controllers prove to be efficient for autonomous navigation. However, uncertainty matrices for this class of systems are usually defined by algebraic methods which demand prior knowledge of the system dynamics. In this case, the control system designer depends, on the quality of the uncertain model to obtain an optimal control performance. This work proposes a robust recursive controller designed via multiobjective optimization to overcome these shortcomings. Furthermore, a local search approach for multiobjective optimization problems is presented. The proposed method applies to any multiobjective evolutionary algorithm already established in the literature. The results presented show that this combination of model-based controller and machine learning improves the effectiveness of the system in terms of robustness, stability and smoothness."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of robust path-following control design for heavy vehicles, what is the primary limitation of traditional uncertainty matrices used to model system dynamics, and how does the proposed method in the paper address this limitation?\n\n**A)** Traditional uncertainty matrices are limited by their inability to capture the complexity of real-world systems, and the proposed method addresses this by using machine learning algorithms to learn the system dynamics.\n\n**B)** Traditional uncertainty matrices are limited by the need for prior knowledge of the system dynamics, and the proposed method addresses this by using a local search approach to optimize the controller design.\n\n**C)** Traditional uncertainty matrices are limited by their inability to handle parametric uncertainties, and the proposed method addresses this by using a multiobjective evolutionary optimization framework to design the controller.\n\n**D)** Traditional uncertainty matrices are limited by their computational complexity, and the proposed method addresses this by using a recursive controller design approach.\n\n**Correct Answer:** C) Traditional uncertainty matrices are limited by their inability to handle parametric uncertainties, and the proposed method addresses this by using a multiobjective evolutionary optimization framework to design the controller.\n\n**Explanation:** The correct answer is C) because the paper highlights the limitation of traditional uncertainty matrices in handling parametric uncertainties, which is a critical issue for heavy self-driving vehicles. The proposed method addresses this limitation by using a multiobjective evolutionary optimization framework, which is a key aspect of the paper's contribution. The other options are incorrect because they either misrepresent the limitation of traditional uncertainty matrices or do not accurately describe the proposed method's approach."}, "40": {"documentation": {"title": "Ternary Quantized Polar Code Decoders: Analysis and Design", "source": "Joachim Neu, Mustafa Cemil Co\\c{s}kun, Gianluigi Liva", "docs_id": "1910.01176", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ternary Quantized Polar Code Decoders: Analysis and Design. The performance of short polar codes under successive cancellation (SC) and SC list (SCL) decoding is analyzed for the case where the decoder messages are coarsely quantized. This setting is of particular interest for applications requiring low-complexity energy-efficient transceivers (e.g., internet-of-things or wireless sensor networks). We focus on the extreme case where the decoder messages are quantized with 3 levels. We show how under SCL decoding quantized log-likelihood ratios lead to a large inaccuracy in the calculation of path metrics, resulting in considerable performance losses with respect to an unquantized SCL decoder. We then introduce two novel techniques which improve the performance of SCL decoding with coarse quantization. The first technique consists of a modification of the final decision step of SCL decoding, where the selected codeword is the one maximizing the maximum-likelihood decoding metric within the final list. The second technique relies on statistical knowledge about the reliability of the bit estimates, obtained through a suitably modified density evolution analysis, to improve the list construction phase, yielding a higher probability of having the transmitted codeword in the list. The effectiveness of the two techniques is demonstrated through simulations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the impact of coarse quantization on the performance of successive cancellation (SC) and successive cancellation list (SCL) decoding in polar codes, and evaluate the effectiveness of the two proposed techniques for improving SCL decoding performance with coarse quantization.\n\n**A)** The performance of SC and SCL decoding with coarse quantization is equivalent to that of unquantized decoding, as the quantization error does not affect the calculation of path metrics.\n\n**B)** The two proposed techniques for improving SCL decoding performance with coarse quantization are equally effective, with the first technique improving performance by 10% and the second technique by 20%.\n\n**C)** The first proposed technique, which modifies the final decision step of SCL decoding, is more effective than the second technique, which relies on statistical knowledge about the reliability of bit estimates, in improving SCL decoding performance with coarse quantization.\n\n**D)** The performance of SCL decoding with coarse quantization is limited by the inaccuracy in the calculation of path metrics, which can be improved by using the second proposed technique, which incorporates statistical knowledge about the reliability of bit estimates.\n\n**Correct Answer:** D) The performance of SCL decoding with coarse quantization is limited by the inaccuracy in the calculation of path metrics, which can be improved by using the second proposed technique, which incorporates statistical knowledge about the reliability of bit estimates.\n\n**Explanation:** The correct answer is D) because the documentation states that quantized log-likelihood ratios lead to a large inaccuracy in the calculation of path metrics, resulting in considerable performance losses with respect to an unquantized SCL decoder. The second proposed technique, which incorporates statistical knowledge about the reliability of bit estimates, is shown to improve the list construction phase, yielding a higher probability of having the transmitted codeword in the list. This is the key insight that allows the second technique to improve SCL decoding performance with coarse quantization."}, "41": {"documentation": {"title": "The class of interacting binaries Double Periodic Variables", "source": "R.E. Mennickent, Z. Kolaczkowski", "docs_id": "0908.3900", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The class of interacting binaries Double Periodic Variables. We introduce the class of intermediate mass binaries named Double Periodic Variables (DPVs), characterized by orbital photometric variability (ellipsoidal or eclipsing) in time scales of few days and a long photometric cycle lasting roughly 33 times the orbital period. After a search conducted in the OGLE and ASAS catalogues, we identified 114 of these systems in the Magellanic Clouds and 11 in the Galaxy. We present results of our photometric and spectroscopic campaigns on DPVs conducted during the last years, outlining their main observational characteristics. We present convincing evidence supporting the view that DPVs are semidetached interacting binaries with optically thick discs around the gainer, that experience regular cycles of mass loss into the interstellar medium. The mechanism regulating this long-term process still is unknown but probably is related to relaxation cycles of the circumprimary disc. A key observational fact is the modulation of the FWHM of HeI 5875 with the long cycle in V393 Sco. The DPV evolution stage is investigated along with their relationship to Algols and W Serpentid stars. We conclude that DPVs can be used to test models of non-conservative binary evolution including the formation of circumbinary discs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the most likely mechanism regulating the long-term process of mass loss in Double Periodic Variables (DPVs), and what observational evidence supports this view?\n\nA) Relaxation cycles of the circumsecondary disc, supported by the modulation of the FWHM of HeI 5875 with the long cycle in V393 Sco.\nB) Tidal interactions between the two stars, resulting in a transfer of angular momentum and a decrease in the orbital period.\nC) The formation of a common envelope around the binary system, leading to a loss of material into the interstellar medium.\nD) The presence of a third, unseen companion star, which is causing the observed mass loss through tidal interactions.\n\nCorrect Answer: A) Relaxation cycles of the circumprimary disc, supported by the modulation of the FWHM of HeI 5875 with the long cycle in V393 Sco.\n\nExplanation: The correct answer is supported by the text, which states that the mechanism regulating the long-term process of mass loss in DPVs \"still is unknown but probably is related to relaxation cycles of the circumprimary disc.\" Additionally, the text mentions that the modulation of the FWHM of HeI 5875 with the long cycle in V393 Sco provides \"convincing evidence\" supporting this view. This suggests that the relaxation cycles of the circumprimary disc are a plausible explanation for the observed mass loss in DPVs."}, "42": {"documentation": {"title": "A new method to obtain risk neutral probability, without stochastic\n  calculus and price modeling, confirms the universal validity of\n  Black-Scholes-Merton formula and volatility's role", "source": "Yannis G. Yatracos", "docs_id": "1304.4929", "section": ["q-fin.PR", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A new method to obtain risk neutral probability, without stochastic\n  calculus and price modeling, confirms the universal validity of\n  Black-Scholes-Merton formula and volatility's role. A new method is proposed to obtain the risk neutral probability of share prices without stochastic calculus and price modeling, via an embedding of the price return modeling problem in Le Cam's statistical experiments framework. Strategies-probabilities $P_{t_0,n}$ and $P_{T,n}$ are thus determined and used, respectively,for the trader selling the share's European call option at time $t_0$ and for the buyer who may exercise it in the future, at $T; \\ n$ increases with the number of share's transactions in $[t_0,T].$ When the transaction times are dense in $[t_0,T]$ it is shown, with mild conditions, that under each of these probabilities $\\log \\frac{S_T}{S_{t_0}}$ has infinitely divisible distribution and in particular normal distribution for \"calm\" share; $S_t$ is the share's price at time $t.$ The price of the share's call is the limit of the expected values of the call's payoff under the translated $P_{t_0,n}.$ It coincides for \"calm\" share prices with the Black-Scholes-Merton formula with variance not necessarily proportional to $(T-t_0),$ thus confirming formula's universal validity without model assumptions. Additional results clarify volatility's role in the transaction and the behaviors of the trader and the buyer. Traders may use the pricing formulae after estimation of the unknown parameters."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the new method to obtain risk-neutral probability, what is the significance of the Le Cam's statistical experiments framework in determining the strategies-probabilities $P_{t_0,n}$ and $P_{T,n}$ for the trader selling the share's European call option at time $t_0$ and for the buyer who may exercise it in the future at $T$, respectively?\n\n**A)** The Le Cam's framework is used to estimate the volatility of the share's price, which is a key input in the Black-Scholes-Merton formula.\n\n**B)** The Le Cam's framework is employed to derive the risk-neutral probability of share prices, which is essential for pricing the share's call option.\n\n**C)** The Le Cam's framework is utilized to analyze the behavior of the trader and the buyer in the transaction, providing insights into their strategies and probabilities.\n\n**D)** The Le Cam's framework is used to validate the universal validity of the Black-Scholes-Merton formula, which is confirmed by the new method to obtain risk-neutral probability.\n\n**Correct Answer:** D) The Le Cam's framework is used to validate the universal validity of the Black-Scholes-Merton formula, which is confirmed by the new method to obtain risk-neutral probability.\n\n**Explanation:** The correct answer is D) because the Le Cam's framework is used to validate the universal validity of the Black-Scholes-Merton formula, which is confirmed by the new method to obtain risk-neutral probability. The framework is employed to show that under each of the probabilities $P_{t_0,n}$ and $P_{T,n}$, the distribution of $\\log \\frac{S_T}{S_{t_0}}$ is infinitely divisible and normal, which is a key result in confirming the universal validity of the Black-Scholes-Merton formula."}, "43": {"documentation": {"title": "A Probabilistic Approach to Floating-Point Arithmetic", "source": "Fredrik Dahlqvist and Rocco Salvia and George A Constantinides", "docs_id": "1912.00867", "section": ["math.NA", "cs.NA", "cs.PL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Probabilistic Approach to Floating-Point Arithmetic. Finite-precision floating point arithmetic unavoidably introduces rounding errors which are traditionally bounded using a worst-case analysis. However, worst-case analysis might be overly conservative because worst-case errors can be extremely rare events in practice. Here we develop a probabilistic model of rounding errors with which it becomes possible to estimate the likelihood that the rounding error of an algorithm lies within a given interval. Given an input distribution, we show how to compute the distribution of rounding errors. We do this exactly for low precision arithmetic, for high precision arithmetic we derive a simple approximation. The model is then entirely compositional: given a numerical program written in a simple imperative programming language we can recursively compute the distribution of rounding errors at each step of the computation and propagate it through each program instruction. This is done by applying a formalism originally developed by Kozen to formalize the semantics of probabilistic programs. We then discuss an implementation of the model and use it to perform probabilistic range analyses on some benchmarks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the probabilistic model of rounding errors for floating-point arithmetic, what is the primary advantage of using a compositional approach to compute the distribution of rounding errors for a numerical program?\n\n**A)** It allows for the direct computation of the distribution of rounding errors for each individual instruction in the program.\n**B)** It enables the propagation of the distribution of rounding errors through each program instruction, taking into account the effects of previous instructions.\n**C)** It provides a way to analyze the worst-case errors for a given program, which is more conservative than the probabilistic approach.\n**D)** It simplifies the implementation of the model by reducing the complexity of the computation.\n\n**Correct Answer:** B) It enables the propagation of the distribution of rounding errors through each program instruction, taking into account the effects of previous instructions.\n\n**Explanation:** The compositional approach to computing the distribution of rounding errors is the primary advantage of this method. By recursively computing the distribution of rounding errors at each step of the computation and propagating it through each program instruction, the model can accurately capture the effects of previous instructions on the rounding errors. This allows for a more precise estimation of the likelihood that the rounding error of an algorithm lies within a given interval."}, "44": {"documentation": {"title": "A Novel Adaptive Channel Allocation Scheme to Handle Handoffs", "source": "Siva Alagu and T. Meyyappan", "docs_id": "1206.3061", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Novel Adaptive Channel Allocation Scheme to Handle Handoffs. Wireless networking is becoming an increasingly important and popular way of providing global information access to users on the move. One of the main challenges for seamless mobility is the availability of simple and robust handoff algorithms, which allow a mobile node to roam among heterogeneous wireless networks. In this paper, the authors devise a scheme, A Novel Adaptive Channel Allocation Scheme (ACAS) where the number of guard channel(s) is adjusted automatically based on the average handoff blocking rate measured in the past certain period of time. The handoff blocking rate is controlled under the designated threshold and the new call blocking rate is minimized. The performance evaluation of the ACAS is done through simulation of nodes. The result shows that the ACAS scheme outperforms the Static Channel Allocation Scheme by controlling a hard constraint on the handoff rejection probability. The proposed scheme achieves the optimal performance by maximizing the resource utilization and adapts itself to changing traffic conditions automatically."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of the A Novel Adaptive Channel Allocation Scheme (ACAS) proposed in the paper, and how does it differ from the Static Channel Allocation Scheme?\n\nA) To minimize the handoff blocking rate by adjusting the number of guard channels dynamically, while maintaining a constant handoff rejection probability.\nB) To maximize resource utilization and adapt to changing traffic conditions by adjusting the number of guard channels automatically, while controlling the handoff blocking rate under a designated threshold.\nC) To minimize the new call blocking rate by adjusting the number of guard channels statically, while maintaining a constant handoff rejection probability.\nD) To maximize the handoff rejection probability by adjusting the number of guard channels dynamically, while minimizing the new call blocking rate.\n\n**Correct Answer:** B) To maximize resource utilization and adapt to changing traffic conditions by adjusting the number of guard channels automatically, while controlling the handoff blocking rate under a designated threshold.\n\n**Explanation:** The correct answer is B) because the ACAS scheme aims to maximize resource utilization and adapt to changing traffic conditions by adjusting the number of guard channels automatically. This is achieved by controlling the handoff blocking rate under a designated threshold, which minimizes the new call blocking rate. The other options are incorrect because they either describe the opposite objective (A and C) or a different approach (D)."}, "45": {"documentation": {"title": "Multifrequency Forcing of a Hopf Oscillator Model of the Inner Ear", "source": "K. A. Montgomery", "docs_id": "0707.4503", "section": ["nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multifrequency Forcing of a Hopf Oscillator Model of the Inner Ear. In response to a sound stimulus, the inner ear emits sounds called otoacoustic emissions. While the exact mechanism for the production of otoacoustic emissions is not known, active motion of individual hair cells is thought to play a role. Two possible sources for otoacoustic emissions, both localized within individual hair cells, include somatic motility and hair bundle motility. Because physiological models of each of these systems are thought to be poised near a Hopf bifurcation, the dynamics of each can be described by the normal form for a system near a Hopf bifurcation. Here we demonstrate that experimental results from three-frequency suppression experiments can be predicted based on the response of an array of noninteracting Hopf oscillators tuned at different frequencies. This supports the idea that active motion of individual hair cells contributes to active processing of sounds in the ear. Interestingly, the model suggests an explanation for differing results recorded in mammals and nonmammals."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study \"Multifrequency Forcing of a Hopf Oscillator Model of the Inner Ear\" propose that the active motion of individual hair cells in the inner ear contributes to the production of otoacoustic emissions. However, the exact mechanism of this contribution is still unclear. Which of the following statements best summarizes the authors' findings?\n\nA) The authors found that the dynamics of individual hair cells can be described by a single-frequency oscillator model, and that this model is sufficient to predict the experimental results.\nB) The authors demonstrated that an array of noninteracting Hopf oscillators tuned at different frequencies can predict the experimental results from three-frequency suppression experiments, supporting the idea that active motion of individual hair cells contributes to active processing of sounds in the ear.\nC) The authors found that the dynamics of individual hair cells can be described by a normal form for a system near a Hopf bifurcation, but that this model is not sufficient to explain the differing results recorded in mammals and nonmammals.\nD) The authors proposed that the active motion of individual hair cells is not necessary for the production of otoacoustic emissions, and that other mechanisms such as passive motion of the hair bundle are sufficient.\n\n**Correct Answer:** B) The authors demonstrated that an array of noninteracting Hopf oscillators tuned at different frequencies can predict the experimental results from three-frequency suppression experiments, supporting the idea that active motion of individual hair cells contributes to active processing of sounds in the ear.\n\n**Explanation:**\n\nThe correct answer, B, is supported by the text, which states that \"Here we demonstrate that experimental results from three-frequency suppression experiments can be predicted based on the response of an array of noninteracting Hopf oscillators tuned at different frequencies.\" This suggests that the authors found that an array of noninteracting Hopf oscillators can predict the experimental results, which supports the idea that active motion of individual hair cells contributes to active processing of sounds in the ear.\n\nThe incorrect answers can be eliminated through careful reading of the text. Answer A is incorrect because the text states that the dynamics of individual hair cells can be described by the normal form for a system near a Hopf bifurcation, not a single-frequency oscillator model. Answer C is incorrect because the text does not mention that the model is not sufficient to explain the differing results recorded in mammals and nonmammals. Answer D is incorrect because the text states that active motion of individual hair cells is thought to play a role in the production of otoacoustic emissions, not that it is not necessary."}, "46": {"documentation": {"title": "Joint Trajectory and Resource Allocation Design for Energy-Efficient\n  Secure UAV Communication Systems", "source": "Yuanxin Cai and Zhiqiang Wei and Ruide Li and Derrick Wing Kwan Ng and\n  Jinhong Yuan", "docs_id": "2003.07028", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Joint Trajectory and Resource Allocation Design for Energy-Efficient\n  Secure UAV Communication Systems. In this paper, we study the trajectory and resource allocation design for downlink energy-efficient secure unmanned aerial vehicle (UAV) communication systems, where an information UAV assisted by a multi-antenna jammer UAV serves multiple ground users in the existence of multiple ground eavesdroppers. The resource allocation strategy and the trajectory of the information UAV, and the jamming policy of the jammer UAV are jointly optimized for maximizing the system energy efficiency. The joint design is formulated as a non-convex optimization problem taking into account the quality of service (QoS) requirement, the security constraint, and the imperfect channel state information (CSI) of the eavesdroppers. The formulated problem is generally intractable. As a compromise approach, the problem is divided into two subproblems which facilitates the design of a low-complexity suboptimal algorithm based on alternating optimization approach. Simulation results illustrate that the proposed algorithm converges within a small number of iterations and demonstrate some interesting insights: (1) the introduction of a jammer UAV facilitates a highly flexible trajectory design of the information UAV which is critical to improving the system energy efficiency; (2) by exploiting the spatial degrees of freedom brought by the multi-antenna jammer UAV, our proposed design can focus the artificial noise on eavesdroppers offering a strong security mean to the system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Joint Trajectory and Resource Allocation Design for Energy-Efficient Secure UAV Communication Systems\", what is the primary objective of the joint optimization problem formulated for the information UAV, jammer UAV, and ground users?\n\n**A)** To maximize the system throughput while ensuring QoS and security constraints.\n**B)** To minimize the system energy consumption while ensuring QoS, security, and imperfect CSI of eavesdroppers.\n**C)** To optimize the trajectory of the information UAV and the jamming policy of the jammer UAV for maximizing system energy efficiency.\n**D)** To design a low-complexity suboptimal algorithm for the joint optimization problem using an alternating optimization approach.\n\n**Correct Answer:** C) To optimize the trajectory of the information UAV and the jamming policy of the jammer UAV for maximizing system energy efficiency.\n\n**Explanation:** The correct answer is C) because the paper states that the joint design is formulated as a non-convex optimization problem to maximize system energy efficiency, taking into account the QoS requirement, security constraint, and imperfect CSI of eavesdroppers. The optimization problem is divided into two subproblems, and the paper proposes a low-complexity suboptimal algorithm based on an alternating optimization approach. The primary objective of the joint optimization problem is to optimize the trajectory of the information UAV and the jamming policy of the jammer UAV to maximize system energy efficiency."}, "47": {"documentation": {"title": "Evaluating language models of tonal harmony", "source": "David R. W. Sears, Filip Korzeniowski, and Gerhard Widmer", "docs_id": "1806.08724", "section": ["cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evaluating language models of tonal harmony. This study borrows and extends probabilistic language models from natural language processing to discover the syntactic properties of tonal harmony. Language models come in many shapes and sizes, but their central purpose is always the same: to predict the next event in a sequence of letters, words, notes, or chords. However, few studies employing such models have evaluated the most state-of-the-art architectures using a large-scale corpus of Western tonal music, instead preferring to use relatively small datasets containing chord annotations from contemporary genres like jazz, pop, and rock. Using symbolic representations of prominent instrumental genres from the common-practice period, this study applies a flexible, data-driven encoding scheme to (1) evaluate Finite Context (or n-gram) models and Recurrent Neural Networks (RNNs) in a chord prediction task; (2) compare predictive accuracy from the best-performing models for chord onsets from each of the selected datasets; and (3) explain differences between the two model architectures in a regression analysis. We find that Finite Context models using the Prediction by Partial Match (PPM) algorithm outperform RNNs, particularly for the piano datasets, with the regression model suggesting that RNNs struggle with particularly rare chord types."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary purpose of language models in the context of tonal harmony, and how do they differ from previous studies that employed such models in Western tonal music?\n\n**A)** Language models are used to predict the next note in a sequence of musical notes, whereas previous studies focused on predicting chord progressions. This difference in focus led to the development of new, more accurate models.\n\n**B)** Language models are used to predict the next event in a sequence of letters, words, or chords, but previous studies employed smaller datasets containing chord annotations from contemporary genres, whereas this study uses a large-scale corpus of Western tonal music.\n\n**C)** Language models are used to predict the next chord in a sequence of chords, but previous studies struggled to accurately model the syntactic properties of tonal harmony, whereas this study uses a flexible, data-driven encoding scheme to evaluate Finite Context models and Recurrent Neural Networks.\n\n**D)** Language models are used to predict the next chord progression in a sequence of chords, but previous studies found that Finite Context models using the Prediction by Partial Match (PPM) algorithm outperform RNNs, particularly for piano datasets.\n\n**Correct Answer:** D) Language models are used to predict the next chord progression in a sequence of chords, but previous studies found that Finite Context models using the Prediction by Partial Match (PPM) algorithm outperform RNNs, particularly for piano datasets.\n\n**Explanation:** This question requires the test-taker to understand the primary purpose of language models in the context of tonal harmony, as stated in the introduction of the study. The correct answer highlights the key finding of the study, which is that Finite Context models using the PPM algorithm outperform RNNs, particularly for piano datasets. The incorrect options (A, B, and C) either misrepresent the primary purpose of language models or introduce irrelevant information, requiring the test-taker to carefully analyze the question and identify the correct answer."}, "48": {"documentation": {"title": "Automatic Calibration of Artificial Neural Networks for Zebrafish\n  Collective Behaviours using a Quality Diversity Algorithm", "source": "Leo Cazenille, Nicolas Bredeche, Jos\\'e Halloy", "docs_id": "1907.09209", "section": ["cs.NE", "cs.AI", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Automatic Calibration of Artificial Neural Networks for Zebrafish\n  Collective Behaviours using a Quality Diversity Algorithm. During the last two decades, various models have been proposed for fish collective motion. These models are mainly developed to decipher the biological mechanisms of social interaction between animals. They consider very simple homogeneous unbounded environments and it is not clear that they can simulate accurately the collective trajectories. Moreover when the models are more accurate, the question of their scalability to either larger groups or more elaborate environments remains open. This study deals with learning how to simulate realistic collective motion of collective of zebrafish, using real-world tracking data. The objective is to devise an agent-based model that can be implemented on an artificial robotic fish that can blend into a collective of real fish. We present a novel approach that uses Quality Diversity algorithms, a class of algorithms that emphasise exploration over pure optimisation. In particular, we use CVT-MAP-Elites, a variant of the state-of-the-art MAP-Elites algorithm for high dimensional search space. Results show that Quality Diversity algorithms not only outperform classic evolutionary reinforcement learning methods at the macroscopic level (i.e. group behaviour), but are also able to generate more realistic biomimetic behaviours at the microscopic level (i.e. individual behaviour)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of the proposed agent-based model in the study \"Automatic Calibration of Artificial Neural Networks for Zebrafish Collective Behaviours using a Quality Diversity Algorithm\"?\n\n**A)** To develop a model that can accurately simulate the collective motion of a large group of zebrafish in a simple homogeneous environment.\n**B)** To create an artificial robotic fish that can blend into a collective of real fish and simulate realistic collective motion.\n**C)** To devise an agent-based model that can be implemented on an artificial robotic fish to learn how to simulate realistic collective motion of a small group of zebrafish in a complex environment.\n**D)** To use Quality Diversity algorithms to optimize the performance of a classic evolutionary reinforcement learning method for simulating collective motion.\n\n**Correct Answer:** C) To devise an agent-based model that can be implemented on an artificial robotic fish to learn how to simulate realistic collective motion of a small group of zebrafish in a complex environment.\n\n**Explanation:** The correct answer is C) because the study aims to develop an agent-based model that can be implemented on an artificial robotic fish to learn how to simulate realistic collective motion of a small group of zebrafish in a complex environment. The model is designed to learn from real-world tracking data and use Quality Diversity algorithms to optimize its performance. The other options are incorrect because they do not accurately reflect the primary objective of the proposed model. Option A is incorrect because the study focuses on simulating collective motion in a complex environment, not a simple homogeneous environment. Option B is incorrect because the study aims to develop an agent-based model, not create an artificial robotic fish. Option D is incorrect because the study uses Quality Diversity algorithms to optimize the performance of the agent-based model, not to optimize a classic evolutionary reinforcement learning method."}, "49": {"documentation": {"title": "Ensemble of ACCDOA- and EINV2-based Systems with D3Nets and Impulse\n  Response Simulation for Sound Event Localization and Detection", "source": "Kazuki Shimada, Naoya Takahashi, Yuichiro Koyama, Shusuke Takahashi,\n  Emiru Tsunoo, Masafumi Takahashi, Yuki Mitsufuji", "docs_id": "2106.10806", "section": ["eess.AS", "cs.SD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ensemble of ACCDOA- and EINV2-based Systems with D3Nets and Impulse\n  Response Simulation for Sound Event Localization and Detection. This report describes our systems submitted to the DCASE2021 challenge task 3: sound event localization and detection (SELD) with directional interference. Our previous system based on activity-coupled Cartesian direction of arrival (ACCDOA) representation enables us to solve a SELD task with a single target. This ACCDOA-based system with efficient network architecture called RD3Net and data augmentation techniques outperformed state-of-the-art SELD systems in terms of localization and location-dependent detection. Using the ACCDOA-based system as a base, we perform model ensembles by averaging outputs of several systems trained with different conditions such as input features, training folds, and model architectures. We also use the event independent network v2 (EINV2)-based system to increase the diversity of the model ensembles. To generalize the models, we further propose impulse response simulation (IRS), which generates simulated multi-channel signals by convolving simulated room impulse responses (RIRs) with source signals extracted from the original dataset. Our systems significantly improved over the baseline system on the development dataset."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using ensemble methods, specifically model ensembles and impulse response simulation, in the proposed system for sound event localization and detection (SELD) in the DCASE2021 challenge task 3?\n\nA) Improved localization accuracy through data augmentation techniques\nB) Enhanced detection performance by averaging outputs of multiple systems\nC) Increased robustness to directional interference through ensemble methods and IRS\nD) Reduced computational complexity through efficient network architectures\n\nCorrect Answer: C) Increased robustness to directional interference through ensemble methods and IRS\n\nExplanation: The correct answer, C, highlights the key benefit of using ensemble methods and impulse response simulation in the proposed system. By combining multiple systems and simulating room impulse responses, the system can better handle directional interference and improve its overall performance in SELD tasks. The other options, while related to the topic, do not accurately capture the primary advantage of the proposed system. Option A mentions data augmentation techniques, which are used in the ACCDOA-based system, but are not the primary advantage of the ensemble methods. Option B is incorrect because the ensemble methods are used to combine the outputs of multiple systems, not to improve detection performance. Option D is also incorrect because the proposed system does not aim to reduce computational complexity, but rather to improve robustness to directional interference."}, "50": {"documentation": {"title": "Integrating Secure and High-Speed Communications into Frequency Hopping\n  MIMO Radar", "source": "Kai Wu, J. Andrew Zhang, Xiaojing Huang, Y. Jay Guo", "docs_id": "2009.13750", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Integrating Secure and High-Speed Communications into Frequency Hopping\n  MIMO Radar. Dual-function radar-communication (DFRC) based on frequency hopping (FH) MIMO radar (FH-MIMO DFRC) achieves symbol rate much higher than radar pulse repetition frequency. Such DFRC, however, is prone to eavesdropping due to the spatially uniform illumination of FH-MIMO radar. How to enhance the physical layer security of FH-MIMO DFRC is vital yet unsolved. In this paper, we reveal the potential of using permutations of hopping frequencies to achieve secure and high-speed FH-MIMO DFRC. Detecting permutations at a communication user is challenging due to the dependence on spatial angle. We propose a series of baseband waveform processing methods which address the challenge specifically for the legitimate user (Bob) and meanwhile scrambles constellations almost omnidirectionally. We discover a deterministic sign rule from the signals processed by the proposed methods. Based on the rule, we develop accurate algorithms for information decoding at Bob. Confirmed by simulation, our design achieves substantially high physical layer security for FH-MIMO DFRC, improves decoding performance compared with existing designs and reduces mutual interference among radar targets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in enhancing the physical layer security of Dual-Function Radar-Communication (DFRC) based on Frequency Hopping (FH) Multiple-Input Multiple-Output (MIMO) Radar, and how does the proposed solution address this challenge?\n\n**A)** The primary challenge is the high symbol rate of FH-MIMO DFRC, which makes it difficult to detect permutations of hopping frequencies. The proposed solution addresses this challenge by using permutations of hopping frequencies to scramble constellations almost omnidirectionally.\n\n**B)** The primary challenge is the dependence of detecting permutations on spatial angle, which makes it difficult for legitimate users to distinguish between legitimate and eavesdropped signals. The proposed solution addresses this challenge by developing accurate algorithms for information decoding at legitimate users.\n\n**C)** The primary challenge is the mutual interference among radar targets, which reduces the decoding performance of FH-MIMO DFRC. The proposed solution addresses this challenge by reducing mutual interference among radar targets through the use of permutations of hopping frequencies.\n\n**D)** The primary challenge is the high computational complexity of processing baseband waveforms, which hinders the development of accurate algorithms for information decoding. The proposed solution addresses this challenge by proposing a series of baseband waveform processing methods that address the challenge specifically for legitimate users.\n\n**Correct Answer:** B) The primary challenge is the dependence of detecting permutations on spatial angle, which makes it difficult for legitimate users to distinguish between legitimate and eavesdropped signals. The proposed solution addresses this challenge by developing accurate algorithms for information decoding at legitimate users.\n\n**Explanation:** The correct answer is B) because the documentation states that detecting permutations at a communication user is challenging due to the dependence on spatial angle. The proposed solution addresses this challenge by developing accurate algorithms for information decoding at legitimate users, which is mentioned in the documentation as a key aspect of the proposed design."}, "51": {"documentation": {"title": "Image quality enhancement in wireless capsule endoscopy with adaptive\n  fraction gamma transformation and unsharp masking filter", "source": "Rezvan Ezatian (1), Donya Khaledyan (2), Kian Jafari (1), Morteza\n  Heidari (2), Abolfazl Zargari Khuzani (3), Najmeh Mashhadi (4) ((1) Faculty\n  of Electrical Engineering, Shahid Beheshti University, Tehran, Iran, (2)\n  School of Electrical & Computer Engineering, University of Oklahoma, Norman,\n  USA, (3) The Department of Electrical and Computer Engineering, University of\n  California, Santa Cruz, USA, (4) The Department of Computer Science and\n  Engineering, University of California, Santa Cruz, USA)", "docs_id": "2009.12631", "section": ["eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Image quality enhancement in wireless capsule endoscopy with adaptive\n  fraction gamma transformation and unsharp masking filter. Wireless Capsule Endoscopy (WCE) presented in 2001 as one of the key approaches to observe the entire gastrointestinal (GI) tract, generally the small bowels. It has been used to detect diseases in the gastrointestinal tract. Endoscopic image analysis is still a required field with many open problems. The quality of many images it produced is rather unacceptable due to the nature of this imaging system, which causes some issues to prognosticate by physicians and computer-aided diagnosis. In this paper, a novel technique is proposed to improve the quality of images captured by the WCE. More specifically, it enhanced the brightness, contrast, and preserve the color information while reducing its computational complexity. Furthermore, the experimental results of PSNR and SSIM confirm that the error rate in this method is near to the ground and negligible. Moreover, the proposed method improves intensity restricted average local entropy (IRMLE) by 22%, color enhancement factor (CEF) by 10%, and can keep the lightness of image effectively. The performances of our method have better visual quality and objective assessments in compare to the state-of-art methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed technique in improving the quality of images captured by Wireless Capsule Endoscopy (WCE), and what are some of the key benefits of this method?\n\n**A)** The primary goal is to enhance the color information of the images while reducing computational complexity, and the benefits include improved visual quality and objective assessments.\n\n**B)** The primary goal is to improve the brightness and contrast of the images while preserving the color information, and the benefits include a 22% improvement in intensity restricted average local entropy (IRMLE) and a 10% improvement in color enhancement factor (CEF).\n\n**C)** The primary goal is to reduce the computational complexity of the image processing algorithm while maintaining the lightness of the image, and the benefits include improved visual quality and objective assessments.\n\n**D)** The primary goal is to enhance the brightness and contrast of the images while preserving the color information, and the benefits include a 22% improvement in IRLME and a 10% improvement in CEF.\n\n**Correct Answer:** B) The primary goal is to improve the brightness and contrast of the images while preserving the color information, and the benefits include a 22% improvement in intensity restricted average local entropy (IRMLE) and a 10% improvement in color enhancement factor (CEF).\n\n**Explanation:** The correct answer is B) because the primary goal of the proposed technique is to improve the brightness and contrast of the images while preserving the color information, as stated in the introduction of the paper. Additionally, the benefits of this method include a 22% improvement in IRLME and a 10% improvement in CEF, which are mentioned in the experimental results section of the paper."}, "52": {"documentation": {"title": "Prospects of inflation in delicate D-brane cosmology", "source": "Sudhakar Panda, M. Sami, Shinji Tsujikawa", "docs_id": "0707.2848", "section": ["hep-th", "astro-ph", "gr-qc", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prospects of inflation in delicate D-brane cosmology. We study D-brane inflation in a warped conifold background that includes brane-position dependent corrections for the nonperturbative superpotential. Instead of stabilizing the volume modulus chi at instantaneous minima of the potential and studying the inflation dynamics with an effective single field (radial distance between a brane and an anti-brane) phi, we investigate the multi-field inflation scenario involving these two fields. The two-field dynamics with the potential V(phi,chi) in this model is significantly different from the effective single-field description in terms of the field phi when the field chi is integrated out. The latter picture underestimates the total number of e-foldings even by one order of magnitude. We show that a correct single-field description is provided by a field psi obtained from a rotation in the two-field space along the background trajectory. This model can give a large number of e-foldings required to solve flatness and horizon problems at the expense of fine-tunings of model parameters. We also estimate the spectra of density perturbations and show that the slow-roll parameter eta_{psi psi}=M_{pl}^2 V_{,psi psi}/V in terms of the rotated field psi determines the spectral index of scalar metric perturbations. We find that it is generally difficult to satisfy, simultaneously, both constraints of the spectral index and the COBE normalization, while the tensor to scalar ratio is sufficiently small to match with observations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of D-brane cosmology, what is the relationship between the slow-roll parameter \u03b7_{\u03c8\u03c8} and the spectral index of scalar metric perturbations, and how does this relate to the constraints of the spectral index and the COBE normalization?\n\nA) \u03b7_{\u03c8\u03c8} is directly proportional to the spectral index, and a large value of \u03b7_{\u03c8\u03c8} is required to satisfy both constraints.\n\nB) \u03b7_{\u03c8\u03c8} is inversely proportional to the spectral index, and a small value of \u03b7_{\u03c8\u03c8} is required to satisfy both constraints.\n\nC) \u03b7_{\u03c8\u03c8} is a measure of the model's ability to reproduce the observed spectral index, and a value of \u03b7_{\u03c8\u03c8} = 0 would imply a perfect match with observations.\n\nD) \u03b7_{\u03c8\u03c8} is a measure of the model's ability to reproduce the observed tensor to scalar ratio, and a value of \u03b7_{\u03c8\u03c8} = 1 would imply a perfect match with observations.\n\nCorrect Answer: B) \u03b7_{\u03c8\u03c8} is inversely proportional to the spectral index, and a small value of \u03b7_{\u03c8\u03c8} is required to satisfy both constraints.\n\nExplanation: The correct answer is B) \u03b7_{\u03c8\u03c8} is inversely proportional to the spectral index, and a small value of \u03b7_{\u03c8\u03c8} is required to satisfy both constraints. This is because the slow-roll parameter \u03b7_{\u03c8\u03c8} is defined as M_{pl}^2 V_{,\u03c8\u03c8}/V, where V is the potential energy density. In the context of D-brane cosmology, the potential energy density V is related to the spectral index of scalar metric perturbations. A small value of \u03b7_{\u03c8\u03c8} is required to satisfy both constraints of the spectral index and the COBE normalization, which is a measure of the model's ability to reproduce the observed density perturbations."}, "53": {"documentation": {"title": "Attack Agnostic Statistical Method for Adversarial Detection", "source": "Sambuddha Saha, Aashish Kumar, Pratyush Sahay, George Jose, Srinivas\n  Kruthiventi, Harikrishna Muralidhara", "docs_id": "1911.10008", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Attack Agnostic Statistical Method for Adversarial Detection. Deep Learning based AI systems have shown great promise in various domains such as vision, audio, autonomous systems (vehicles, drones), etc. Recent research on neural networks has shown the susceptibility of deep networks to adversarial attacks - a technique of adding small perturbations to the inputs which can fool a deep network into misclassifying them. Developing defenses against such adversarial attacks is an active research area, with some approaches proposing robust models that are immune to such adversaries, while other techniques attempt to detect such adversarial inputs. In this paper, we present a novel statistical approach for adversarial detection in image classification. Our approach is based on constructing a per-class feature distribution and detecting adversaries based on comparison of features of a test image with the feature distribution of its class. For this purpose, we make use of various statistical distances such as ED (Energy Distance), MMD (Maximum Mean Discrepancy) for adversarial detection, and analyze the performance of each metric. We experimentally show that our approach achieves good adversarial detection performance on MNIST and CIFAR-10 datasets irrespective of the attack method, sample size and the degree of adversarial perturbation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary approach used in the paper \"Attack Agnostic Statistical Method for Adversarial Detection\" for detecting adversarial inputs in image classification?\n\n**A)** Constructing a per-class feature distribution and comparing the features of a test image with the feature distribution of its class using statistical distances such as ED (Energy Distance) and MMD (Maximum Mean Discrepancy).\n\n**B)** Training a robust model that is immune to adversarial attacks and then classifying the test images.\n\n**C)** Using a neural network to learn the distribution of adversarial inputs and then detecting them based on the learned distribution.\n\n**D)** Applying a pre-processing technique to the input images to remove noise and then classifying them.\n\n**Correct Answer:** A) Constructing a per-class feature distribution and comparing the features of a test image with the feature distribution of its class using statistical distances such as ED (Energy Distance) and MMD (Maximum Mean Discrepancy).\n\n**Explanation:** The correct answer is A) because the paper presents a novel statistical approach for adversarial detection in image classification, which involves constructing a per-class feature distribution and detecting adversaries based on comparison of features of a test image with the feature distribution of its class. The use of statistical distances such as ED and MMD is also a key aspect of this approach. The other options are incorrect because they do not accurately describe the primary approach used in the paper."}, "54": {"documentation": {"title": "Yang-Baxter deformations of Minkowski spacetime", "source": "Takuya Matsumoto, Domenico Orlando, Susanne Reffert, Jun-ichi\n  Sakamoto, Kentaroh Yoshida", "docs_id": "1505.04553", "section": ["hep-th", "gr-qc", "math-ph", "math.MP", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Yang-Baxter deformations of Minkowski spacetime. We study Yang-Baxter deformations of 4D Minkowski spacetime. The Yang-Baxter sigma model description was originally developed for principal chiral models based on a modified classical Yang-Baxter equation. It has been extended to coset curved spaces and models based on the usual classical Yang-Baxter equation. On the other hand, for flat space, there is the obvious problem that the standard bilinear form degenerates if we employ the familiar coset Poincar\\'e group/Lorentz group. Instead we consider a slice of AdS$_5$ by embedding the 4D Poincar\\'e group into the 4D conformal group $SO(2,4)$. With this procedure we obtain metrics and $B$-fields as Yang-Baxter deformations which correspond to well-known configurations such as T-duals of Melvin backgrounds, Hashimoto-Sethi and Spradlin-Takayanagi-Volovich backgrounds, the T-dual of Grant space, pp-waves, and T-duals of dS$_4$ and AdS$_4$. Finally we consider a deformation with a classical $r$-matrix of Drinfeld-Jimbo type and explicitly derive the associated metric and $B$-field which we conjecture to correspond to a new integrable system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the Yang-Baxter deformations of 4D Minkowski spacetime as described in the paper \"Yang-Baxter deformations of Minkowski spacetime\". What is the correct form of the classical $r$-matrix for the Drinfeld-Jimbo type deformation, and what are the associated metric and $B$-field that correspond to a new integrable system?\n\nA) $r = \\sum_{i=1}^{4} \\frac{1}{i} \\partial_{i} \\otimes \\partial_{i}$, with metric $ds^2 = -\\frac{1}{2} dt^2 + \\frac{1}{2} dx^2 + \\frac{1}{2} dy^2 + \\frac{1}{2} dz^2 + \\frac{1}{2} dw^2$ and $B$-field $B_{ij} = \\frac{1}{2} \\epsilon_{ijk} \\epsilon_{klm} dx^k \\wedge dx^l \\wedge dx^m \\wedge dx^n$\n\nB) $r = \\sum_{i=1}^{4} \\frac{1}{i} \\partial_{i} \\otimes \\partial_{i}$, with metric $ds^2 = -\\frac{1}{2} dt^2 + \\frac{1}{2} dx^2 + \\frac{1}{2} dy^2 + \\frac{1}{2} dz^2 + \\frac{1}{2} dw^2$ and $B$-field $B_{ij} = \\frac{1}{2} \\epsilon_{ijk} \\epsilon_{klm} dx^k \\wedge dx^l \\wedge dx^m \\wedge dx^n$\n\nC) $r = \\sum_{i=1}^{4} \\frac{1}{i} \\partial_{i} \\otimes \\partial_{i}$, with metric $ds^2 = -\\frac{1}{2} dt^2 + \\frac{1}{2} dx^2 + \\frac{1}{2} dy^2 + \\frac{1}{2} dz^2 + \\frac{1}{2} dw^2$ and $B$-field $B_{ij} = \\frac{1}{2} \\epsilon_{ijk} \\epsilon_{klm} dx^k \\wedge dx^l \\wedge dx^m \\wedge dx^n$\n\nD) $r = \\sum_{i=1}^{4} \\frac{1}{i} \\partial_{i} \\otimes \\partial_{i}$, with metric $ds^2 = -\\frac{1}{2} dt^2 + \\frac{1}{2} dx^2 + \\frac{1}{2} dy^2 + \\frac{1}{2} dz^2 + \\frac{1}{2} dw^2$ and $B$-field $B_{ij} = \\frac{1}{2} \\epsilon_{ijk} \\epsilon_{klm} dx^k \\wedge dx^l \\wedge dx^m \\wedge dx^n$\n\nCorrect Answer: A) $r = \\sum_{i=1}^{4} \\frac{1}{i} \\partial_{i} \\otimes \\partial_{i}$, with metric $ds^2 = -\\frac{1}{2} dt^2 + \\frac{1}{2} dx^2 + \\frac{1}{2} dy^2 + \\frac{1}{2} dz^2 + \\frac{1}{2} dw^2$ and $B$-field $B_{ij} = \\frac{1}{2} \\epsilon_{ijk} \\epsilon_{klm} dx^k \\wedge dx^l \\wedge dx^m \\wedge dx^n$\n\nExplanation: The correct answer is A) because the classical $r$-matrix for the Drinfeld-Jimbo type deformation is given by $r = \\sum_{i=1}^{4} \\frac{1}{i} \\partial_{i} \\otimes \\partial_{i}$. The associated metric and $B$-field are then given by $ds^2 = -\\frac{1}{2} dt^2 + \\frac{1}{2} dx^2 + \\frac{1}{2} dy^2 + \\frac{1}{2} dz^2 + \\frac{1}{2} dw^2$ and $B_{ij} = \\frac{1}{2} \\epsilon_{ijk} \\epsilon_{klm} dx^k \\wedge dx^l \\wedge dx^m \\wedge dx^n$, respectively. These expressions correspond to a new integrable system, as stated in the paper."}, "55": {"documentation": {"title": "Ioffe-Regel criterion of Anderson localization in the model of resonant\n  point scatterers", "source": "S.E. Skipetrov and I.M. Sokolov", "docs_id": "1803.11479", "section": ["cond-mat.dis-nn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ioffe-Regel criterion of Anderson localization in the model of resonant\n  point scatterers. We establish a phase diagram of a model in which scalar waves are scattered by resonant point scatterers pinned at random positions in the free three-dimensional (3D) space. A transition to Anderson localization takes place in a narrow frequency band near the resonance frequency provided that the number density of scatterers $\\rho$ exceeds a critical value $\\rho_c \\simeq 0.08 k_0^{3}$, where $k_0$ is the wave number in the free space. The localization condition $\\rho > \\rho_c$ can be rewritten as $k_0 \\ell_0 < 1$, where $\\ell_0$ is the on-resonance mean free path in the independent-scattering approximation. At mobility edges, the decay of the average amplitude of a monochromatic plane wave is not purely exponential and the growth of its phase is nonlinear with the propagation distance. This makes it impossible to define the mean free path $\\ell$ and the effective wave number $k$ in a usual way. If the latter are defined as an effective decay length of the intensity and an effective growth rate of the phase of the average wave field, the Ioffe-Regel parameter $(k\\ell)_c$ at the mobility edges can be calculated and takes values from 0.3 to 1.2 depending on $\\rho$. Thus, the Ioffe-Regel criterion of localization $k\\ell < (k\\ell)_c = \\mathrm{const} \\sim 1$ is valid only qualitatively and cannot be used as a quantitative condition of Anderson localization in 3D."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the correct interpretation of the Ioffe-Regel criterion of Anderson localization in the context of resonant point scatterers in a 3D space?\n\nA) The Ioffe-Regel criterion is a quantitative condition for Anderson localization, valid for all values of the number density of scatterers \u03c1.\nB) The Ioffe-Regel criterion is a qualitative condition for Anderson localization, indicating the existence of mobility edges, but not providing a precise threshold for localization.\nC) The Ioffe-Regel criterion is a measure of the effective wave number k and mean free path \u2113 in the independent-scattering approximation, and can be used to define a quantitative condition for localization.\nD) The Ioffe-Regel criterion is a parameter that depends on the Ioffe-Regel parameter (k\u2113)c, which is a constant value between 0.3 and 1.2, and can be used to determine the critical value of \u03c1 for localization.\n\nCorrect Answer: B) The Ioffe-Regel criterion is a qualitative condition for Anderson localization, indicating the existence of mobility edges, but not providing a precise threshold for localization.\n\nExplanation: The Ioffe-Regel criterion is a qualitative condition that indicates the existence of mobility edges, but it does not provide a precise threshold for localization. The localization condition \u03c1 > \u03c1c can be rewritten as k0\u21130 < 1, but the Ioffe-Regel parameter (k\u2113)c at the mobility edges is a constant value between 0.3 and 1.2, depending on \u03c1. This means that the Ioffe-Regel criterion is not a quantitative condition for localization, but rather a qualitative indicator of the existence of mobility edges."}, "56": {"documentation": {"title": "Parameter-free methods distinguish Wnt pathway models and guide design\n  of experiments", "source": "Adam L. MacLean, Zvi Rosen, Helen M. Byrne, Heather A. Harrington", "docs_id": "1409.0269", "section": ["q-bio.QM", "math.AG", "q-bio.MN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Parameter-free methods distinguish Wnt pathway models and guide design\n  of experiments. The canonical Wnt signaling pathway, mediated by $\\beta$-catenin, is crucially involved in development, adult stem cell tissue maintenance and a host of diseases including cancer. We undertake analysis of different mathematical models of Wnt from the literature, and compare them to a new mechanistic model of Wnt signaling that targets spatial localization of key molecules. Using Bayesian methods we infer parameters for each of the models to mammalian Wnt signaling data and find that all models can fit this time course. We are able to overcome this lack of data by appealing to algebraic methods (concepts from chemical reaction network theory and matroid theory) to analyze the models without recourse to specific parameter values. These approaches provide insight into Wnt signaling: The new model (unlike any other investigated) permits a bistable switch in the system via control of shuttling and degradation parameters, corresponding to stem-like vs committed cell states in the differentiation hierarchy. Our analysis also identifies groups of variables that must be measured to fully characterize and discriminate between competing models, and thus serves as a guide for performing minimal experiments for model comparison."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the Wnt signaling pathway models presented in the study, and explain the significance of the new mechanistic model that targets spatial localization of key molecules. How do the algebraic methods used to analyze the models contribute to the understanding of Wnt signaling?\n\n**A)** The new model is significant because it provides a novel mechanism for regulating the Wnt signaling pathway, allowing for the identification of key molecules involved in stem-like and committed cell states. However, the algebraic methods used to analyze the models are not crucial to understanding Wnt signaling.\n\n**B)** The new model is significant because it permits a bistable switch in the system via control of shuttling and degradation parameters, corresponding to stem-like vs committed cell states in the differentiation hierarchy. The algebraic methods used to analyze the models are essential in identifying the groups of variables that must be measured to fully characterize and discriminate between competing models.\n\n**C)** The new model is significant because it targets spatial localization of key molecules, allowing for a better understanding of Wnt signaling in different cellular contexts. However, the algebraic methods used to analyze the models are not directly related to the understanding of Wnt signaling.\n\n**D)** The new model is significant because it provides a novel mechanism for regulating the Wnt signaling pathway, allowing for the identification of key molecules involved in stem-like and committed cell states. However, the algebraic methods used to analyze the models are not necessary to understand Wnt signaling, as the models can be fit to mammalian Wnt signaling data without them.\n\n**Correct Answer:** B) The new model is significant because it permits a bistable switch in the system via control of shuttling and degradation parameters, corresponding to stem-like vs committed cell states in the differentiation hierarchy. The algebraic methods used to analyze the models are essential in identifying the groups of variables that must be measured to fully characterize and discriminate between competing models.\n\n**Explanation:** The correct answer highlights the significance of the new mechanistic model, which introduces a bistable switch in the system via control of shuttling and degradation parameters. This allows for the identification of key molecules involved in stem-like and committed cell states in the differentiation hierarchy. The algebraic methods used to analyze the models are essential in identifying the groups of variables that must be measured to fully characterize and discriminate between competing models, which is a crucial step in guiding the design of experiments."}, "57": {"documentation": {"title": "Optimal Content Placement for Offloading in Cache-enabled Heterogeneous\n  Wireless Networks", "source": "Dong Liu and Chenyang Yang", "docs_id": "1604.03280", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Content Placement for Offloading in Cache-enabled Heterogeneous\n  Wireless Networks. Caching at base stations (BSs) is a promising way to offload traffic and eliminate backhaul bottleneck in heterogeneous networks (HetNets). In this paper, we investigate the optimal content placement maximizing the successful offloading probability in a cache-enabled HetNet where a tier of multi-antenna macro BSs (MBSs) is overlaid with a tier of helpers with caches. Based on probabilistic caching framework, we resort to stochastic geometry theory to derive the closed-form successful offloading probability and formulate the caching probability optimization problem, which is not concave in general. In two extreme cases with high and low user-to-helper density ratios, we obtain the optimal caching probability and analyze the impacts of BS density and transmit power of the two tiers and the signal-to-interference-plus-noise ratio (SINR) threshold. In general case, we obtain the optimal caching probability that maximizes the lower bound of successful offloading probability and analyze the impact of user density. Simulation and numerical results show that when the ratios of MBS-to-helper density, MBS-to-helper transmit power and user-to-helper density, and the SINR threshold are large, the optimal caching policy tends to cache the most popular files everywhere."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Optimal Content Placement for Offloading in Cache-enabled Heterogeneous Wireless Networks paper, what is the primary goal of the caching probability optimization problem, and how does it relate to the successful offloading probability?\n\n**A)** The primary goal is to minimize the caching probability to maximize the successful offloading probability, and this is achieved by caching the most popular files in the most densely populated areas.\n\n**B)** The primary goal is to maximize the caching probability to maximize the successful offloading probability, and this is achieved by caching the most popular files in the areas with the highest user-to-helper density ratios.\n\n**C)** The primary goal is to maximize the caching probability to maximize the successful offloading probability, and this is achieved by caching the most popular files in the areas with the highest signal-to-interference-plus-noise ratio (SINR) thresholds.\n\n**D)** The primary goal is to minimize the caching probability to maximize the successful offloading probability, and this is achieved by caching the most popular files in the areas with the lowest user-to-helper density ratios.\n\n**Correct Answer:** B) The primary goal is to maximize the caching probability to maximize the successful offloading probability, and this is achieved by caching the most popular files in the areas with the highest user-to-helper density ratios.\n\n**Explanation:** The correct answer is B) because the paper states that the optimal caching probability is obtained by maximizing the lower bound of successful offloading probability, and this is achieved by caching the most popular files in the areas with the highest user-to-helper density ratios. This is evident from the statement \"In general case, we obtain the optimal caching probability that maximizes the lower bound of successful offloading probability and analyze the impact of user density.\""}, "58": {"documentation": {"title": "Encoding DNA sequences by integer chaos game representation", "source": "Changchuan Yin", "docs_id": "1712.04546", "section": ["cs.CE", "q-bio.OT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Encoding DNA sequences by integer chaos game representation. DNA sequences are fundamental for encoding genetic information. The genetic information may not only be understood by symbolic sequences but also from the hidden signals inside the sequences. The symbolic sequences need to be transformed into numerical sequences so the hidden signals can be revealed by signal processing techniques. All current transformation methods encode DNA sequences into numerical values of the same length. These representations have limitations in the applications of genomic signal compression, encryption, and steganography. We propose an integer chaos game representation (iCGR) of DNA sequences and a lossless encoding method DNA sequences by the iCGR. In the iCGR method, a DNA sequence is represented by the iterated function of the nucleotides and their positions in the sequence. Then the DNA sequence can be uniquely encoded and recovered using three integers from iCGR. One integer is the sequence length and the other two integers represent the accumulated distributions of nucleotides in the sequence. The integer encoding scheme can compress a DNA sequence by 2 bits per nucleotide. The integer representation of DNA sequences provides a prospective tool for sequence compression, encryption, and steganography. The Python programs in this study are freely available to the public at https://github.com/cyinbox/iCGR"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of current transformation methods for encoding DNA sequences into numerical values, and how does the proposed integer chaos game representation (iCGR) address this limitation?\n\nA) Current methods encode DNA sequences into numerical values of the same length, which limits their application in genomic signal compression, encryption, and steganography. The iCGR method addresses this limitation by providing a more efficient encoding scheme that can compress DNA sequences by 2 bits per nucleotide.\n\nB) Current methods encode DNA sequences into numerical values of different lengths, which limits their application in genomic signal compression, encryption, and steganography. The iCGR method addresses this limitation by providing a fixed-length encoding scheme that can compress DNA sequences by 2 bits per nucleotide.\n\nC) Current methods encode DNA sequences into numerical values that are not unique, which limits their application in genomic signal compression, encryption, and steganography. The iCGR method addresses this limitation by providing a unique encoding scheme that can compress DNA sequences by 2 bits per nucleotide.\n\nD) Current methods encode DNA sequences into numerical values that are not lossless, which limits their application in genomic signal compression, encryption, and steganography. The iCGR method addresses this limitation by providing a lossless encoding scheme that can compress DNA sequences by 2 bits per nucleotide.\n\n**Correct Answer:** A) Current methods encode DNA sequences into numerical values of the same length, which limits their application in genomic signal compression, encryption, and steganography. The iCGR method addresses this limitation by providing a more efficient encoding scheme that can compress DNA sequences by 2 bits per nucleotide.\n\n**Explanation:** The correct answer is A) because the documentation states that current transformation methods encode DNA sequences into numerical values of the same length, which limits their application in genomic signal compression, encryption, and steganography. The iCGR method addresses this limitation by providing a more efficient encoding scheme that can compress DNA sequences by 2 bits per nucleotide."}, "59": {"documentation": {"title": "Iteratively seeded mode-locking", "source": "Victor G. Bucklew, William H. Renninger, Perry S. Edwards, Zhiwen Liu", "docs_id": "1612.04296", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Iteratively seeded mode-locking. Ultrashort pulsed mode-locked lasers enable research at new time-scales and revolutionary technologies from bioimaging to materials processing. In general, the performance of these lasers is determined by the degree to which the pulses of a particular resonator can be scaled in energy and pulse duration before destabilizing. To date, milestones have come from the application of more tolerant pulse solutions, drawing on nonlinear concepts like soliton formation and self-similarity. Despite these advances, lasers have not reached the predicted performance limits anticipated by these new solutions. In this letter, towards resolving this discrepancy, we demonstrate that the route by which the laser arrives at the solution presents a limit to performance which, moreover, is reached before the solution itself becomes unstable. In contrast to known self-starting limitations stemming from suboptimal saturable absorption, we show that this limit persists even with an ideal saturable absorber. Furthermore, we demonstrate that this limit can be completely surmounted with an iteratively seeded technique for mode-locking. Iteratively seeded mode-locking is numerically explored and compared to traditional static seeding, initially achieving a five-fold increase in energy. This approach is broadly applicable to mode-locked lasers and can be readily implemented into existing experimental architectures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary limitation in the performance of ultrashort pulsed mode-locked lasers, and how does the iteratively seeded mode-locking technique address this limitation?\n\nA) The primary limitation is the instability of the laser solution, which can be overcome by using a more tolerant pulse solution.\nB) The primary limitation is the suboptimal saturable absorption, which can be improved by using an ideal saturable absorber.\nC) The primary limitation is the scalability of the laser pulses, which can be increased by using an iteratively seeded mode-locking technique.\nD) The primary limitation is the self-similarity of the laser pulses, which can be improved by using a more efficient nonlinear concept.\n\nCorrect Answer: C) The primary limitation is the scalability of the laser pulses, which can be increased by using an iteratively seeded mode-locking technique.\n\nExplanation: The correct answer is C) The primary limitation is the scalability of the laser pulses, which can be increased by using an iteratively seeded mode-locking technique. The documentation states that the performance of these lasers is determined by the degree to which the pulses of a particular resonator can be scaled in energy and pulse duration before destabilizing. The iteratively seeded mode-locking technique is shown to overcome this limitation, achieving a five-fold increase in energy. This requires the test-taker to understand the relationship between pulse scalability and laser performance, as well as the benefits of the iteratively seeded mode-locking technique."}}