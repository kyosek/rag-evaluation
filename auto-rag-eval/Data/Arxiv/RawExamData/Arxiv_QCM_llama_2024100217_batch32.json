{"0": {"documentation": {"title": "Graph IRF Models and Fusion Rings", "source": "Doron Gepner (Caltech and Weizmann)", "docs_id": "hep-th/9306143", "section": ["hep-th", "math.AG", "math.QA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Graph IRF Models and Fusion Rings. Recently, a class of interaction round the face (IRF) solvable lattice models were introduced, based on any rational conformal field theory (RCFT). We investigate here the connection between the general solvable IRF models and the fusion ones. To this end, we introduce an associative algebra associated to any graph, as the algebra of products of the eigenvalues of the incidence matrix. If a model is based on an RCFT, its associated graph algebra is the fusion ring of the RCFT. A number of examples are studied. The Gordon--generalized IRF models are studied, and are shown to come from RCFT, by the graph algebra construction. The IRF models based on the Dynkin diagrams of A-D-E are studied. While the $A$ case stems from an RCFT, it is shown that the $D-E$ cases do not. The graph algebras are constructed, and it is speculated that a natural isomorphism relating these to RCFT exists. The question whether all solvable IRF models stems from an RCFT remains open, though the $D-E$ cases shows that a mixing of the primary fields is needed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the relationship between the graph IRF models and the fusion rings, and how do the Gordon-generalized IRF models and the IRF models based on Dynkin diagrams of A-D-E relate to rational conformal field theory (RCFT)?\n\nA) The graph IRF models are a special case of fusion rings, and the Gordon-generalized IRF models are a direct consequence of RCFT, while the IRF models based on Dynkin diagrams of A-D-E do not stem from RCFT.\n\nB) The graph IRF models are a generalization of fusion rings, and the Gordon-generalized IRF models are a specific example of an IRF model that can be constructed from an RCFT, while the IRF models based on Dynkin diagrams of A-D-E do not have a clear connection to RCFT.\n\nC) The graph IRF models are equivalent to fusion rings, and the Gordon-generalized IRF models and the IRF models based on Dynkin diagrams of A-D-E are both examples of IRF models that can be constructed from RCFT, with the latter two cases requiring a mixing of primary fields.\n\nD) The graph IRF models are a distinct class of models that do not relate to fusion rings, and the Gordon-generalized IRF models and the IRF models based on Dynkin diagrams of A-D-E are both examples of models that do not stem from RCFT.\n\nCorrect Answer: C) The graph IRF models are equivalent to fusion rings, and the Gordon-generalized IRF models and the IRF models based on Dynkin diagrams of A-D-E are both examples of IRF models that can be constructed from RCFT, with the latter two cases requiring a mixing of primary fields.\n\nExplanation: The correct answer is based on the fact that the graph algebra associated with any graph is the fusion ring of the RCFT, and the Gordon-generalized IRF models and the IRF models based on Dynkin diagrams of A-D-E can be constructed from RCFT using the graph algebra construction. The correct answer also acknowledges that the IRF models based on Dynkin diagrams of D-E do not stem from RCFT, but rather require a mixing of primary fields."}, "1": {"documentation": {"title": "Optical analysis of spatially periodic patterns in nematic liquid\n  crystals: diffraction and shadowgraphy", "source": "Werner Pesch and Alexei Krekhov", "docs_id": "1305.3472", "section": ["cond-mat.soft", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optical analysis of spatially periodic patterns in nematic liquid\n  crystals: diffraction and shadowgraphy. Optical methods are most convenient to analyze spatially periodic patterns with wavevector $\\bm q$ in a thin layer of a nematic liquid crystal. In the standard experimental setup a beam of parallel light with a 'short' wavelength $\\lambda \\ll 2 \\pi/q$ passes the nematic layer. Recording the transmitted light the patterns are either directly visualized by shadowgraphy or characterized more indirectly by the diffraction fringes due to the optical grating effects of the pattern. In this work we present a systematic short-wavelength analysis of these methods for the commonly used planar orientation of the optical axis of liquid crystal at the confining surfaces. Our approach covers general 3D experimental geometries with respect to the relative orientation of $\\bm q$ and of the wavevector $\\bm k$ of the incident light. In particular the importance of phase grating effects is emphasized, which are not accessible in a pure geometric optics approach. Finally, as a byproduct we present also an optical analysis of convection rolls in Rayleigh-B\\'enard convection, where the refraction index of the fluid is isotropic in contrast to its uniaxial symmetry in nematic liquid crystals. Our analysis is in excellent agreement with an earlier physical optics approach by Trainoff and Cannell [Physics of Fluids {\\bf 14}, 1340 (2002)], which is restricted to a 2D geometry and technically much more demanding."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the optical analysis of spatially periodic patterns in nematic liquid crystals, what is the significance of the phase grating effects in the diffraction pattern, and how do they differ from the optical grating effects in a pure geometric optics approach?\n\n**A)** Phase grating effects are only significant when the wavelength of the incident light is comparable to the size of the pattern, and they are not relevant in a pure geometric optics approach.\n\n**B)** Phase grating effects are a result of the interference between the light waves scattered by the pattern, and they are not accessible in a pure geometric optics approach, which assumes a fixed phase relationship between the light waves.\n\n**C)** Phase grating effects are a consequence of the anisotropic refraction index of the nematic liquid crystal, and they are not relevant in a pure geometric optics approach, which assumes a constant refraction index.\n\n**D)** Phase grating effects are a result of the interaction between the incident light and the pattern, and they are only significant when the wavelength of the incident light is much smaller than the size of the pattern.\n\n**Correct Answer:** B) Phase grating effects are a result of the interference between the light waves scattered by the pattern, and they are not accessible in a pure geometric optics approach, which assumes a fixed phase relationship between the light waves.\n\n**Explanation:** The correct answer is B) because phase grating effects are a result of the interference between the light waves scattered by the pattern, which is a consequence of the anisotropic refraction index of the nematic liquid crystal. In a pure geometric optics approach, the phase relationship between the light waves is assumed to be fixed, which neglects the phase grating effects. The other options are incorrect because they either misrepresent the significance of phase grating effects or incorrectly assume the relevance of the phase grating effects in a pure geometric optics approach."}, "2": {"documentation": {"title": "On the map of Vogel's plane", "source": "R. L. Mkrtchyan", "docs_id": "1209.5709", "section": ["math-ph", "hep-th", "math.GR", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the map of Vogel's plane. We search points in a Vogel plane with regular universal expression for character of adjoint representation. This gives seven patterns of singularities cancellation, each giving a certain Diophantine equation of third order on three variables. Solutions of these equations are classical series of simple Lie algebras (including an \"odd symplectic\" one), $D_{2,1,\\lambda}$ superalgebra, the straight line of three-dimensional algebras, and a number of isolated solutions, including exceptional simple Lie algebras. One of these Diophantine equations, namely knm=4k+4n+2m+12 contains all simple Lie algebras, except SO(2N+1). Isolated solutions contain, beside exceptional simple Lie algebras, so called $E_{71/2}$ algebra and also two other similar (unknown) objects with positive dimensions. In addition, there are 47 isolated solutions in \"unphysical semiplane\" with negative dimensions. Isolated solutions mainly lie on a few straight lines in Vogel plane. All solutions give an integers in universal dimension formulae for first three symmetric powers of adjoint representation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the Diophantine equation knm = 4k + 4n + 2m + 12, where k, n, and m are integers. Which of the following statements is true for all solutions to this equation?\n\nA) All solutions are classical series of simple Lie algebras.\nB) All solutions are isolated and lie on a straight line in Vogel plane.\nC) All solutions are superalgebras of the form $D_{2,1,\\lambda}$.\nD) All solutions are exceptional simple Lie algebras.\n\nCorrect Answer: A) All solutions are classical series of simple Lie algebras.\n\nExplanation: The Diophantine equation knm = 4k + 4n + 2m + 12 is known to contain all simple Lie algebras, except SO(2N+1). This means that any solution to this equation will be a classical series of simple Lie algebras, which includes the \"odd symplectic\" one, $D_{2,1,\\lambda}$ superalgebra, and other classical series of simple Lie algebras. The other options are incorrect because isolated solutions may not necessarily lie on a straight line in Vogel plane (B), may not be superalgebras of the form $D_{2,1,\\lambda}$ (C), and may not be exceptional simple Lie algebras (D)."}, "3": {"documentation": {"title": "Scattering studies with low-energy kaon-proton femtoscopy in\n  proton-proton collisions at the LHC", "source": "ALICE Collaboration", "docs_id": "1905.13470", "section": ["nucl-ex", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scattering studies with low-energy kaon-proton femtoscopy in\n  proton-proton collisions at the LHC. The study of the strength and behaviour of the antikaon-nucleon ($\\mathrm{\\overline{K}N}$) interaction constitutes one of the key focuses of the strangeness sector in low-energy Quantum Chromodynamics (QCD). In this letter a unique high-precision measurement of the strong interaction between kaons and protons, close and above the kinematic threshold, is presented. The femtoscopic measurements of the correlation function at low pair-frame relative momentum of (K$^+$ p $\\oplus$ K$^-$ $\\overline{\\mathrm{p}}$) and (K$^-$ p $\\oplus$ K$^+$ $\\overline{\\mathrm{p}}$) pairs measured in pp collisions at $\\sqrt{s}$ = 5, 7 and 13 TeV are reported. A structure observed around a relative momentum of 58 MeV/$c$ in the measured correlation function of (K$^-$ p $\\oplus$ K$^+$ $\\overline{\\mathrm{p}}$) with a significance of 4.4. $\\sigma$ constitutes the first experimental evidence for the opening of the $(\\mathrm{\\overline{K}^0 n} \\oplus \\mathrm{K^0 \\overline{n}})$ isospin breaking channel due to the mass difference between charged and neutral kaons. The measured correlation functions have been compared to J\\\"{u}lich and Kyoto models in addition to the Coulomb potential. The high-precision data at low relative momenta presented in this work prove femtoscopy to be a powerful complementary tool to scattering experiments and provide new constraints above the $\\mathrm{\\overline{K}N}$ threshold for low-energy QCD chiral models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the significance of the structure observed around a relative momentum of 58 MeV/c in the measured correlation function of (K$^-$ p $\\oplus$ K$^+$ $\\overline{\\mathrm{p}}$) pairs, and what does it imply for the opening of the $(\\mathrm{\\overline{K}^0 n} \\oplus \\mathrm{K^0 \\overline{n}})$ isospin breaking channel?\n\nA) The structure is a result of the Coulomb potential and does not imply isospin breaking.\nB) The structure is a result of the J\u00fclich model and implies the opening of the $(\\mathrm{\\overline{K}^0 n} \\oplus \\mathrm{K^0 \\overline{n}})$ isospin breaking channel.\nC) The structure is a result of the Kyoto model and does not imply isospin breaking.\nD) The structure is a result of the mass difference between charged and neutral kaons and implies the opening of the $(\\mathrm{\\overline{K}^0 n} \\oplus \\mathrm{K^0 \\overline{n}})$ isospin breaking channel.\n\nCorrect Answer: D) The structure is a result of the mass difference between charged and neutral kaons and implies the opening of the $(\\mathrm{\\overline{K}^0 n} \\oplus \\mathrm{K^0 \\overline{n}})$ isospin breaking channel.\n\nExplanation: The correct answer is D) because the text states that the structure observed around a relative momentum of 58 MeV/c \"constitutes the first experimental evidence for the opening of the $(\\mathrm{\\overline{K}^0 n} \\oplus \\mathrm{K^0 \\overline{n}})$ isospin breaking channel due to the mass difference between charged and neutral kaons.\" This implies that the structure is a result of the mass difference between charged and neutral kaons, which is option D. Options A, B, and C are incorrect because they do not accurately reflect the information provided in the text."}, "4": {"documentation": {"title": "Radio-frequency Dark Photon Dark Matter across the Sun", "source": "Haipeng An, Fa Peng Huang, Jia Liu, Wei Xue", "docs_id": "2010.15836", "section": ["hep-ph", "astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Radio-frequency Dark Photon Dark Matter across the Sun. Dark photon as an ultralight dark matter candidate can interact with the Standard Model particles via kinetic mixing. We propose to search for the ultralight dark photon dark matter using radio telescopes with solar observations. The dark photon dark matter can efficiently convert into photons in the outermost region of the solar atmosphere, the solar corona, where the plasma mass of photons is close to the dark photon rest mass. Due to the strong resonant conversion and benefiting from the short distance between the Sun and the Earth, the radio telescopes can lead the dark photon search sensitivity in the mass range of $4 \\times 10^{-8} - 4\\times 10^{-6} \\, \\rm{eV}$, corresponding to the frequency $10 - 1000 \\, {\\rm MHz}$. As a promising example, the operating radio telescope LOFAR can reach the kinetic mixing $\\epsilon \\sim 10^{-13}$ ($10^{-14}$) within 1 (100) hour solar observations. The future experiment SKA phase 1 can reach $\\epsilon \\sim 10^{-16} - 10^{-14}$ with $1$ hour solar observations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the Arxiv paper propose to search for ultralight dark photon dark matter using radio telescopes with solar observations. What is the estimated mass range of the dark photon dark matter that can be detected by radio telescopes, and what is the corresponding frequency range?\n\nA) $4 \\times 10^{-6} - 4 \\times 10^{-8} \\, \\rm{eV}$, $1000 - 10 \\, {\\rm MHz}$\nB) $4 \\times 10^{-8} - 4 \\times 10^{-6} \\, \\rm{eV}$, $10 - 1000 \\, {\\rm MHz}$\nC) $4 \\times 10^{-6} - 4 \\times 10^{-8} \\, \\rm{eV}$, $1000 - 10 \\, {\\rm MHz}$\nD) $4 \\times 10^{-8} - 4 \\times 10^{-6} \\, \\rm{eV}$, $10 - 1000 \\, {\\rm MHz}$\n\n**Correct Answer:** B) $4 \\times 10^{-8} - 4 \\times 10^{-6} \\, \\rm{eV}$, $10 - 1000 \\, {\\rm MHz}$\n\n**Explanation:** The correct answer is B) $4 \\times 10^{-8} - 4 \\times 10^{-6} \\, \\rm{eV}$, $10 - 1000 \\, {\\rm MHz}$, as stated in the original text: \"The dark photon dark matter can efficiently convert into photons in the outermost region of the solar atmosphere, the solar corona, where the plasma mass of photons is close to the dark photon rest mass. Due to the strong resonant conversion and benefiting from the short distance between the Sun and the Earth, the radio telescopes can lead the dark photon search sensitivity in the mass range of $4 \\times 10^{-8} - 4\\times 10^{-6} \\, \\rm{eV}$, corresponding to the frequency $10 - 1000 \\, {\\rm MHz}$.\"\n\n**Candidate A** is incorrect because it reverses the mass range and frequency range.\n\n**Candidate C** is incorrect because it repeats the incorrect mass range and frequency range of Candidate A.\n\n**Candidate D** is incorrect because it repeats the correct mass range but the frequency range is incorrect."}, "5": {"documentation": {"title": "Information sharing promotes prosocial behaviour", "source": "Attila Szolnoki, Matjaz Perc", "docs_id": "1308.3485", "section": ["physics.soc-ph", "cond-mat.stat-mech", "cs.SI", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Information sharing promotes prosocial behaviour. More often than not, bad decisions are bad regardless of where and when they are made. Information sharing might thus be utilized to mitigate them. Here we show that sharing the information about strategy choice between players residing on two different networks reinforces the evolution of cooperation. In evolutionary games the strategy reflects the action of each individual that warrants the highest utility in a competitive setting. We therefore assume that identical strategies on the two networks reinforce themselves by lessening their propensity to change. Besides network reciprocity working in favour of cooperation on each individual network, we observe the spontaneous emerge of correlated behaviour between the two networks, which further deters defection. If information is shared not just between individuals but also between groups, the positive effect is even stronger, and this despite the fact that information sharing is implemented without any assumptions with regards to content."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of evolutionary games, what is the primary mechanism by which information sharing between two different networks reinforces the evolution of cooperation, and how does this mechanism differ when information is shared between individuals versus groups?\n\n**A)** Information sharing between individuals reinforces cooperation by increasing the propensity to change strategies, thereby reducing the likelihood of defection. This mechanism is weaker when information is shared between groups.\n\n**B)** Information sharing between individuals reinforces cooperation by lessening their propensity to change strategies, thereby reducing the likelihood of defection. This mechanism is stronger when information is shared between groups.\n\n**C)** Information sharing between individuals reinforces cooperation by increasing the likelihood of defection, thereby reducing the propensity to change strategies. This mechanism is weaker when information is shared between groups.\n\n**D)** Information sharing between individuals reinforces cooperation by increasing the propensity to change strategies, thereby increasing the likelihood of defection. This mechanism is stronger when information is shared between groups.\n\n**Correct Answer:** B) Information sharing between individuals reinforces cooperation by lessening their propensity to change strategies, thereby reducing the likelihood of defection. This mechanism is stronger when information is shared between groups.\n\n**Explanation:** According to the provided documentation, information sharing between individuals reinforces cooperation by lessening their propensity to change strategies, thereby reducing the likelihood of defection. Additionally, when information is shared between groups, the positive effect is even stronger, suggesting that the mechanism is indeed stronger when information is shared between groups."}, "6": {"documentation": {"title": "TEMImageNet Training Library and AtomSegNet Deep-Learning Models for\n  High-Precision Atom Segmentation, Localization, Denoising, and\n  Super-Resolution Processing of Atomic-Resolution Images", "source": "Ruoqian Lin, Rui Zhang, Chunyang Wang, Xiao-Qing Yang, Huolin L. Xin", "docs_id": "2012.09093", "section": ["cond-mat.mtrl-sci", "cs.AI", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "TEMImageNet Training Library and AtomSegNet Deep-Learning Models for\n  High-Precision Atom Segmentation, Localization, Denoising, and\n  Super-Resolution Processing of Atomic-Resolution Images. Atom segmentation and localization, noise reduction and deblurring of atomic-resolution scanning transmission electron microscopy (STEM) images with high precision and robustness is a challenging task. Although several conventional algorithms, such has thresholding, edge detection and clustering, can achieve reasonable performance in some predefined sceneries, they tend to fail when interferences from the background are strong and unpredictable. Particularly, for atomic-resolution STEM images, so far there is no well-established algorithm that is robust enough to segment or detect all atomic columns when there is large thickness variation in a recorded image. Herein, we report the development of a training library and a deep learning method that can perform robust and precise atom segmentation, localization, denoising, and super-resolution processing of experimental images. Despite using simulated images as training datasets, the deep-learning model can self-adapt to experimental STEM images and shows outstanding performance in atom detection and localization in challenging contrast conditions and the precision consistently outperforms the state-of-the-art two-dimensional Gaussian fit method. Taking a step further, we have deployed our deep-learning models to a desktop app with a graphical user interface and the app is free and open-source. We have also built a TEM ImageNet project website for easy browsing and downloading of the training data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary challenge in atom segmentation and localization of atomic-resolution scanning transmission electron microscopy (STEM) images, and how does the proposed deep-learning method address this challenge?\n\nA) The primary challenge is the limited availability of training data, and the proposed method addresses this by using simulated images as training datasets.\nB) The primary challenge is the large thickness variation in recorded images, and the proposed method addresses this by using a two-dimensional Gaussian fit method to segment and localize atomic columns.\nC) The primary challenge is the strong interferences from the background, and the proposed method addresses this by using a deep-learning model that can self-adapt to experimental STEM images and shows outstanding performance in atom detection and localization in challenging contrast conditions.\nD) The primary challenge is the lack of robustness of conventional algorithms, and the proposed method addresses this by using a combination of thresholding, edge detection, and clustering techniques.\n\nCorrect Answer: C) The primary challenge is the strong interferences from the background, and the proposed method addresses this by using a deep-learning model that can self-adapt to experimental STEM images and shows outstanding performance in atom detection and localization in challenging contrast conditions.\n\nExplanation: The correct answer is C) because the documentation states that conventional algorithms such as thresholding, edge detection, and clustering \"tend to fail when interferences from the background are strong and unpredictable\". The proposed deep-learning method addresses this challenge by using a model that can self-adapt to experimental STEM images and shows outstanding performance in atom detection and localization in challenging contrast conditions."}, "7": {"documentation": {"title": "Computational aspects of Bayesian spectral density estimation", "source": "Nicolas Chopin, Judith Rousseau and Brunero Liseo", "docs_id": "1211.4483", "section": ["stat.CO", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Computational aspects of Bayesian spectral density estimation. Gaussian time-series models are often specified through their spectral density. Such models present several computational challenges, in particular because of the non-sparse nature of the covariance matrix. We derive a fast approximation of the likelihood for such models. We propose to sample from the approximate posterior (that is, the prior times the approximate likelihood), and then to recover the exact posterior through importance sampling. We show that the variance of the importance sampling weights vanishes as the sample size goes to infinity. We explain why the approximate posterior may typically multi-modal, and we derive a Sequential Monte Carlo sampler based on an annealing sequence in order to sample from that target distribution. Performance of the overall approach is evaluated on simulated and real datasets. In addition, for one real world dataset, we provide some numerical evidence that a Bayesian approach to semi-parametric estimation of spectral density may provide more reasonable results than its Frequentist counter-parts."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary computational challenge associated with Gaussian time-series models specified through their spectral density, and how does the proposed approach address this challenge?\n\n**A)** The primary challenge is the non-sparse nature of the covariance matrix, which leads to computational inefficiencies. The proposed approach addresses this challenge by using importance sampling to recover the exact posterior from the approximate posterior.\n\n**B)** The primary challenge is the non-stationarity of the time-series data, which makes it difficult to estimate the spectral density. The proposed approach addresses this challenge by using an annealing sequence to sample from the approximate posterior.\n\n**C)** The primary challenge is the high dimensionality of the spectral density parameter space, which makes it difficult to estimate the parameters. The proposed approach addresses this challenge by using a Sequential Monte Carlo sampler to sample from the approximate posterior.\n\n**D)** The primary challenge is the non-normality of the time-series data, which makes it difficult to estimate the spectral density. The proposed approach addresses this challenge by using a Bayesian approach to semi-parametric estimation of spectral density.\n\n**Correct Answer:** A) The primary challenge is the non-sparse nature of the covariance matrix, which leads to computational inefficiencies. The proposed approach addresses this challenge by using importance sampling to recover the exact posterior from the approximate posterior.\n\n**Explanation:** The correct answer is A) because the documentation states that Gaussian time-series models present several computational challenges, particularly due to the non-sparse nature of the covariance matrix. The proposed approach addresses this challenge by using importance sampling to recover the exact posterior from the approximate posterior, which is mentioned in the documentation. The other options are incorrect because they do not accurately reflect the primary computational challenge associated with Gaussian time-series models specified through their spectral density, and the proposed approach."}, "8": {"documentation": {"title": "Evaluating the Performance of Machine Learning Algorithms in Financial\n  Market Forecasting: A Comprehensive Survey", "source": "Lukas Ryll and Sebastian Seidens", "docs_id": "1906.07786", "section": ["q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evaluating the Performance of Machine Learning Algorithms in Financial\n  Market Forecasting: A Comprehensive Survey. With increasing competition and pace in the financial markets, robust forecasting methods are becoming more and more valuable to investors. While machine learning algorithms offer a proven way of modeling non-linearities in time series, their advantages against common stochastic models in the domain of financial market prediction are largely based on limited empirical results. The same holds true for determining advantages of certain machine learning architectures against others. This study surveys more than 150 related articles on applying machine learning to financial market forecasting. Based on a comprehensive literature review, we build a table across seven main parameters describing the experiments conducted in these studies. Through listing and classifying different algorithms, we also introduce a simple, standardized syntax for textually representing machine learning algorithms. Based on performance metrics gathered from papers included in the survey, we further conduct rank analyses to assess the comparative performance of different algorithm classes. Our analysis shows that machine learning algorithms tend to outperform most traditional stochastic methods in financial market forecasting. We further find evidence that, on average, recurrent neural networks outperform feed forward neural networks as well as support vector machines which implies the existence of exploitable temporal dependencies in financial time series across multiple asset classes and geographies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Evaluate the performance of machine learning algorithms in financial market forecasting, specifically in terms of their ability to capture temporal dependencies in financial time series.\n\n**A)** According to the study, machine learning algorithms tend to outperform traditional stochastic methods in financial market forecasting due to their ability to model non-linear relationships in time series. However, the study also suggests that recurrent neural networks may not always outperform feed forward neural networks, as the latter can capture non-linear relationships more effectively.\n\n**B)** The study's analysis shows that machine learning algorithms, particularly recurrent neural networks, outperform traditional stochastic methods in financial market forecasting due to their ability to capture temporal dependencies in financial time series. This implies that investors can use machine learning algorithms to identify exploitable patterns in financial data.\n\n**C)** The study's findings suggest that machine learning algorithms, including support vector machines, may not be the most effective choice for financial market forecasting, as they often struggle to capture temporal dependencies in financial time series. Instead, feed forward neural networks and recurrent neural networks may be more suitable for this task.\n\n**D)** The study's analysis reveals that machine learning algorithms, particularly recurrent neural networks, outperform traditional stochastic methods in financial market forecasting due to their ability to model non-linear relationships in time series. However, the study also notes that the performance of machine learning algorithms can vary depending on the specific asset class and geography being forecasted.\n\n**Correct Answer:** B) The study's analysis shows that machine learning algorithms, particularly recurrent neural networks, outperform traditional stochastic methods in financial market forecasting due to their ability to capture temporal dependencies in financial time series. This implies that investors can use machine learning algorithms to identify exploitable patterns in financial data.\n\n**Explanation:** This question requires the test-taker to carefully read and understand the study's findings, particularly the section on the performance of machine learning algorithms in financial market forecasting. The correct answer, B, accurately summarizes the study's analysis, which shows that machine learning algorithms, particularly recurrent neural networks, outperform traditional stochastic methods due to their ability to capture temporal dependencies in financial time series. The other options contain errors or incomplete information, making them incorrect choices."}, "9": {"documentation": {"title": "Principled estimation of regression discontinuity designs", "source": "L. Jason Anastasopoulos", "docs_id": "1910.06381", "section": ["stat.AP", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Principled estimation of regression discontinuity designs. Regression discontinuity designs are frequently used to estimate the causal effect of election outcomes and policy interventions. In these contexts, treatment effects are typically estimated with covariates included to improve efficiency. While including covariates improves precision asymptotically, in practice, treatment effects are estimated with a small number of observations, resulting in considerable fluctuations in treatment effect magnitude and precision depending upon the covariates chosen. This practice thus incentivizes researchers to select covariates which maximize treatment effect statistical significance rather than precision. Here, I propose a principled approach for estimating RDDs which provides a means of improving precision with covariates while minimizing adverse incentives. This is accomplished by integrating the adaptive LASSO, a machine learning method, into RDD estimation using an R package developed for this purpose, adaptiveRDD. Using simulations, I show that this method significantly improves treatment effect precision, particularly when estimating treatment effects with fewer than 200 observations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of regression discontinuity designs (RDDs), what is the primary concern with including covariates in treatment effect estimation, and how does the proposed adaptive LASSO method address this issue?\n\n**A)** The primary concern is that including covariates can lead to overfitting, which can result in biased estimates of treatment effects. The adaptive LASSO method addresses this issue by using a regularization technique to reduce overfitting.\n\n**B)** The primary concern is that including covariates can lead to considerable fluctuations in treatment effect magnitude and precision depending on the covariates chosen. The adaptive LASSO method addresses this issue by using a machine learning approach to select covariates that maximize treatment effect statistical significance while minimizing adverse incentives.\n\n**C)** The primary concern is that including covariates can lead to inefficient estimation of treatment effects, particularly when estimating treatment effects with a small number of observations. The adaptive LASSO method addresses this issue by using a method that improves precision asymptotically.\n\n**D)** The primary concern is that including covariates can lead to biased estimates of treatment effects due to the selection of covariates that maximize treatment effect statistical significance rather than precision. The adaptive LASSO method addresses this issue by providing a means of improving precision with covariates while minimizing adverse incentives.\n\n**Correct Answer:** B) The primary concern is that including covariates can lead to considerable fluctuations in treatment effect magnitude and precision depending on the covariates chosen. The adaptive LASSO method addresses this issue by using a machine learning approach to select covariates that maximize treatment effect statistical significance while minimizing adverse incentives.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the primary concern with including covariates in treatment effect estimation in RDDs, as stated in the provided documentation. The adaptive LASSO method is proposed as a solution to address this issue by using a machine learning approach to select covariates that maximize treatment effect statistical significance while minimizing adverse incentives."}, "10": {"documentation": {"title": "Prediction of Workplace Injuries", "source": "Mehdi Sadeqi, Azin Asgarian, Ariel Sibilia", "docs_id": "1906.03080", "section": ["cs.CY", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prediction of Workplace Injuries. Workplace injuries result in substantial human and financial losses. As reported by the International Labour Organization (ILO), there are more than 374 million work-related injuries reported every year. In this study, we investigate the problem of injury risk prediction and prevention in a work environment. While injuries represent a significant number across all organizations, they are rare events within a single organization. Hence, collecting a sufficiently large dataset from a single organization is extremely difficult. In addition, the collected datasets are often highly imbalanced which increases the problem difficulty. Finally, risk predictions need to provide additional context for injuries to be prevented. We propose and evaluate the following for a complete solution: 1) several ensemble-based resampling methods to address the class imbalance issues, 2) a novel transfer learning approach to transfer the knowledge across organizations, and 3) various techniques to uncover the association and causal effect of different variables on injury risk, while controlling for relevant confounding factors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the primary challenges in predicting workplace injuries, and how do these challenges impact the development of effective injury risk prediction models?\n\n**A)** The primary challenges are collecting a large and diverse dataset, and the models are often too complex to interpret. However, the lack of data and the need for interpretable models are not the main issues.\n\n**B)** The primary challenges are the rarity of injuries within a single organization, the high cost of collecting a large dataset, and the need to control for confounding factors. However, the rarity of injuries and the cost of data collection are not the main issues.\n\n**C)** The primary challenges are the high dimensionality of the data, the presence of missing values, and the need to handle class imbalance issues. However, the dimensionality of the data and the presence of missing values are not the main issues.\n\n**D)** The primary challenges are the rarity of injuries within a single organization, the high cost of collecting a large dataset, and the need to transfer knowledge across organizations. However, the rarity of injuries and the need to transfer knowledge are not the main issues.\n\n**Correct Answer:** D) The primary challenges are the rarity of injuries within a single organization, the high cost of collecting a large dataset, and the need to transfer knowledge across organizations.\n\n**Explanation:** The correct answer is D) because the documentation states that \"injuries represent a significant number across all organizations, they are rare events within a single organization\" and \"collecting a sufficiently large dataset from a single organization is extremely difficult\". Additionally, the documentation mentions that \"the collected datasets are often highly imbalanced\" and proposes a novel transfer learning approach to transfer knowledge across organizations. The other options do not accurately capture the primary challenges in predicting workplace injuries."}, "11": {"documentation": {"title": "Risk Management via Anomaly Circumvent: Mnemonic Deep Learning for\n  Midterm Stock Prediction", "source": "Xinyi Li, Yinchuan Li, Xiao-Yang Liu and Christina Dan Wang", "docs_id": "1908.01112", "section": ["q-fin.ST", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Risk Management via Anomaly Circumvent: Mnemonic Deep Learning for\n  Midterm Stock Prediction. Midterm stock price prediction is crucial for value investments in the stock market. However, most deep learning models are essentially short-term and applying them to midterm predictions encounters large cumulative errors because they cannot avoid anomalies. In this paper, we propose a novel deep neural network Mid-LSTM for midterm stock prediction, which incorporates the market trend as hidden states. First, based on the autoregressive moving average model (ARMA), a midterm ARMA is formulated by taking into consideration both hidden states and the capital asset pricing model. Then, a midterm LSTM-based deep neural network is designed, which consists of three components: LSTM, hidden Markov model and linear regression networks. The proposed Mid-LSTM can avoid anomalies to reduce large prediction errors, and has good explanatory effects on the factors affecting stock prices. Extensive experiments on S&P 500 stocks show that (i) the proposed Mid-LSTM achieves 2-4% improvement in prediction accuracy, and (ii) in portfolio allocation investment, we achieve up to 120.16% annual return and 2.99 average Sharpe ratio."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge faced by traditional deep learning models when it comes to midterm stock price prediction, and how does the proposed Mid-LSTM model address this issue?\n\n**A)** Traditional deep learning models struggle with the complexity of market trends, which leads to large cumulative errors. The Mid-LSTM model overcomes this by incorporating market trend as hidden states.\n\n**B)** Traditional deep learning models are limited by their inability to handle non-linear relationships between stock prices and other market factors. The Mid-LSTM model uses a combination of LSTM, hidden Markov model, and linear regression networks to capture these relationships.\n\n**C)** Traditional deep learning models are prone to overfitting due to the large amount of data available. The Mid-LSTM model avoids overfitting by using a midterm ARMA model that takes into consideration both hidden states and the capital asset pricing model.\n\n**D)** Traditional deep learning models are unable to handle anomalies in stock price data. The Mid-LSTM model addresses this by using a combination of LSTM and hidden Markov model to detect and avoid anomalies.\n\n**Correct Answer:** D) Traditional deep learning models are unable to handle anomalies in stock price data. The Mid-LSTM model addresses this by using a combination of LSTM and hidden Markov model to detect and avoid anomalies.\n\n**Explanation:** The correct answer is D) because the documentation states that most deep learning models are \"essentially short-term\" and cannot avoid anomalies, leading to large cumulative errors. The proposed Mid-LSTM model addresses this issue by incorporating a hidden Markov model to detect and avoid anomalies, as mentioned in the introduction of the paper."}, "12": {"documentation": {"title": "Near-IR Direct Detection of Water Vapor in Tau Boo b", "source": "Alexandra C. Lockwood, John A. Johnson, Chad F. Bender, John S. Carr,\n  Travis Barman, Alexander J.W. Richert, Geoffrey A. Blake", "docs_id": "1402.0846", "section": ["astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Near-IR Direct Detection of Water Vapor in Tau Boo b. We use high dynamic range, high-resolution L-band spectroscopy to measure the radial velocity variations of the hot Jupiter in the tau Bootis planetary system. The detection of an exoplanet by the shift in the stellar spectrum alone provides a measure of the planet's minimum mass, with the true mass degenerate with the unknown orbital inclination. Treating the tau Boo system as a high flux ratio double-lined spectroscopic binary permits the direct measurement of the planet's true mass as well as its atmospheric properties. After removing telluric absorption and cross-correlating with a model planetary spectrum dominated by water opacity, we measure a 6-sigma detection of the planet at K_p = 111 +- 5 km/s, with a 1-sigma upper limit on the spectroscopic flux ratio of 10^-4. This radial velocity leads to a planetary orbital inclination of i = 45+3-4degrees and a mass of M_P = 5.90+0.35-0.20 M_ Jup. We report the first detection of water vapor in the atmosphere of a non-transiting hot Jupiter, tau Boo b."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary method used to detect water vapor in the atmosphere of the hot Jupiter tau Boo b, and what is the significance of the measured radial velocity variation?\n\nA) High-resolution spectroscopy of the stellar spectrum, which allows for the direct measurement of the planet's true mass and atmospheric properties.\nB) Transit photometry, which measures the decrease in brightness of the star as the planet passes in front of it, and is used to infer the presence of water vapor.\nC) Radial velocity measurements, which detect the star's wobble caused by the planet's gravitational pull, and are used to infer the planet's mass and atmospheric properties.\nD) Transit spectroscopy, which measures the change in the star's spectrum as the planet passes in front of it, and is used to infer the presence of water vapor.\n\nCorrect Answer: C) Radial velocity measurements, which detect the star's wobble caused by the planet's gravitational pull, and are used to infer the planet's mass and atmospheric properties.\n\nExplanation: The question requires the test-taker to understand the method used to detect water vapor in the atmosphere of the hot Jupiter tau Boo b. The correct answer, radial velocity measurements, is the primary method used in the study, as mentioned in the abstract. The other options are incorrect because transit photometry and transit spectroscopy are not mentioned in the study, and high-resolution spectroscopy is used to measure the radial velocity variation, but not to detect water vapor directly. The test-taker needs to analyze the information provided and identify the correct method used to detect water vapor."}, "13": {"documentation": {"title": "An Efficient Technique for Text Compression", "source": "Md. Abul Kalam Azad, Rezwana Sharmeen, Shabbir Ahmad, and S. M.\n  Kamruzzaman", "docs_id": "1009.4981", "section": ["cs.IT", "cs.IR", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Efficient Technique for Text Compression. For storing a word or the whole text segment, we need a huge storage space. Typically a character requires 1 Byte for storing it in memory. Compression of the memory is very important for data management. In case of memory requirement compression for text data, lossless memory compression is needed. We are suggesting a lossless memory requirement compression method for text data compression. The proposed compression method will compress the text segment or the text file based on two level approaches firstly reduction and secondly compression. Reduction will be done using a word lookup table not using traditional indexing system, then compression will be done using currently available compression methods. The word lookup table will be a part of the operating system and the reduction will be done by the operating system. According to this method each word will be replaced by an address value. This method can quite effectively reduce the size of persistent memory required for text data. At the end of the first level compression with the use of word lookup table, a binary file containing the addresses will be generated. Since the proposed method does not use any compression algorithm in the first level so this file can be compressed using the popular compression algorithms and finally will provide a great deal of data compression on purely English text data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a word lookup table in the proposed lossless memory compression method for text data, and how does it differ from traditional indexing systems?\n\n**A)** The word lookup table reduces the size of persistent memory required for text data by replacing each word with a unique address value, allowing for more efficient compression.\n\n**B)** The word lookup table uses a traditional indexing system to compress text data, resulting in a more efficient compression ratio.\n\n**C)** The word lookup table is a part of the operating system, and its primary function is to compress text data using a combination of reduction and compression techniques.\n\n**D)** The word lookup table is not used in the proposed method, and instead, traditional indexing systems are used to compress text data.\n\n**Correct Answer:** A) The word lookup table reduces the size of persistent memory required for text data by replacing each word with a unique address value, allowing for more efficient compression.\n\n**Explanation:** The correct answer is A) because the word lookup table is used to reduce the size of persistent memory required for text data by replacing each word with a unique address value. This allows for more efficient compression, as the size of the memory required for text data is significantly reduced. The other options are incorrect because they either misrepresent the function of the word lookup table (B) or state that the word lookup table is not used in the proposed method (D), which is not the case. Option C) is also incorrect because it states that the word lookup table is a part of the operating system, but the documentation only mentions that the word lookup table will be a part of the operating system, not that it is used by the operating system."}, "14": {"documentation": {"title": "Toward Realistic Amorphous Topological Insulators", "source": "Marcio Costa, Gabriel R. Schleder, Marco Buongiorno Nardelli, Caio\n  Lewenkopf and Adalberto Fazzio", "docs_id": "1911.08215", "section": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Toward Realistic Amorphous Topological Insulators. The topological properties of materials are, until now, associated with the features of their crystalline structure, although translational symmetry is not an explicit requirement of the topological phases. Recent studies of hopping models on random lattices have demonstrated that amorphous model systems show a nontrivial topology. Using {\\it ab initio} calculations we show that two-dimensional amorphous materials can also display topological insulator properties. More specifically, we present a realistic state-of-the-art study of the electronic and transport properties of amorphous bismuthene systems, showing that these materials are topological insulators. These systems are characterized by the topological index $\\mathbb{Z}_{2}=1$ and bulk-edge duality, and their linear conductance is quantized, ${\\cal G}=2e^{2}/h$, for Fermi energies within the topological gap. Our study opens the path to the experimental and theoretical investigation of amorphous topological insulator materials."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary finding of the study on amorphous bismuthene systems, and how does it relate to the concept of topological insulators?\n\nA) The study shows that amorphous bismuthene systems exhibit a non-trivial topology, but do not display topological insulator properties.\nB) The study demonstrates that amorphous bismuthene systems are topological insulators, characterized by a topological index of $\\mathbb{Z}_{2}=1$ and bulk-edge duality.\nC) The study reveals that amorphous bismuthene systems have a trivial topology and do not exhibit topological insulator properties.\nD) The study shows that amorphous bismuthene systems are not topological insulators, but rather exhibit a non-trivial topology.\n\nCorrect Answer: B) The study demonstrates that amorphous bismuthene systems are topological insulators, characterized by a topological index of $\\mathbb{Z}_{2}=1$ and bulk-edge duality.\n\nExplanation: The correct answer is B) because the study explicitly states that the amorphous bismuthene systems are topological insulators, characterized by a topological index of $\\mathbb{Z}_{2}=1$ and bulk-edge duality. This is supported by the fact that the systems exhibit a topological gap and quantized linear conductance, ${\\cal G}=2e^{2}/h$, for Fermi energies within the topological gap."}, "15": {"documentation": {"title": "Heuristic Strategies in Uncertain Approval Voting Environments", "source": "Jaelle Scheuerman, Jason L. Harman, Nicholas Mattei, K. Brent Venable", "docs_id": "1912.00011", "section": ["cs.GT", "cs.AI", "cs.MA", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Heuristic Strategies in Uncertain Approval Voting Environments. In many collective decision making situations, agents vote to choose an alternative that best represents the preferences of the group. Agents may manipulate the vote to achieve a better outcome by voting in a way that does not reflect their true preferences. In real world voting scenarios, people often do not have complete information about other voter preferences and it can be computationally complex to identify a strategy that will maximize their expected utility. In such situations, it is often assumed that voters will vote truthfully rather than expending the effort to strategize. However, being truthful is just one possible heuristic that may be used. In this paper, we examine the effectiveness of heuristics in single winner and multi-winner approval voting scenarios with missing votes. In particular, we look at heuristics where a voter ignores information about other voting profiles and makes their decisions based solely on how much they like each candidate. In a behavioral experiment, we show that people vote truthfully in some situations and prioritize high utility candidates in others. We examine when these behaviors maximize expected utility and show how the structure of the voting environment affects both how well each heuristic performs and how humans employ these heuristics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nIn a single-winner approval voting scenario with missing votes, a voter has the following preferences:\n\n{A, B, C, D}\n\nThe voter's true preferences are:\n\n{A, B, C}\n\nHowever, due to missing votes, the voter is uncertain about the voting profiles of other voters. To maximize their expected utility, the voter should:\n\nA) Vote for candidate A, as it is their most preferred option\nB) Vote for candidate B, as it is their second-most preferred option\nC) Vote for candidate C, as it is their third-most preferred option\nD) Vote for candidate D, as it is their least preferred option\n\nCorrect Answer: A) Vote for candidate A, as it is their most preferred option\n\nExplanation: The voter's true preferences are {A, B, C}, and they should vote for their most preferred option, A, to maximize their expected utility. This is because the voter is uncertain about the voting profiles of other voters, and voting for A ensures that they are choosing the option that they are most likely to prefer. Voting for B, C, or D would result in a lower expected utility, as the voter may not be able to guarantee that they will prefer those options."}, "16": {"documentation": {"title": "Extremely low-frequency electromagnetic fields cause DNA strand breaks\n  in normal Vero cells", "source": "Cosmin Teodor Miha, Gabriela Vochita, Florin Brinza, Pincu Rotinberg", "docs_id": "1301.5418", "section": ["q-bio.SC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Extremely low-frequency electromagnetic fields cause DNA strand breaks\n  in normal Vero cells. Extremely low frequency electromagnetic fields aren't considered as a real carcinogenic agent despite the fact that some studies have showed impairment of the DNA integrity in different cells lines. The aim of this study was evaluation of the late effects of a 100 Hz and 5.6 mT electromagnetic field, applied continuously or discontinuously, on the DNA integrity of Vero cells assessed by alkaline Comet assay and by cell cycle analysis. Normal Vero cells were exposed to extremely low frequency electromagnetic fields (100 Hz, 5.6 mT) for 45 minutes. The Comet assay and cell cycle analysis were performed 48 hours after the treatment. Exposed samples presented an increase of the number of cells with high damaged DNA as compared with non-exposed cells. Quantitative evaluation of the comet assay showed a significantly ($<$0.001) increase of the tail lengths, of the quantity of DNA in tail and of Olive tail moments, respectively. The analysis of the registered comet indices showed that an extremely low frequency electromagnetic field of 100 Hz and 5.6 mT had a genotoxic impact on Vero cells. Cell cycle analysis showed an increase of the frequency of the cells in S phase, proving the occurrence of single strand breaks. The most probable mechanism of induction of the registered effects is the production of different types of reactive oxygen species."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the most likely mechanism of induction of DNA strand breaks in Vero cells exposed to extremely low frequency electromagnetic fields, as suggested by the study?\n\nA) The production of reactive nitrogen species\nB) The production of different types of reactive oxygen species\nC) The disruption of the cell membrane's lipid bilayer\nD) The alteration of the cell's redox balance\n\nCorrect Answer: B) The production of different types of reactive oxygen species\n\nExplanation: The study found that the exposure to extremely low frequency electromagnetic fields caused an increase in the number of cells with high damaged DNA, as well as an increase in the tail lengths, quantity of DNA in tail, and Olive tail moments in the Comet assay. Additionally, cell cycle analysis showed an increase in the frequency of cells in S phase, indicating single strand breaks. The most probable mechanism of induction of these effects is the production of different types of reactive oxygen species, which can cause DNA damage and strand breaks. This is consistent with the findings of the study and is supported by the scientific literature on the effects of electromagnetic fields on biological systems."}, "17": {"documentation": {"title": "Therapeutic hypothermia: quantification of the transition of core body\n  temperature using the flexible mixture bent-cable model for longitudinal data", "source": "Shahedul A Khan, Grace S Chiu, Joel A Dubin", "docs_id": "1210.3087", "section": ["stat.ME", "q-bio.TO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Therapeutic hypothermia: quantification of the transition of core body\n  temperature using the flexible mixture bent-cable model for longitudinal data. By reducing core body temperature, T_c, induced hypothermia is a therapeutic tool to prevent brain damage resulting from physical trauma. However, all physiological systems begin to slow down due to hypothermia that in turn can result in increased risk of mortality. Therefore, quantification of the transition of T_c to early hypothermia is of great clinical interest. Conceptually, T_c may exhibit an either gradual or abrupt transition. Bent-cable regression is an appealing statistical tool to model such data due to the model's flexibility and greatly interpretable regression coefficients. It handles more flexibly models that traditionally have been handled by low-order polynomial models (for gradual transition) or piecewise linear changepoint models (for abrupt change). We consider a rat model for humans to quantify the temporal trend of T_c to primarily address the question: What is the critical time point associated with a breakdown in the compensatory mechanisms following the start of hypothermia therapy? To this end, we develop a Bayesian modelling framework for bent-cable regression of longitudinal data to simultaneously account for gradual and abrupt transitions. Our analysis reveals that: (a) about 39% of rats exhibit a gradual transition in T_c; (b) the critical time point is approximately the same regardless of transition type; (c) both transition types show a significant increase of T_c followed by a significant decrease."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of therapeutic hypothermia, what is the primary advantage of using the bent-cable regression model for longitudinal data, and how does it differ from traditional models used to model gradual or abrupt transitions in core body temperature?\n\n**A)** The bent-cable regression model is more prone to overfitting than traditional models, making it less suitable for complex data.\n**B)** The bent-cable regression model is more flexible and interpretable than traditional models, allowing for the estimation of regression coefficients that capture both gradual and abrupt transitions in core body temperature.\n**C)** The bent-cable regression model is more computationally intensive than traditional models, requiring more resources to estimate parameters.\n**D)** The bent-cable regression model is less sensitive to outliers than traditional models, making it more robust to noisy data.\n\n**Correct Answer:** B) The bent-cable regression model is more flexible and interpretable than traditional models, allowing for the estimation of regression coefficients that capture both gradual and abrupt transitions in core body temperature.\n\n**Explanation:** The bent-cable regression model is an appealing statistical tool for modeling the transition of core body temperature due to therapeutic hypothermia because it can handle both gradual and abrupt transitions, which are difficult to model using traditional low-order polynomial models or piecewise linear changepoint models. The model's flexibility and interpretable regression coefficients make it a valuable tool for clinicians and researchers to understand the temporal trend of core body temperature and identify critical time points associated with a breakdown in compensatory mechanisms following the start of hypothermia therapy."}, "18": {"documentation": {"title": "CANet: An Unsupervised Intrusion Detection System for High Dimensional\n  CAN Bus Data", "source": "Markus Hanselmann, Thilo Strauss, Katharina Dormann, Holger Ulmer", "docs_id": "1906.02492", "section": ["cs.CR", "cs.LG", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "CANet: An Unsupervised Intrusion Detection System for High Dimensional\n  CAN Bus Data. We propose a novel neural network architecture for detecting intrusions on the CAN bus. The Controller Area Network (CAN) is the standard communication method between the Electronic Control Units (ECUs) of automobiles. However, CAN lacks security mechanisms and it has recently been shown that it can be attacked remotely. Hence, it is desirable to monitor CAN traffic to detect intrusions. In order to detect both, known and unknown intrusion scenarios, we consider a novel unsupervised learning approach which we call CANet. To our knowledge, this is the first deep learning based intrusion detection system (IDS) that takes individual CAN messages with different IDs and evaluates them in the moment they are received. This is a significant advancement because messages with different IDs are typically sent at different times and with different frequencies. Our method is evaluated on real and synthetic CAN data. For reproducibility of the method, our synthetic data is publicly available. A comparison with previous machine learning based methods shows that CANet outperforms them by a significant margin."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of the CANet intrusion detection system, and how does it differ from previous machine learning-based methods?\n\n**A)** CANet was developed to detect intrusions on the CAN bus due to the lack of security mechanisms in the Controller Area Network (CAN) protocol, and it differs from previous methods in that it uses a deep learning approach to evaluate individual CAN messages in real-time.\n\n**B)** CANet was developed to detect intrusions on the CAN bus due to the increasing threat of remote attacks on automotive systems, and it differs from previous methods in that it uses a supervised learning approach to classify CAN messages into known and unknown intrusion scenarios.\n\n**C)** CANet was developed to detect intrusions on the CAN bus due to the need for real-time monitoring of CAN traffic, and it differs from previous methods in that it uses a novel unsupervised learning approach to evaluate individual CAN messages in the moment they are received.\n\n**D)** CANet was developed to detect intrusions on the CAN bus due to the increasing demand for intrusion detection systems in the automotive industry, and it differs from previous methods in that it uses a hybrid approach combining deep learning and traditional machine learning techniques.\n\n**Correct Answer:** C) CANet was developed to detect intrusions on the CAN bus due to the need for real-time monitoring of CAN traffic, and it differs from previous methods in that it uses a novel unsupervised learning approach to evaluate individual CAN messages in the moment they are received.\n\n**Explanation:** The correct answer is C) because the documentation states that CANet is a novel unsupervised learning approach that evaluates individual CAN messages in the moment they are received, which is a key aspect of its design. The other options are incorrect because they either misrepresent the motivation behind CANet's development (A and B) or provide an incomplete or inaccurate description of its approach (D)."}, "19": {"documentation": {"title": "Modeling and analysis of the electromechanical behavior of\n  surface-bonded piezoelectric actuators using finite element method", "source": "Huangchao Yu and Xiaodong Wang", "docs_id": "1611.02375", "section": ["physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modeling and analysis of the electromechanical behavior of\n  surface-bonded piezoelectric actuators using finite element method. Piezoelectric actuators have been widely used to form a self-monitoring smart system to do Structural health monitoring (SHM). One of the most fundamental issues in using actuators is to determine the actuation effects being transferred from the actuators to the host structure. This report summaries the state of the art of modeling techniques for piezoelectric actuators and provides a numerical analysis of the static and dynamic electromechanical behavior of piezoelectric actuators surface-bonded to an elastic medium under in-plane mechanical and electric loads using finite element method. Also case study is conducted to study the effect of material properties, bonding layer and loading frequency using static and harmonic analysis of ANSYS. Finally, stresses and displacements are determined, and singularity behavior at the tips of the actuator is proved. The results indicate that material properties, bonding layers and frequency have a significant influence on the stresses transferred to the host structure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A surface-bonded piezoelectric actuator is used in a structural health monitoring (SHM) system. The actuator is subjected to in-plane mechanical and electric loads. Using the finite element method, determine the effect of the bonding layer on the stresses transferred to the host structure.\n\n**A)** The bonding layer has a negligible effect on the stresses transferred to the host structure.\n\n**B)** The bonding layer has a significant effect on the stresses transferred to the host structure, and its properties should be considered in the design of the actuator.\n\n**C)** The bonding layer is not a critical factor in determining the stresses transferred to the host structure, and its properties can be ignored in the analysis.\n\n**D)** The bonding layer is a critical factor in determining the stresses transferred to the host structure, and its properties should be carefully considered in the design of the actuator.\n\n**Correct Answer:** B) The bonding layer has a significant effect on the stresses transferred to the host structure, and its properties should be considered in the design of the actuator.\n\n**Explanation:** The correct answer is B) because the documentation states that the bonding layer has a significant influence on the stresses transferred to the host structure. The finite element method analysis conducted in the study showed that the bonding layer plays a crucial role in determining the electromechanical behavior of the piezoelectric actuator. Therefore, the properties of the bonding layer should be carefully considered in the design of the actuator to ensure accurate modeling and analysis.\n\n**Explanation for incorrect options:**\n\n* A) is incorrect because the documentation states that the bonding layer has a significant influence on the stresses transferred to the host structure.\n* C) is incorrect because the bonding layer is a critical factor in determining the stresses transferred to the host structure, and its properties should be carefully considered in the design of the actuator.\n* D) is incorrect because while the bonding layer is a critical factor, the documentation does not explicitly state that its properties should be carefully considered in the design of the actuator. However, option B is still the correct answer because the bonding layer does have a significant effect on the stresses transferred to the host structure."}, "20": {"documentation": {"title": "Uniform Interpolation Constrained Geodesic Learning on Data Manifold", "source": "Cong Geng, Jia Wang, Li Chen, Wenbo Bao, Chu Chu, Zhiyong Gao", "docs_id": "2002.04829", "section": ["cs.CV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Uniform Interpolation Constrained Geodesic Learning on Data Manifold. In this paper, we propose a method to learn a minimizing geodesic within a data manifold. Along the learned geodesic, our method can generate high-quality interpolations between two given data samples. Specifically, we use an autoencoder network to map data samples into latent space and perform interpolation via an interpolation network. We add prior geometric information to regularize our autoencoder for the convexity of representations so that for any given interpolation approach, the generated interpolations remain within the distribution of the data manifold. Before the learning of a geodesic, a proper Riemannianmetric should be defined. Therefore, we induce a Riemannian metric by the canonical metric in the Euclidean space which the data manifold is isometrically immersed in. Based on this defined Riemannian metric, we introduce a constant speed loss and a minimizing geodesic loss to regularize the interpolation network to generate uniform interpolation along the learned geodesic on the manifold. We provide a theoretical analysis of our model and use image translation as an example to demonstrate the effectiveness of our method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary objective of the proposed method in \"Uniform Interpolation Constrained Geodesic Learning on Data Manifold\" when learning a geodesic within a data manifold?\n\nA) To minimize the Euclidean distance between data samples\nB) To maximize the similarity between data samples\nC) To generate uniform interpolations along the learned geodesic on the manifold\nD) To reduce the complexity of the data manifold\n\nCorrect Answer: C) To generate uniform interpolations along the learned geodesic on the manifold\n\nExplanation: The correct answer is C) To generate uniform interpolations along the learned geodesic on the manifold. The paper proposes a method to learn a minimizing geodesic within a data manifold, and the primary objective is to generate high-quality interpolations between two given data samples along this geodesic. The method uses an autoencoder network to map data samples into latent space and an interpolation network to perform interpolation, and adds prior geometric information to regularize the autoencoder for the convexity of representations. The constant speed loss and minimizing geodesic loss are used to regularize the interpolation network to generate uniform interpolation along the learned geodesic on the manifold.\n\nCandidate A is incorrect because minimizing the Euclidean distance between data samples is not the primary objective of the method. Candidate B is incorrect because maximizing the similarity between data samples is not the primary objective of the method. Candidate D is incorrect because reducing the complexity of the data manifold is not the primary objective of the method."}, "21": {"documentation": {"title": "Aggregated knowledge from a small number of debates outperforms the\n  wisdom of large crowds", "source": "Joaquin Navajas, Tamara Niella, Gerry Garbulsky, Bahador Bahrami,\n  Mariano Sigman", "docs_id": "1703.00045", "section": ["cs.SI", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Aggregated knowledge from a small number of debates outperforms the\n  wisdom of large crowds. The aggregation of many independent estimates can outperform the most accurate individual judgment. This centenarian finding, popularly known as the wisdom of crowds, has been applied to problems ranging from the diagnosis of cancer to financial forecasting. It is widely believed that social influence undermines collective wisdom by reducing the diversity of opinions within the crowd. Here, we show that if a large crowd is structured in small independent groups, deliberation and social influence within groups improve the crowd's collective accuracy. We asked a live crowd (N=5180) to respond to general-knowledge questions (e.g., what is the height of the Eiffel Tower?). Participants first answered individually, then deliberated and made consensus decisions in groups of five, and finally provided revised individual estimates. We found that averaging consensus decisions was substantially more accurate than aggregating the initial independent opinions. Remarkably, combining as few as four consensus choices outperformed the wisdom of thousands of individuals."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** A researcher conducted an experiment where a large crowd of 5180 participants was divided into small groups of five to deliberate and make consensus decisions on general-knowledge questions. The results showed that averaging the consensus decisions was substantially more accurate than aggregating the initial independent opinions. However, when the researcher combined only four consensus choices, the accuracy was still higher than the wisdom of thousands of individuals. What can be inferred about the effect of social influence on collective wisdom in this experiment?\n\nA) Social influence reduces the accuracy of collective wisdom.\nB) Social influence has no effect on collective wisdom.\nC) Social influence improves collective wisdom when deliberation occurs.\nD) Social influence improves collective wisdom when deliberation does not occur.\n\n**Correct Answer:** C) Social influence improves collective wisdom when deliberation occurs."}, "22": {"documentation": {"title": "Quantum chaos in triangular billiards", "source": "\\v{C}rt Lozej and Giulio Casati and Toma\\v{z} Prosen", "docs_id": "2110.04168", "section": ["nlin.CD", "cond-mat.stat-mech", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantum chaos in triangular billiards. We present an extensive numerical study of spectral statistics and eigenfunctions of quantized triangular billiards. We compute two million consecutive eigenvalues for six representative cases of triangular billiards, three with generic angles with irrational ratios with $\\pi$, whose classical dynamics is presumably mixing, and three with exactly one angle rational with $\\pi$, which are presumably only weakly mixing or even only non-ergodic in case of right-triangles. We find excellent agreement of short and long range spectral statistics with the Gaussian orthogonal ensemble of random matrix theory for the most irrational generic triangle, while the other cases show small but significant deviations which are attributed either to scarring or super-scarring mechanism. This result, which extends the quantum chaos conjecture to systems with dynamical mixing in the absence of hard (Lyapunov) chaos, has been corroborated by analysing distributions of phase-space localisation measures of eigenstates and inspecting the structure of characteristic typical and atypical eigenfunctions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What can be inferred about the spectral statistics of quantized triangular billiards, and how does this relate to the concept of quantum chaos?\n\nA) The spectral statistics of quantized triangular billiards are always in agreement with the Gaussian orthogonal ensemble of random matrix theory, regardless of the triangle's angles.\n\nB) The spectral statistics of quantized triangular billiards show excellent agreement with the Gaussian orthogonal ensemble of random matrix theory for the most irrational generic triangle, but deviate significantly for rational angles.\n\nC) The spectral statistics of quantized triangular billiards are always in disagreement with the Gaussian orthogonal ensemble of random matrix theory, due to the presence of scarring or super-scarring mechanisms.\n\nD) The spectral statistics of quantized triangular billiards are only in agreement with the Gaussian orthogonal ensemble of random matrix theory for right-triangles, which are presumed to be non-ergodic.\n\nCorrect Answer: B) The spectral statistics of quantized triangular billiards show excellent agreement with the Gaussian orthogonal ensemble of random matrix theory for the most irrational generic triangle, but deviate significantly for rational angles.\n\nExplanation: The correct answer is B) because the documentation states that \"We find excellent agreement of short and long range spectral statistics with the Gaussian orthogonal ensemble of random matrix theory for the most irrational generic triangle, while the other cases show small but significant deviations which are attributed either to scarring or super-scarring mechanism.\" This indicates that the spectral statistics are in agreement for the most irrational generic triangle, but deviate for rational angles."}, "23": {"documentation": {"title": "Evaluation of techniques for predicting seizure Build up", "source": "Amira Hajjeji and Nawel Jmail and Abir Hadriche and Amal Ncibi and\n  Chokri Ben Amar", "docs_id": "1911.07081", "section": ["eess.SP", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evaluation of techniques for predicting seizure Build up. The analysis of electrophysiological signal of scalp: EEG (electroencephalography), MEG (magnetoencephalography) and depth (intracerebral EEG) IEEG is a way to delimit epileptogenic zone (EZ). These epileptic signals present two different activities (oscillations and spikes) which can be overlapped in the time frequency plane. Automatic recognition of epileptic seizure occurrence needs several preprocessing steps. In this study, we evaluated two filtering techniques: the stationary wavelet transforms (SWT) and the Despikifying in order to extract pre ictal gamma oscillations (bio markers of seizure build up). Then, we used a temporal basis set of Jmail et al 2017 as a preprocessing step to evaluate the performance of both technique. Moreover, we used time-frequency and spatio-temporal mapping of simulated and real data for both techniques in order to predict seizure build up (in time and space). We concluded that SWT can detect the oscillations, but Despikyfying is more robust than SWT in reconstructing pure pre ictal gamma oscillations and hence in predicting seizure build up."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Evaluate the effectiveness of two filtering techniques, Stationary Wavelet Transforms (SWT) and Despikifying, in extracting pre-ictal gamma oscillations from EEG signals, which are considered bio-markers of seizure build-up. How do the results of the study compare the performance of these two techniques in predicting seizure build-up in time and space?\n\n**A)** The study concludes that Despikifying is more effective than SWT in detecting oscillations, but SWT is more robust in reconstructing pure pre-ictal gamma oscillations. This suggests that Despikifying is better suited for predicting seizure build-up in real-time, while SWT is more suitable for long-term monitoring.\n\n**B)** The results indicate that SWT is more effective than Despikifying in extracting pre-ictal gamma oscillations, which is crucial for predicting seizure build-up. However, Despikifying is more robust in reconstructing pure oscillations, making it a better choice for long-term monitoring.\n\n**C)** The study shows that both SWT and Despikifying are equally effective in detecting oscillations, but Despikifying is more robust in reconstructing pure pre-ictal gamma oscillations. This suggests that both techniques can be used for predicting seizure build-up, but Despikifying is more suitable for real-time monitoring.\n\n**D)** The results of the study indicate that Despikifying is more effective than SWT in predicting seizure build-up, but only in the context of spatio-temporal mapping. SWT is more effective in time-frequency mapping, suggesting that the choice of technique depends on the specific application.\n\n**Correct Answer:** B) The results indicate that SWT is more effective than Despikifying in extracting pre-ictal gamma oscillations, which is crucial for predicting seizure build-up. However, Despikifying is more robust in reconstructing pure oscillations, making it a better choice for long-term monitoring.\n\n**Explanation:** The study concludes that SWT can detect oscillations, but Despikifying is more robust in reconstructing pure pre-ictal gamma oscillations. This suggests that SWT is more effective in extracting oscillations, which is crucial for predicting seizure build-up, while Despikifying is more suitable for long-term monitoring due to its ability to reconstruct pure oscillations."}, "24": {"documentation": {"title": "Lower bound the T-count via unitary stabilizer nullity", "source": "Jiaqing Jiang, Xin Wang", "docs_id": "2103.09999", "section": ["quant-ph", "cs.CC", "cs.IT", "hep-th", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lower bound the T-count via unitary stabilizer nullity. We introduce magic measures for multi-qubit quantum gates and establish lower bounds on the non-Clifford resources for fault-tolerant quantum computation. First, we introduce the stabilizer nullity of an arbitrary multi-qubit unitary, which is based on the subgroup of the quotient Pauli group associated with the unitary. This unitary stabilizer nullity extends the state stabilizer nullity by Beverland et al. to a dynamic version. We in particular show this magic measure has desirable properties such as sub-additivity under composition and additivity under tensor product. Second, we prove that a given unitary's stabilizer nullity is a lower bound for the T-count, utilizing the above properties in gate synthesis. Third, we compare the state and the unitary stabilizer nullity, proving that the lower bounds for the T-count obtained by the unitary stabilizer nullity are never less than the state stabilizer nullity. Moreover, we show an explicit $n$-qubit unitary family of unitary stabilizer nullity $2n$, which implies that its T-count is at least $2n$. This gives an example where the bounds derived by the unitary stabilizer nullity strictly outperform the state stabilizer nullity by a factor of $2$. We further connect the unitary stabilizer nullity and the state stabilizer nullity with auxiliary systems, showing that adding auxiliary systems and choosing proper stabilizer states can strictly improving the lower bound obtained by the state stabilizer nullity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a family of $n$-qubit unitaries $U$ with stabilizer nullity $k$. Suppose we want to lower bound the T-count of $U$ using the magic measure for multi-qubit quantum gates. Which of the following statements is true?\n\nA) The lower bound on the T-count is equal to the state stabilizer nullity $k$.\nB) The lower bound on the T-count is equal to the unitary stabilizer nullity $k$.\nC) The lower bound on the T-count is strictly greater than the state stabilizer nullity $k$.\nD) The lower bound on the T-count is strictly less than the unitary stabilizer nullity $k$.\n\nCorrect Answer: B) The lower bound on the T-count is equal to the unitary stabilizer nullity $k$.\n\nExplanation: The documentation states that the magic measure for multi-qubit quantum gates establishes lower bounds on the non-Clifford resources for fault-tolerant quantum computation. It also shows that a given unitary's stabilizer nullity is a lower bound for the T-count, utilizing the properties of the magic measure. Furthermore, the documentation proves that the lower bounds for the T-count obtained by the unitary stabilizer nullity are never less than the state stabilizer nullity. Therefore, the correct answer is B) The lower bound on the T-count is equal to the unitary stabilizer nullity $k$."}, "25": {"documentation": {"title": "Measurement of prompt and nonprompt J/psi production in pp and pPb\n  collisions at sqrt(s[NN]) = 5.02 TeV", "source": "CMS Collaboration", "docs_id": "1702.01462", "section": ["nucl-ex", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurement of prompt and nonprompt J/psi production in pp and pPb\n  collisions at sqrt(s[NN]) = 5.02 TeV. This paper reports the measurement of J/psi meson production in proton-proton (pp) and proton-lead (pPb) collisions at a center-of-mass energy per nucleon pair of 5.02 TeV by the CMS experiment at the LHC. The data samples used in the analysis correspond to integrated luminosities of 28 inverse picobarns and 35 inverse nanobarns for pp and pPb collisions, respectively. Prompt and nonprompt J/psi mesons, the latter produced from the decay of B mesons, are measured in their dimuon decay channels. Differential cross sections are measured in the transverse momentum range of 2 < pt < 30 GeV/c, and center-of-mass rapidity ranges of abs(y[CM]) < 2.4 (pp) and -2.87 < y[CM] < 1.93 (pPb). The nuclear modification factor, R[pPb], is measured as functions of both pt and y[CM]. Small modifications of the J/psi cross sections are observed in pPb relative to pp collisions. The ratio of J/psi production cross sections in p-going and Pb-going directions, R[FB], studied as functions of pt and y[CM], shows a significant decrease for increasing transverse energy deposited at large pseudorapidities. These results, which cover a wide kinematic range, provide new insight on the role of cold nuclear matter effects on prompt and nonprompt J/psi production."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the role of cold nuclear matter effects on prompt and nonprompt J/\u03c8 production in pPb collisions, based on the measurements of the nuclear modification factor R[pPb] and the ratio of J/\u03c8 production cross sections in p-going and Pb-going directions, R[FB], as reported in the paper?\n\nA) The measurements suggest that cold nuclear matter effects have a negligible impact on prompt and nonprompt J/\u03c8 production in pPb collisions.\n\nB) The results indicate that the ratio of J/\u03c8 production cross sections in p-going and Pb-going directions is independent of the transverse energy deposited at large pseudorapidities.\n\nC) The nuclear modification factor R[pPb] and the ratio of J/\u03c8 production cross sections in p-going and Pb-going directions, R[FB], both suggest that cold nuclear matter effects play a significant role in modifying the production of prompt and nonprompt J/\u03c8 mesons in pPb collisions.\n\nD) The measurements imply that the production of prompt and nonprompt J/\u03c8 mesons is not affected by the presence of cold nuclear matter in pPb collisions.\n\nCorrect Answer: C) The nuclear modification factor R[pPb] and the ratio of J/\u03c8 production cross sections in p-going and Pb-going directions, R[FB], both suggest that cold nuclear matter effects play a significant role in modifying the production of prompt and nonprompt J/\u03c8 mesons in pPb collisions.\n\nExplanation: The correct answer is C) because the measurements of R[pPb] and R[FB] indicate that cold nuclear matter effects do indeed play a significant role in modifying the production of prompt and nonprompt J/\u03c8 mesons in pPb collisions. Specifically, the ratio of J/\u03c8 production cross sections in p-going and Pb-going directions shows a significant decrease for increasing transverse energy deposited at large pseudorapidities, which suggests that cold nuclear matter effects are influencing the production of J/\u03c8 mesons. The nuclear modification factor R[pPb] also shows modifications of the J/\u03c8 cross sections as a function of pt and y[CM], which further supports the conclusion that cold nuclear matter effects are at play."}, "26": {"documentation": {"title": "Effects of forecast errors on optimal utilisation in aggregate\n  production planning with stochastic customer demand", "source": "Klaus Altendorfer, Thomas Felberbauer, Herbert Jodlbauer", "docs_id": "1812.00773", "section": ["math.OC", "q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effects of forecast errors on optimal utilisation in aggregate\n  production planning with stochastic customer demand. The hierarchical structure of production planning has the advantage of assigning different decision variables to their respective time horizons and therefore ensures their manageability. However, the restrictive structure of this top-down approach implying that upper level decisions are the constraints for lower level decisions also has its shortcomings. One problem that occurs is that deterministic mixed integer decision problems are often used for long-term planning, but the real production system faces a set of stochastic influences. Therefore, a planned utilisation factor has to be included into this deterministic aggregate planning problem. In practice, this decision is often based on past data and not consciously taken. In this paper, the effect of long-term forecast error on the optimal planned utilisation factor is evaluated for a production system facing stochastic demand and the benefit of exploiting this decision's potential is discussed. Overall costs including capacity, backorder and inventory costs, are determined with simulation for different multi-stage and multi-item production system structures. The results show that the planned utilisation factor used in the aggregate planning problem has a high influence on optimal costs. Additionally, the negative effect of forecast errors is evaluated and discussed in detail for different production system environments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In a production planning system with stochastic customer demand, what is the primary advantage of using a hierarchical structure in aggregate planning, and how does it impact the incorporation of planned utilisation factors?\n\nA) The hierarchical structure allows for more flexibility in decision-making, enabling the incorporation of planned utilisation factors in a more nuanced manner.\nB) The hierarchical structure ensures that upper-level decisions are the constraints for lower-level decisions, which can lead to more accurate planned utilisation factors.\nC) The hierarchical structure simplifies the decision-making process by assigning different decision variables to their respective time horizons, making it easier to manage the production system.\nD) The hierarchical structure does not impact the incorporation of planned utilisation factors, as it is a top-down approach that relies solely on deterministic mixed integer decision problems.\n\n**Correct Answer:** B) The hierarchical structure ensures that upper-level decisions are the constraints for lower-level decisions, which can lead to more accurate planned utilisation factors.\n\n**Explanation:** The hierarchical structure of production planning has the advantage of assigning different decision variables to their respective time horizons, making it easier to manage the production system. However, this top-down approach also has the disadvantage of implying that upper-level decisions are the constraints for lower-level decisions, which can lead to inaccurate planned utilisation factors. The correct answer, B, highlights this trade-off and the importance of considering the impact of planned utilisation factors in aggregate planning problems."}, "27": {"documentation": {"title": "Stochastic Models of Neural Plasticity: A Scaling Approach", "source": "Philippe Robert and Gaetan Vignoud", "docs_id": "2106.04845", "section": ["math.PR", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stochastic Models of Neural Plasticity: A Scaling Approach. In neuroscience, synaptic plasticity refers to the set of mechanisms driving the dynamics of neuronal connections, called synapses and represented by a scalar value, the synaptic weight. A Spike-Timing Dependent Plasticity (STDP) rule is a biologically-based model representing the time evolution of the synaptic weight as a functional of the past spiking activity of adjacent neurons. A general mathematical framework has been introduced in~arXiv:2010.08195. In this paper we develop and investigate a scaling approach of these models based on several biological assumptions. Experiments show that long-term synaptic plasticity evolves on a much slower timescale than the cellular mechanisms driving the activity of neuronal cells, like their spiking activity or the concentration of various chemical components created/suppressed by this spiking activity. For this reason, a scaled version of the stochastic model of~arXiv:2010.08195 is introduced and a limit theorem, an averaging principle, is stated for a large class of plasticity kernels. A companion paper~arXiv:2010.08790 is entirely devoted to the tightness properties used to prove these convergence results. These averaging principles are used to study two important STDP models: pair-based rules and calcium-based rules. Our results are compared with the approximations of neuroscience STDP models. A class of discrete models of STDP rules is also investigated for the analytical tractability of its limiting dynamical system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Stochastic Models of Neural Plasticity: A Scaling Approach, what is the primary assumption underlying the development of a scaled version of the stochastic model of arXiv:2010.08195, and how does this assumption relate to the timescales of cellular mechanisms driving neuronal activity?\n\n**A)** The primary assumption is that synaptic plasticity evolves on a timescale comparable to the cellular mechanisms driving neuronal activity, and this assumption is used to justify the introduction of a scaled model that neglects the slower timescale of synaptic plasticity.\n\n**B)** The primary assumption is that synaptic plasticity evolves on a much slower timescale than the cellular mechanisms driving neuronal activity, and this assumption is used to justify the introduction of a scaled model that averages over the slower timescale of synaptic plasticity.\n\n**C)** The primary assumption is that the stochastic model of arXiv:2010.08195 is sufficient to capture the dynamics of synaptic plasticity, and this assumption is used to justify the development of a companion paper on tightness properties.\n\n**D)** The primary assumption is that the scaling approach is based on a mathematical framework introduced in arXiv:2010.08790, and this assumption is used to justify the development of a companion paper on tightness properties.\n\n**Correct Answer:** B) The primary assumption is that synaptic plasticity evolves on a much slower timescale than the cellular mechanisms driving neuronal activity, and this assumption is used to justify the introduction of a scaled model that averages over the slower timescale of synaptic plasticity.\n\n**Explanation:** The correct answer is B) because the documentation states that \"experiments show that long-term synaptic plasticity evolves on a much slower timescale than the cellular mechanisms driving the activity of neuronal cells...\". This implies that the primary assumption underlying the development of the scaled model is that synaptic plasticity evolves on a slower timescale than the cellular mechanisms driving neuronal activity. The scaled model is then introduced to average over this slower timescale, allowing for a more tractable mathematical framework."}, "28": {"documentation": {"title": "RBFs methods for null control problems of the Stokes system with\n  Dirichlet and Navier slip boundary conditions", "source": "Pedro Gonz\\'alez Casanova, Louis Breton and Cristhian Montoya", "docs_id": "1811.00459", "section": ["math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "RBFs methods for null control problems of the Stokes system with\n  Dirichlet and Navier slip boundary conditions. The purpose of this article is to introduce radial basis function, (RBFs), methods for solving null control problems for the Stokes system with few internal scalar controls and Dirichlet or Navier slip boundary conditions. To the best of our knowledge, it has not been reported in the literature any numerical approximation through RBFs to solve the direct Stokes problem with Navier slip boundary conditions. In this paper we fill this gap to show its application for solving the null control problem for the Stokes system. To achieve this goal, we introduce two radial basis function solvers, one global and the other local, to discretized the primal and adjoint systems related to the control problem. Both techniques are based on divergence free global RBFs. Stability analysis for these methods is performed in terms of the spectral radius of the corresponding Gram matrices. By using a conjugate gradient algorithm, adapted to the radial basis function setting, the control problem is solved. Several test problems in two dimensions are numerically solved by these RBFs methods to test their feasibility. The solutions to these problems are also obtained by finite elements techniques, (FE), to compare their relative performance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary purpose of introducing radial basis function (RBF) solvers in the context of solving null control problems for the Stokes system with Dirichlet and Navier slip boundary conditions?\n\nA) To improve the stability of the numerical approximation\nB) To reduce the computational complexity of the problem\nC) To provide a novel approach for solving the direct Stokes problem with Navier slip boundary conditions\nD) To enhance the accuracy of the finite element method\n\n**Correct Answer:** C) To provide a novel approach for solving the direct Stokes problem with Navier slip boundary conditions\n\n**Explanation:** The correct answer is C) because the article explicitly states that the purpose of the paper is to introduce RBF methods for solving null control problems for the Stokes system with few internal scalar controls and Dirichlet or Navier slip boundary conditions, and that it fills a gap in the literature by applying RBFs to the direct Stokes problem with Navier slip boundary conditions.\n\n**Candidate A (A)**: This answer is incorrect because while stability analysis is performed in terms of the spectral radius of the corresponding Gram matrices, it is not the primary purpose of introducing RBF solvers.\n\n**Candidate B (B)**: This answer is incorrect because the article does not mention reducing the computational complexity as a primary purpose of introducing RBF solvers.\n\n**Candidate D (D)**: This answer is incorrect because the article is focused on introducing RBF methods for solving null control problems, not enhancing the accuracy of the finite element method."}, "29": {"documentation": {"title": "A Logistic-Harvest Model with Allee Effect under Multiplicative Noise", "source": "Almaz Tesfay, Daniel Tesfay, James Brannan, Jinqiao Duan", "docs_id": "2008.01692", "section": ["q-bio.PE", "math.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Logistic-Harvest Model with Allee Effect under Multiplicative Noise. This work is devoted to the study of a stochastic logistic growth model with and without the Allee effect. Such a model describes the evolution of a population under environmental stochastic fluctuations and is in the form of a stochastic differential equation driven by multiplicative Gaussian noise. With the help of the associated Fokker-Planck equation, we analyze the population extinction probability and the probability of reaching a large population size before reaching a small one. We further study the impact of the harvest rate, noise intensity, and the Allee effect on population evolution. The analysis and numerical experiments show that if the noise intensity and harvest rate are small, the population grows exponentially, and upon reaching the carrying capacity, the population size fluctuates around it. In the stochastic logistic-harvest model without the Allee effect, when noise intensity becomes small (or goes to zero), the stationary probability density becomes more acute and its maximum point approaches one. However, for large noise intensity and harvest rate, the population size fluctuates wildly and does not grow exponentially to the carrying capacity. So as far as biological meanings are concerned, we must catch at small values of noise intensity and harvest rate. Finally, we discuss the biological implications of our results."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the impact of the Allee effect on the population extinction probability in the stochastic logistic-harvest model with multiplicative noise. How does the Allee effect influence the probability of reaching a large population size before reaching a small one?\n\n**A)** The Allee effect has no significant impact on the population extinction probability, as it only affects the growth rate of the population.\n\n**B)** The Allee effect increases the population extinction probability, as it reduces the growth rate of the population when the population size is below a certain threshold.\n\n**C)** The Allee effect decreases the population extinction probability, as it increases the growth rate of the population when the population size is below a certain threshold, allowing the population to recover more quickly.\n\n**D)** The Allee effect has a negligible impact on the probability of reaching a large population size before reaching a small one, as it only affects the growth rate of the population in the early stages of its evolution.\n\n**Correct Answer:** B) The Allee effect increases the population extinction probability, as it reduces the growth rate of the population when the population size is below a certain threshold.\n\n**Explanation:** According to the documentation, the Allee effect reduces the growth rate of the population when the population size is below a certain threshold, which increases the population extinction probability. This is because the Allee effect makes it more difficult for the population to recover from small population sizes, leading to a higher likelihood of extinction."}, "30": {"documentation": {"title": "Field redefinitions, Weyl invariance, and nature of mavericks", "source": "Predrag Dominis Prester", "docs_id": "1405.1941", "section": ["gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Field redefinitions, Weyl invariance, and nature of mavericks. In the theories of gravity with non-minimally coupled scalar fields there are \"mavericks\" -- unexpected solutions with odd properties, e.g., black holes with scalar hair in theories with scalar potential bounded from below. Probably the most famous example is Bocharova-Bronnikov-Melnikov-Bekenstein (BBMB) black hole solution in a theory with a scalar field conformally coupled to the gravity and with vanishing potential. Its existence naively violates no-hair conjecture without violating no-hair theorems because of the singular behavior of the scalar field at the horizon. Despite being discovered more than 40 years ago, nature of BBMB solution is still the subject of research and debate. We argue here that the key in understanding nature of maverick solutions is the proper choice of field redefinition schemes in which the solutions are regular. It appears that in such \"regular\" schemes mavericks have different physical interpretations, in particular they are not elementary but composite objects. For example, BBMB solution is not an extremal black hole but a collection of a wormhole and a naked singularity. In the process we show that Weyl-invariant formulation of gravity is a perfect tool for such analyzes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of non-minimally coupled scalar fields in theories of gravity, what is the key to understanding the nature of \"maverick solutions\" such as the Bocharova-Bronnikov-Melnikov-Bekenstein (BBMB) black hole solution?\n\nA) The existence of a scalar field with vanishing potential\nB) The singular behavior of the scalar field at the horizon\nC) The proper choice of field redefinition schemes in which the solutions are regular\nD) The Weyl-invariant formulation of gravity is not necessary for analyzing maverick solutions\n\nCorrect Answer: C) The proper choice of field redefinition schemes in which the solutions are regular\n\nExplanation: The correct answer is C) The proper choice of field redefinition schemes in which the solutions are regular. The text states that \"the key in understanding nature of maverick solutions is the proper choice of field redefinition schemes in which the solutions are regular\". This suggests that the choice of field redefinition scheme is crucial in understanding the nature of maverick solutions, and that regular solutions are a key aspect of this understanding.\n\nThe other options are incorrect because:\n\nA) The existence of a scalar field with vanishing potential is a characteristic of the BBMB black hole solution, but it is not the key to understanding its nature.\n\nB) The singular behavior of the scalar field at the horizon is a feature of the BBMB black hole solution, but it is not the key to understanding its nature.\n\nD) The Weyl-invariant formulation of gravity is actually a tool that is used to analyze maverick solutions, rather than a reason why they are not understood."}, "31": {"documentation": {"title": "Wavelet Based QRS Complex Detection of ECG Signal", "source": "Sayantan Mukhopadhyay, Shouvik Biswas, Anamitra Bardhan Roy, Nilanjan\n  Dey", "docs_id": "1209.1563", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Wavelet Based QRS Complex Detection of ECG Signal. The Electrocardiogram (ECG) is a sensitive diagnostic tool that is used to detect various cardiovascular diseases by measuring and recording the electrical activity of the heart in exquisite detail. A wide range of heart condition is determined by thorough examination of the features of the ECG report. Automatic extraction of time plane features is important for identification of vital cardiac diseases. This paper presents a multi-resolution wavelet transform based system for detection 'P', 'Q', 'R', 'S', 'T' peaks complex from original ECG signal. 'R-R' time lapse is an important minutia of the ECG signal that corresponds to the heartbeat of the concerned person. Abrupt increase in height of the 'R' wave or changes in the measurement of the 'R-R' denote various anomalies of human heart. Similarly 'P-P', 'Q-Q', 'S-S', 'T-T' also corresponds to different anomalies of heart and their peak amplitude also envisages other cardiac diseases. In this proposed method the 'PQRST' peaks are marked and stored over the entire signal and the time interval between two consecutive 'R' peaks and other peaks interval are measured to detect anomalies in behavior of heart, if any. The peaks are achieved by the composition of Daubeheissub bands wavelet of original ECG signal. The accuracy of the 'PQRST' complex detection and interval measurement is achieved up to 100% with high exactitude by processing and thresholding the original ECG signal."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of the proposed wavelet-based system for detecting 'P', 'Q', 'R', 'S', 'T' peaks complex from the original ECG signal?\n\n**A)** To analyze the amplitude of the 'R' wave for detecting cardiac arrhythmias.\n**B)** To measure the time interval between consecutive 'R' peaks and other peaks to identify anomalies in heart behavior.\n**C)** To apply a thresholding technique to the original ECG signal to enhance the visibility of the 'PQRST' peaks.\n**D)** To use a multi-resolution wavelet transform to decompose the ECG signal into different frequency bands.\n\n**Correct Answer:** D) To use a multi-resolution wavelet transform to decompose the ECG signal into different frequency bands.\n\n**Explanation:** The proposed system uses a multi-resolution wavelet transform to decompose the original ECG signal into different frequency bands, which allows for the detection of 'PQRST' peaks complex with high accuracy. This is evident from the statement in the documentation that \"The peaks are achieved by the composition of Daubechiesub bands wavelet of original ECG signal.\" This indicates that the primary objective of the system is to use wavelet transform to decompose the ECG signal, making option D the correct answer."}, "32": {"documentation": {"title": "Visualization of Stress Wave Propagation via Air-Coupled Acoustic\n  Emission Sensors", "source": "J. Rivey, G. Lee, J. Yang, Y. Kim, S. Kim", "docs_id": "1603.06298", "section": ["nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Visualization of Stress Wave Propagation via Air-Coupled Acoustic\n  Emission Sensors. We experimentally demonstrate the feasibility of visualizing stress waves propagating in plates using air-coupled acoustic emission sensors. Specifically, we employ a device that embeds arrays of microphones around an optical lens in a helical pattern. By implementing a beamforming technique, this remote sensing system allows us to record wave propagation events in situ via a single-shot and full-field measurement. This is a significant improvement over the conventional wave propagation tracking approaches based on laser doppler vibrometry or digital image correlation techniques. In this paper, we focus on demonstrating the feasibility and efficacy of this air-coupled acoustic emission technique using large metallic plates exposed to external impacts. The visualization results of stress wave propagation will be shown under various impact scenarios. Such wave visualization capability is of tremendous importance from a structural health monitoring and nondestructive evaluation (SHM/NDE) standpoint. The proposed technique can be used to characterize and localize damage by detecting the attenuation, reflection, and scattering of stress waves that occurs at damage locations. This can ultimately lead to the development of new SHM/NDE methods for identifying hidden cracks or delaminations in metallic or composite plate structures simultaneously negating the need for mounted contact sensors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed air-coupled acoustic emission technique over conventional wave propagation tracking approaches, and how does it relate to the field of structural health monitoring and nondestructive evaluation (SHM/NDE)?\n\n**A)** The proposed technique allows for real-time monitoring of stress wave propagation, enabling faster damage detection and characterization. However, it requires a significant amount of computational resources and expertise in beamforming techniques.\n\n**B)** The proposed technique enables full-field measurement of wave propagation events, allowing for the visualization of stress wave propagation in situ. This is a significant improvement over conventional methods, which often rely on laser doppler vibrometry or digital image correlation techniques that are limited to point-wise measurements.\n\n**C)** The proposed technique is more accurate and reliable than conventional methods, as it uses multiple microphones to record wave propagation events and can detect the attenuation, reflection, and scattering of stress waves at damage locations.\n\n**D)** The proposed technique is more cost-effective and easier to implement than conventional methods, as it uses a single device that can be placed remotely and does not require the installation of contact sensors.\n\n**Correct Answer:** B) The proposed technique enables full-field measurement of wave propagation events, allowing for the visualization of stress wave propagation in situ. This is a significant improvement over conventional methods, which often rely on laser doppler vibrometry or digital image correlation techniques that are limited to point-wise measurements.\n\n**Explanation:** The correct answer is B) because the proposed technique enables full-field measurement of wave propagation events, allowing for the visualization of stress wave propagation in situ. This is a significant improvement over conventional methods, which often rely on laser doppler vibrometry or digital image correlation techniques that are limited to point-wise measurements. The other options are incorrect because they do not accurately capture the primary advantage of the proposed technique. Option A is incorrect because while the technique does allow for real-time monitoring, it is not necessarily faster or more accurate than conventional methods. Option C is incorrect because while the technique can detect the attenuation, reflection, and scattering of stress waves, this is not its primary advantage. Option D is incorrect because the technique is not necessarily more cost-effective or easier to implement than conventional methods."}, "33": {"documentation": {"title": "Full-Duplex MIMO Relaying: Achievable Rates under Limited Dynamic Range", "source": "Brian P. Day, Adam R. Margetts, Daniel W. Bliss, and Philip Schniter", "docs_id": "1111.2618", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Full-Duplex MIMO Relaying: Achievable Rates under Limited Dynamic Range. In this paper we consider the problem of full-duplex multiple-input multiple-output (MIMO) relaying between multi-antenna source and destination nodes. The principal difficulty in implementing such a system is that, due to the limited attenuation between the relay's transmit and receive antenna arrays, the relay's outgoing signal may overwhelm its limited-dynamic-range input circuitry, making it difficult---if not impossible---to recover the desired incoming signal. While explicitly modeling transmitter/receiver dynamic-range limitations and channel estimation error, we derive tight upper and lower bounds on the end-to-end achievable rate of decode-and-forward-based full-duplex MIMO relay systems, and propose a transmission scheme based on maximization of the lower bound. The maximization requires us to (numerically) solve a nonconvex optimization problem, for which we detail a novel approach based on bisection search and gradient projection. To gain insights into system design tradeoffs, we also derive an analytic approximation to the achievable rate and numerically demonstrate its accuracy. We then study the behavior of the achievable rate as a function of signal-to-noise ratio, interference-to-noise ratio, transmitter/receiver dynamic range, number of antennas, and training length, using optimized half-duplex signaling as a baseline."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of full-duplex MIMO relaying, what is the primary challenge in implementing such a system, and how does it impact the recovery of the desired incoming signal?\n\n**A)** The primary challenge is the limited bandwidth between the source and destination nodes, making it difficult to transmit and receive signals simultaneously. This challenge is mitigated by using orthogonal frequency-division multiplexing (OFDM) to separate the transmit and receive signals.\n\n**B)** The primary challenge is the limited attenuation between the relay's transmit and receive antenna arrays, which can overwhelm the relay's input circuitry and make it difficult to recover the desired incoming signal. This challenge is addressed by using a decode-and-forward protocol to amplify the incoming signal.\n\n**C)** The primary challenge is the limited number of antennas at the source and destination nodes, which restricts the achievable rate of the system. This challenge is mitigated by using multiple antennas to increase the signal-to-noise ratio (SNR) and improve the system's capacity.\n\n**D)** The primary challenge is the limited dynamic range of the relay's input circuitry, which can lead to distortion and interference in the received signal. This challenge is addressed by using a transmission scheme that maximizes the lower bound on the achievable rate, such as the one proposed in the paper.\n\n**Correct Answer:** D) The primary challenge is the limited dynamic range of the relay's input circuitry, which can lead to distortion and interference in the received signal. This challenge is addressed by using a transmission scheme that maximizes the lower bound on the achievable rate, such as the one proposed in the paper.\n\n**Explanation:** The correct answer is D) because the paper explicitly states that the primary difficulty in implementing full-duplex MIMO relaying is the limited attenuation between the relay's transmit and receive antenna arrays, which can overwhelm the relay's limited-dynamic-range input circuitry. The transmission scheme proposed in the paper addresses this challenge by maximizing the lower bound on the achievable rate, which is a key component of the solution. The other options are incorrect because they do not accurately reflect the primary challenge and solution to the problem described in the paper."}, "34": {"documentation": {"title": "Obliquity of an Earth-like planet from frequency modulation of its\n  direct imaged lightcurve: mock analysis from general circulation model\n  simulation", "source": "Yuta Nakagawa (1), Takanori Kodama (2), Masaki Ishiwatari (3), Hajime\n  Kawahara (1), Yasushi Suto (1), Yoshiyuki O. Takahashi (4), George L.\n  Hashimoto (5), Kiyoshi Kuramoto (3), Kensuke Nakajima (6), Shin-ichi Takehiro\n  (7), and Yoshi-Yuki Hayashi (4), ( (1) Univ. of Tokyo, (2) Univ. of Bordeaux,\n  (3) Hokkaido Univ. (4) Kobe Univ. (5) Okayama Univ. (6) Kyushu Univ. (7)\n  Kyoto Univ.)", "docs_id": "2006.11437", "section": ["astro-ph.EP", "astro-ph.IM", "astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Obliquity of an Earth-like planet from frequency modulation of its\n  direct imaged lightcurve: mock analysis from general circulation model\n  simulation. Direct-imaging techniques of exoplanets have made significant progress recently, and will eventually enable to monitor photometric and spectroscopic signals of earth-like habitable planets in the future. The presence of clouds, however, would remain as one of the most uncertain components in deciphering such direct-imaged signals of planets. We attempt to examine how the planetary obliquity produce different cloud patterns by performing a series of GCM (General Circulation Model) simulation runs using a set of parameters relevant for our Earth. Then we use the simulated photometric lightcurves to compute their frequency modulation due to the planetary spin-orbit coupling over an entire orbital period, and attempt to see to what extent one can estimate the obliquity of an Earth-twin. We find that it is possible to estimate the obliquity of an Earth-twin within the uncertainty of several degrees with a dedicated 4 m space telescope at 10 pc away from the system if the stellar flux is completely blocked. While our conclusion is based on several idealized assumptions, a frequency modulation of a directly-imaged earth-like planet offers a unique methodology to determine its obliquity."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the primary limitation in determining the obliquity of an Earth-like planet using direct-imaging techniques, and how might this limitation be addressed?\n\nA) The presence of clouds, which would obscure the planet's lightcurve and make it difficult to detect frequency modulation.\nB) The distance of the planet from its star, which would require a telescope with extremely high resolution to resolve the planet's orbit.\nC) The uncertainty in the planet's atmospheric composition, which would affect the frequency modulation of its lightcurve.\nD) The presence of stellar activity, which would introduce noise into the planet's lightcurve and make it difficult to detect frequency modulation.\n\nCorrect Answer: A) The presence of clouds, which would obscure the planet's lightcurve and make it difficult to detect frequency modulation.\n\nExplanation: The question requires the test-taker to understand the main limitation in determining the obliquity of an Earth-like planet using direct-imaging techniques, as discussed in the provided Arxiv documentation. The correct answer, A) The presence of clouds, is supported by the text, which states that \"the presence of clouds, however, would remain as one of the most uncertain components in deciphering such direct-imaged signals of planets.\" The other options are incorrect because they do not accurately reflect the main limitation discussed in the text. Option B is incorrect because the distance of the planet from its star is not mentioned as a limitation. Option C is incorrect because the uncertainty in the planet's atmospheric composition is not mentioned as a limitation. Option D is incorrect because stellar activity is not mentioned as a limitation in the text. \n\nThis question requires the test-taker to analyze the text, identify the main limitation, and choose the correct answer. It also requires the test-taker to understand the context and the implications of the limitation, making it a more challenging question."}, "35": {"documentation": {"title": "Learning Directed Acyclic Graphs with Penalized Neighbourhood Regression", "source": "Bryon Aragam, Arash A. Amini, Qing Zhou", "docs_id": "1511.08963", "section": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning Directed Acyclic Graphs with Penalized Neighbourhood Regression. We study a family of regularized score-based estimators for learning the structure of a directed acyclic graph (DAG) for a multivariate normal distribution from high-dimensional data with $p\\gg n$. Our main results establish support recovery guarantees and deviation bounds for a family of penalized least-squares estimators under concave regularization without assuming prior knowledge of a variable ordering. These results apply to a variety of practical situations that allow for arbitrary nondegenerate covariance structures as well as many popular regularizers including the MCP, SCAD, $\\ell_{0}$ and $\\ell_{1}$. The proof relies on interpreting a DAG as a recursive linear structural equation model, which reduces the estimation problem to a series of neighbourhood regressions. We provide a novel statistical analysis of these neighbourhood problems, establishing uniform control over the superexponential family of neighbourhoods associated with a Gaussian distribution. We then apply these results to study the statistical properties of score-based DAG estimators, learning causal DAGs, and inferring conditional independence relations via graphical models. Our results yield---for the first time---finite-sample guarantees for structure learning of Gaussian DAGs in high-dimensions via score-based estimation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary assumption made by the authors of the paper \"Learning Directed Acyclic Graphs with Penalized Neighbourhood Regression\" regarding the regularization used in their score-based estimators?\n\nA) The regularization is convex and does not affect the estimation of the DAG structure.\nB) The regularization is concave and allows for arbitrary nondegenerate covariance structures.\nC) The regularization is linear and does not provide any support recovery guarantees.\nD) The regularization is non-convex and requires prior knowledge of a variable ordering.\n\n**Correct Answer:** B) The regularization is concave and allows for arbitrary nondegenerate covariance structures.\n\n**Explanation:** The paper states that the authors study a family of regularized score-based estimators for learning the structure of a directed acyclic graph (DAG) for a multivariate normal distribution. The main results establish support recovery guarantees and deviation bounds for a family of penalized least-squares estimators under concave regularization. This implies that the regularization used in the estimators is concave and allows for arbitrary nondegenerate covariance structures.\n\n**Candidate A:** This option is incorrect because the paper does not assume that the regularization is convex. In fact, the authors explicitly state that the regularization is concave.\n\n**Candidate B:** This option is correct because the paper states that the regularization used in the estimators is concave and allows for arbitrary nondegenerate covariance structures.\n\n**Candidate C:** This option is incorrect because the paper does not assume that the regularization is linear. In fact, the authors use a variety of regularizers, including the MCP, SCAD, $\\ell_{0}$, and $\\ell_{1}$, which are all non-linear.\n\n**Candidate D:** This option is incorrect because the paper does not require prior knowledge of a variable ordering. In fact, the authors establish support recovery guarantees without assuming prior knowledge of a variable ordering."}, "36": {"documentation": {"title": "A Bayesian nonparametric approach to count-min sketch under power-law\n  data streams", "source": "Emanuele Dolera, Stefano Favaro, Stefano Peluchetti", "docs_id": "2102.03743", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Bayesian nonparametric approach to count-min sketch under power-law\n  data streams. The count-min sketch (CMS) is a randomized data structure that provides estimates of tokens' frequencies in a large data stream using a compressed representation of the data by random hashing. In this paper, we rely on a recent Bayesian nonparametric (BNP) view on the CMS to develop a novel learning-augmented CMS under power-law data streams. We assume that tokens in the stream are drawn from an unknown discrete distribution, which is endowed with a normalized inverse Gaussian process (NIGP) prior. Then, using distributional properties of the NIGP, we compute the posterior distribution of a token's frequency in the stream, given the hashed data, and in turn corresponding BNP estimates. Applications to synthetic and real data show that our approach achieves a remarkable performance in the estimation of low-frequency tokens. This is known to be a desirable feature in the context of natural language processing, where it is indeed common in the context of the power-law behaviour of the data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary assumption made about the distribution of tokens in the data stream in the Bayesian nonparametric approach to count-min sketch under power-law data streams?\n\nA) The tokens are drawn from a discrete distribution with a known mean and variance.\nB) The tokens are drawn from an unknown discrete distribution with a normalized inverse Gaussian process (NIGP) prior.\nC) The tokens are drawn from a continuous distribution with a known probability density function.\nD) The tokens are drawn from a discrete distribution with a power-law distribution.\n\n**Correct Answer:** B) The tokens are drawn from an unknown discrete distribution with a normalized inverse Gaussian process (NIGP) prior.\n\n**Explanation:** The correct answer is B) because the paper states that \"we assume that tokens in the stream are drawn from an unknown discrete distribution, which is endowed with a normalized inverse Gaussian process (NIGP) prior.\" This assumption is crucial in developing the Bayesian nonparametric approach to count-min sketch under power-law data streams. The other options are incorrect because they do not accurately reflect the assumption made in the paper. Option A is incorrect because the distribution is unknown, not known. Option C is incorrect because the distribution is discrete, not continuous. Option D is incorrect because the distribution is unknown, not power-law."}, "37": {"documentation": {"title": "Radio Frequency Phototube, Optical Clock and Precise Measurements in\n  Nuclear Physics", "source": "Amur Margaryan", "docs_id": "0910.3011", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Radio Frequency Phototube, Optical Clock and Precise Measurements in\n  Nuclear Physics. Recently a new experimental program of novel systematic studies of light hypernuclei using pionic decay was established at JLab (Study of Light Hypernuclei by Pionic Decay at JLab, JLab Experiment PR-08-012). The highlights of the proposed program include high precision measurements of binding energies of hypernuclei by using a high resolution pion spectrometer, HpiS. The average values of binding energies will be determined within an accuracy of ~10 keV or better. Therefore, the crucial point of this program is an absolute calibration of the HpiS with accuracy 10E-4 or better. The merging of continuous wave laser-based precision optical-frequency metrology with mode-locked ultrafast lasers has led to precision control of the visible frequency spectrum produced by mode-locked lasers. Such a phase-controlled mode-locked laser forms the foundation of an optical clock or femtosecond optical frequency comb (OFC) generator, with a regular comb of sharp lines with well defined frequencies. Combination of this technique with a recently developed radio frequency (RF) phototube results in a new tool for precision time measurement. We are proposing a new time-of-flight (TOF) system based on an RF phototube and OFC technique. The proposed TOF system achieves 10 fs instability level and opens new possibilities for precise measurements in nuclear physics such as an absolute calibration of magnetic spectrometers within accuracy 10E-4 - 10E-5."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the proposed time-of-flight (TOF) system based on an RF phototube and optical frequency comb (OFC) technique, and how does it relate to the precision measurements in nuclear physics?\n\nA) To achieve a time resolution of 10 fs and calibrate magnetic spectrometers with an accuracy of 10^-4 to 10^-5, thereby enabling precise measurements of binding energies of hypernuclei.\n\nB) To develop a new tool for precision time measurement with a time resolution of 10 fs, but without any specific relation to magnetic spectrometers or hypernuclei.\n\nC) To improve the accuracy of pion spectrometry by 10 keV or better, but without any relation to optical frequency metrology or RF phototubes.\n\nD) To calibrate the HpiS spectrometer with an accuracy of 10^-4 or better, but without any relation to the TOF system or precision time measurement.\n\nCorrect Answer: A) To achieve a time resolution of 10 fs and calibrate magnetic spectrometers with an accuracy of 10^-4 to 10^-5, thereby enabling precise measurements of binding energies of hypernuclei.\n\nExplanation: The correct answer is A) because the proposed TOF system is designed to achieve a time resolution of 10 fs, which is crucial for precise measurements in nuclear physics. Additionally, the system is intended to calibrate magnetic spectrometers with an accuracy of 10^-4 to 10^-5, which is essential for determining the binding energies of hypernuclei with an accuracy of ~10 keV or better. This is the primary goal of the proposed TOF system, as stated in the documentation."}, "38": {"documentation": {"title": "DiagSet: a dataset for prostate cancer histopathological image\n  classification", "source": "Micha{\\l} Koziarski, Bogus{\\l}aw Cyganek, Bogus{\\l}aw Olborski,\n  Zbigniew Antosz, Marcin \\.Zydak, Bogdan Kwolek, Pawe{\\l} W\\k{a}sowicz,\n  Andrzej Buka{\\l}a, Jakub Swad\\'zba, Piotr Sitkowski", "docs_id": "2105.04014", "section": ["eess.IV", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "DiagSet: a dataset for prostate cancer histopathological image\n  classification. Cancer diseases constitute one of the most significant societal challenges. In this paper we introduce a novel histopathological dataset for prostate cancer detection. The proposed dataset, consisting of over 2.6 million tissue patches extracted from 430 fully annotated scans, 4675 scans with assigned binary diagnosis, and 46 scans with diagnosis given independently by a group of histopathologists, can be found at https://ai-econsilio.diag.pl. Furthermore, we propose a machine learning framework for detection of cancerous tissue regions and prediction of scan-level diagnosis, utilizing thresholding and statistical analysis to abstain from the decision in uncertain cases. During the experimental evaluation we identify several factors negatively affecting the performance of considered models, such as presence of label noise, data imbalance, and quantity of data, that can serve as a basis for further research. The proposed approach, composed of ensembles of deep neural networks operating on the histopathological scans at different scales, achieves 94.6% accuracy in patch-level recognition, and is compared in a scan-level diagnosis with 9 human histopathologists."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the primary challenges that the authors of the DiagSet dataset identified in their experimental evaluation, and how do these challenges impact the performance of machine learning models for prostate cancer detection?\n\n**A)** Label noise, data imbalance, and insufficient data quantity are the primary challenges, which can lead to biased models that do not accurately represent the true distribution of cancerous tissue regions.\n\n**B)** The authors found that the presence of noisy labels, data imbalance, and large dataset quantity are the primary challenges, which can result in overfitting and poor generalization of models.\n\n**C)** Label noise, data imbalance, and insufficient data quantity are the primary challenges, which can lead to poor model performance and inaccurate diagnosis of cancerous tissue regions.\n\n**D)** The authors identified that the presence of noisy labels, data imbalance, and large dataset quantity are the primary challenges, which can result in biased models that do not accurately represent the true distribution of cancerous tissue regions.\n\n**Correct Answer:** C) Label noise, data imbalance, and insufficient data quantity are the primary challenges, which can lead to poor model performance and inaccurate diagnosis of cancerous tissue regions.\n\n**Explanation:** The correct answer is C) Label noise, data imbalance, and insufficient data quantity are the primary challenges, which can lead to poor model performance and inaccurate diagnosis of cancerous tissue regions. This is because the authors explicitly stated that these factors negatively affected the performance of considered models, and that they can serve as a basis for further research. The other options are incorrect because they either misrepresent the challenges identified by the authors or introduce new challenges not mentioned in the documentation."}, "39": {"documentation": {"title": "Chaos and subdiffusion in the infinite-range coupled quantum kicked\n  rotors", "source": "Angelo Russomanno, Michele Fava, and Rosario Fazio", "docs_id": "2102.07872", "section": ["quant-ph", "cond-mat.quant-gas", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chaos and subdiffusion in the infinite-range coupled quantum kicked\n  rotors. We map the infinite-range coupled quantum kicked rotors over an infinite-range coupled interacting bosonic model. In this way we can apply exact diagonalization up to quite large system sizes and confirm that the system tends to ergodicity in the large-size limit. In the thermodynamic limit the system is described by a set of coupled Gross-Pitaevskij equations equivalent to an effective nonlinear single-rotor Hamiltonian. These equations give rise to a power-law increase in time of the energy with exponent $\\gamma\\sim 2/3$ in a wide range of parameters. We explain this finding by means of a master-equation approach based on the noisy behaviour of the effective nonlinear single-rotor Hamiltonian and on the Anderson localization of the single-rotor Floquet states. Furthermore, we study chaos by means of the largest Lyapunov exponent and find that it decreases towards zero for portions of the phase space with increasing momentum. Finally, we show that some stroboscopic Floquet integrals of motion of the noninteracting dynamics deviate from their initial values over a time scale related to the interaction strength according to the Nekhoroshev theorem."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the infinite-range coupled quantum kicked rotors, the system exhibits a power-law increase in time of the energy with exponent $\\gamma\\sim 2/3$ in a wide range of parameters. Explain this finding using the master-equation approach and the Anderson localization of the single-rotor Floquet states.\n\n**A)** The power-law increase in energy is due to the noisy behavior of the effective nonlinear single-rotor Hamiltonian, which leads to a random walk in the phase space, causing the energy to increase exponentially with time.\n\n**B)** The power-law increase in energy is a result of the Anderson localization of the single-rotor Floquet states, which leads to a finite-size scaling of the energy, causing it to increase as a power law with time.\n\n**C)** The power-law increase in energy is due to the interplay between the noisy behavior of the effective nonlinear single-rotor Hamiltonian and the Anderson localization of the single-rotor Floquet states, which leads to a non-trivial scaling of the energy with time.\n\n**D)** The power-law increase in energy is a result of the Nekhoroshev theorem, which states that the stroboscopic Floquet integrals of motion of the noninteracting dynamics deviate from their initial values over a time scale related to the interaction strength.\n\n**Correct Answer:** C) The power-law increase in energy is due to the interplay between the noisy behavior of the effective nonlinear single-rotor Hamiltonian and the Anderson localization of the single-rotor Floquet states, which leads to a non-trivial scaling of the energy with time.\n\n**Explanation:** The correct answer, C, is supported by the master-equation approach, which describes the noisy behavior of the effective nonlinear single-rotor Hamiltonian, and the Anderson localization of the single-rotor Floquet states, which explains the finite-size scaling of the energy. The interplay between these two effects leads to a non-trivial scaling of the energy with time, resulting in a power-law increase with exponent $\\gamma\\sim 2/3$. The other options are incorrect because they oversimplify or misrepresent the underlying physics. Option A is incorrect because the noisy behavior of the Hamiltonian does not lead to a random walk in the phase space. Option B is incorrect because the Anderson localization does not lead to a finite-size scaling of the energy. Option D is incorrect because the Nekhoroshev theorem is related to the deviation of the stroboscopic Floquet integrals of motion from their initial values, but it does not explain the power-law increase in energy."}, "40": {"documentation": {"title": "Oxide two-dimensional electron gas with high mobility at\n  room-temperature", "source": "Kitae Eom, Hanjong Paik, Jinsol Seo, Neil Campbell, Evgeny Y. Tsymbal,\n  Sang Ho Oh, Mark Rzchowski, Darrell G. Schlom, and Chang-beom Eom", "docs_id": "2110.02305", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Oxide two-dimensional electron gas with high mobility at\n  room-temperature. The prospect of 2-dimensional electron gases (2DEGs) possessing high mobility at room temperature in wide-bandgap perovskite stannates is enticing for oxide electronics, particularly to realize transparent and high-electron mobility transistors. Nonetheless only a small number of studies to date report 2DEGs in BaSnO3-based heterostructures. Here, we report 2DEG formation at the LaScO3/BaSnO3 (LSO/BSO) interface with a room-temperature mobility of 60 cm2/V s at a carrier concentration of 1.7x1013 cm-2. This is an order of magnitude higher mobility at room temperature than achieved in SrTiO3-based 2DEGs. We achieved this by combining a thick BSO buffer layer with an ex-situ high-temperature treatment, which not only reduces the dislocation density but also produces a SnO2-terminated atomically flat surface, followed by the growth of an overlying BSO/LSO interface. Using weak-beam dark field imaging and in-line electron holography technique, we reveal a reduction of the threading dislocation density, and provide direct evidence for the spatial confinement of a 2DEG at the BSO/LSO interface. Our work opens a new pathway to explore the exciting physics of stannate-based 2DEGs at application-relevant temperatures for oxide nanoelectronics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using a thick BSO buffer layer in conjunction with an ex-situ high-temperature treatment to achieve high mobility in 2DEGs at the LaScO3/BaSnO3 (LSO/BSO) interface?\n\nA) Reduced dislocation density and production of a SnO2-terminated atomically flat surface\nB) Increased carrier concentration and reduced mobility\nC) Improved thermal conductivity and reduced power consumption\nD) Enhanced optical transparency and reduced absorption\n\nCorrect Answer: A) Reduced dislocation density and production of a SnO2-terminated atomically flat surface\n\nExplanation: The correct answer is A) Reduced dislocation density and production of a SnO2-terminated atomically flat surface. The text states that the ex-situ high-temperature treatment \"not only reduces the dislocation density but also produces a SnO2-terminated atomically flat surface\". This is the primary advantage of using this treatment, as it leads to a reduction in dislocation density and the creation of a flat surface, which is essential for achieving high mobility in 2DEGs. The other options are incorrect because they do not accurately reflect the advantages of using this treatment."}, "41": {"documentation": {"title": "Federated Contrastive Learning for Decentralized Unlabeled Medical\n  Images", "source": "Nanqing Dong and Irina Voiculescu", "docs_id": "2109.07504", "section": ["cs.LG", "cs.CV", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Federated Contrastive Learning for Decentralized Unlabeled Medical\n  Images. A label-efficient paradigm in computer vision is based on self-supervised contrastive pre-training on unlabeled data followed by fine-tuning with a small number of labels. Making practical use of a federated computing environment in the clinical domain and learning on medical images poses specific challenges. In this work, we propose FedMoCo, a robust federated contrastive learning (FCL) framework, which makes efficient use of decentralized unlabeled medical data. FedMoCo has two novel modules: metadata transfer, an inter-node statistical data augmentation module, and self-adaptive aggregation, an aggregation module based on representational similarity analysis. To the best of our knowledge, this is the first FCL work on medical images. Our experiments show that FedMoCo can consistently outperform FedAvg, a seminal federated learning framework, in extracting meaningful representations for downstream tasks. We further show that FedMoCo can substantially reduce the amount of labeled data required in a downstream task, such as COVID-19 detection, to achieve a reasonable performance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary contribution of the proposed FedMoCo framework in the context of federated learning for decentralized unlabeled medical images?\n\nA) FedMoCo introduces a novel aggregation method based on gradient averaging.\nB) FedMoCo proposes a metadata transfer module to enhance data augmentation and reduce label noise.\nC) FedMoCo presents a self-adaptive aggregation module based on representational similarity analysis to improve model robustness.\nD) FedMoCo introduces a federated learning framework that relies solely on self-supervised contrastive pre-training.\n\nCorrect Answer: C) FedMoCo presents a self-adaptive aggregation module based on representational similarity analysis to improve model robustness.\n\nExplanation: The correct answer is C) because the documentation states that FedMoCo has a self-adaptive aggregation module based on representational similarity analysis, which is a key innovation of the proposed framework. This module is designed to improve model robustness and is a critical component of FedMoCo's federated learning approach. The other options are incorrect because they either describe features of FedMoCo that are not its primary contribution (A and B) or are not accurate descriptions of the framework (D)."}, "42": {"documentation": {"title": "Neutral tritium gas reduction in the KATRIN differential pumping\n  sections", "source": "Alexander Marsteller, Beate Bornschein, Lutz Bornschein, Guido\n  Drexlin, Fabian Friedel, Rainer Gehring, Steffen Grohmann, Rainer\n  Gumbsheimer, Moritz Hackenjos, Alexander Jansen, Andreas Kosmider, Luisa\n  LaCascio, Steffen Lichter, Klaus M\\\"uller, Florian Priester, Rolf\n  Rinderspacher, Marco R\\\"ollig, Carsten R\\\"ottele, Felix Sharipov, Michael\n  Sturm, Stefan Welte, Joachim Wolf", "docs_id": "2009.10403", "section": ["physics.ins-det", "astro-ph.IM", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neutral tritium gas reduction in the KATRIN differential pumping\n  sections. The KArlsruhe TRItium Neutrino experiment (KATRIN) aims to measure the effective electron anti-neutrino mass with an unprecedented sensitivity of $0.2\\,\\mathrm{eV}/\\mathrm{c}^2$, using $\\beta$-electrons from tritium decay. The electrons are guided magnetically by a system of superconducting magnets through a vacuum beamline from the windowless gaseous tritium source through differential and cryogenic pumping sections to a high resolution spectrometer and a segmented silicon pin detector. At the same time tritium gas has to be prevented from entering the spectrometer. Therefore, the pumping sections have to reduce the tritium flow by more than 14 orders of magnitude. This paper describes the measurement of the reduction factor of the differential pumping section performed with high purity tritium gas during the first measurement campaigns of the KATRIN experiment. The reduction factor results are compared with previously performed simulations, as well as the stringent requirements of the KATRIN experiment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the KArlsruhe TRItium Neutrino experiment (KATRIN) in terms of the effective electron anti-neutrino mass, and what is the target sensitivity of the experiment?\n\nA) To measure the effective electron anti-neutrino mass with a sensitivity of 10^-14 eV/c^2, and to detect neutrino oscillations.\nB) To measure the effective electron anti-neutrino mass with an unprecedented sensitivity of 0.2 eV/c^2, and to detect neutrino oscillations.\nC) To measure the effective electron anti-neutrino mass with a sensitivity of 10^-12 eV/c^2, and to detect neutrino oscillations.\nD) To measure the effective electron anti-neutrino mass with a sensitivity of 10^-10 eV/c^2, and to detect neutrino oscillations.\n\nCorrect Answer: B) To measure the effective electron anti-neutrino mass with an unprecedented sensitivity of 0.2 eV/c^2, and to detect neutrino oscillations.\n\nExplanation: The question requires the student to understand the primary goal of the KATRIN experiment, which is to measure the effective electron anti-neutrino mass with an unprecedented sensitivity of 0.2 eV/c^2. The correct answer is B, as it accurately reflects the stated goal of the experiment. The other options are incorrect, as they either have a different sensitivity or mention detecting neutrino oscillations, which is not the primary goal of the experiment. \n\nNote: The student should be able to identify the correct answer by analyzing the documentation and understanding the context of the KATRIN experiment. The question requires critical thinking and the ability to extract relevant information from the documentation."}, "43": {"documentation": {"title": "Asynchronous Networks and Event Driven Dynamics", "source": "Christian Bick and Michael Field", "docs_id": "1509.04045", "section": ["math.DS", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asynchronous Networks and Event Driven Dynamics. Real-world networks in technology, engineering and biology often exhibit dynamics that cannot be adequately reproduced using network models given by smooth dynamical systems and a fixed network topology. Asynchronous networks give a theoretical and conceptual framework for the study of network dynamics where nodes can evolve independently of one another, be constrained, stop, and later restart, and where the interaction between different components of the network may depend on time, state, and stochastic effects. This framework is sufficiently general to encompass a wide range of applications ranging from engineering to neuroscience. Typically, dynamics is piecewise smooth and there are relationships with Filippov systems. In the first part of the paper, we give examples of asynchronous networks, and describe the basic formalism and structure. In the second part, we make the notion of a functional asynchronous network rigorous, discuss the phenomenon of dynamical locks, and present a foundational result on the spatiotemporal factorization of the dynamics for a large class of functional asynchronous networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary characteristic of asynchronous networks that distinguishes them from traditional network models given by smooth dynamical systems and fixed network topology?\n\nA) The nodes in the network evolve independently of one another, but are constrained by a fixed set of rules.\nB) The interaction between different components of the network is deterministic and time-invariant.\nC) The dynamics of the network is piecewise smooth, allowing for abrupt changes in behavior.\nD) The nodes in the network can evolve independently of one another, be constrained, stop, and later restart, and where the interaction between different components of the network may depend on time, state, and stochastic effects.\n\n**Correct Answer:** D) The nodes in the network can evolve independently of one another, be constrained, stop, and later restart, and where the interaction between different components of the network may depend on time, state, and stochastic effects.\n\n**Explanation:** This question requires the test-taker to understand the key features of asynchronous networks, as described in the documentation. The correct answer highlights the flexibility and adaptability of asynchronous networks, which is a fundamental aspect of their design. The incorrect options (A, B, and C) either misrepresent or oversimplify the characteristics of asynchronous networks, demonstrating a lack of understanding of the subject matter."}, "44": {"documentation": {"title": "Background Dependent Lorentz Violation from String Theory", "source": "Tianjun Li, Dimitri V. Nanopoulos", "docs_id": "1110.0451", "section": ["hep-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Background Dependent Lorentz Violation from String Theory. We revisit Lorentz violation in the Type IIB string theory with D3-branes and D7-branes. We study the relativistic particle velocities in details, and show that there exist both subluminal and superluminal particle propagations. In particular, the additional contributions to the particle velosity \\delta v\\equiv (v-c)/c from string theory is proportional to both the particle energy and the D3-brane number density, and is inversely proportional to the string scale. Thus, we can realize the background dependent Lorentz violation naturally by varying the D3-brane number density in space time. To explain the superluminal neutrino propagations in the OPERA and MINOS experiments, the string scale should be around 10^5 GeV. With very tiny D3-brane number density on the interstellar scale, we can also explain the time delays for the high energy photons compared to the low energy photons in the MAGIC, HESS, and FERMI experiments simultaneously. Interestingly, we can automatically satisfy all the stringent constraints from the synchrotron radiation of the Crab Nebula, the SN1987a observations on neutrinos, and the cosmic ray experiments on charged leptons. We also address the possible phenomenological challenges to our models from the relevant experiments done on the Earth."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Type IIB string theory with D3-branes and D7-branes, what is the relationship between the additional contributions to particle velocity, \u03b4v, and the string scale, and how does this relate to the observed phenomena in high-energy particle experiments?\n\n**A)** \u03b4v is proportional to the particle energy and inversely proportional to the string scale, and this effect is negligible at low energies.\n\n**B)** \u03b4v is proportional to the D3-brane number density and inversely proportional to the string scale, and this effect is only significant at very high energies.\n\n**C)** \u03b4v is proportional to the string scale and inversely proportional to the D3-brane number density, and this effect is only significant at low energies.\n\n**D)** \u03b4v is proportional to both the particle energy and the D3-brane number density, and this effect is inversely proportional to the string scale, and is significant at both high and low energies.\n\n**Correct Answer:** D) \u03b4v is proportional to both the particle energy and the D3-brane number density, and this effect is inversely proportional to the string scale, and is significant at both high and low energies.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"the additional contributions to the particle velocity \u03b4v\u2261(v-c)/c from string theory is proportional to both the particle energy and the D3-brane number density, and is inversely proportional to the string scale.\" This relationship is crucial for understanding how string theory can explain the observed phenomena in high-energy particle experiments, such as superluminal neutrino propagations and time delays for high-energy photons."}, "45": {"documentation": {"title": "$\\Lambda p$ Elastic Scattering in the CLAS Detector", "source": "John W. Price (for the CLAS Collaboration)", "docs_id": "1902.08862", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "$\\Lambda p$ Elastic Scattering in the CLAS Detector. The elastic scattering process $\\Lambda p\\to\\Lambda p$ offers insights on multiple problems in nuclear physics. $\\mathrm{SU}(3)_F$ symmetry implies a close agreement between the $\\Lambda p$ and $pp$ scattering cross sections. The $\\Lambda p$ elastic scattering cross section can also illuminate the structure of neutron stars. A data-mining project was started using multiple CLAS data sets taken for other purposes with photon beams on a long liquid hydrogen target. A $\\Lambda$ produced in a process such as $\\gamma p\\to K^+\\Lambda$ can interact with a second proton inside the target before either decaying or leaving the target. The good angular acceptance and momentum resolution of CLAS make it well-suited for this type of analysis, even though it was not designed for such a measurement. The scattered $\\Lambda$ can be identified from the $\\pi^-p$ invariant mass. The four-vector of the initial $\\Lambda$ is then reconstructed in the process $Xp\\to\\Lambda p$, which shows a strong peak at the $\\Lambda$ mass with roughly twice the number of events as the existing world data sample. This observation opens up the possibility of other measurements using secondary beams of short-lived particles. This paper will discuss the current status of the analysis, and our plans for future work on this project."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary motivation behind the data-mining project using CLAS data sets for $\\Lambda p$ elastic scattering, and what are the advantages of CLAS in this analysis?\n\nA) The project aims to study the $\\Lambda p$ scattering cross section to test the $\\mathrm{SU}(3)_F$ symmetry, and CLAS's good angular acceptance and momentum resolution make it suitable for this analysis.\nB) The project seeks to investigate the structure of neutron stars using the $\\Lambda p$ elastic scattering cross section, and CLAS's design for photon beam experiments makes it well-suited for this purpose.\nC) The project aims to study the $\\gamma p\\to K^+\\Lambda$ process to understand the production of $\\Lambda$ particles, and CLAS's momentum resolution is sufficient for this analysis.\nD) The project seeks to measure the $\\pi^-p$ invariant mass to study the properties of the $\\Lambda$ particle, and CLAS's angular acceptance is sufficient for this analysis.\n\nCorrect Answer: A) The project aims to study the $\\Lambda p$ scattering cross section to test the $\\mathrm{SU}(3)_F$ symmetry, and CLAS's good angular acceptance and momentum resolution make it suitable for this analysis.\n\nExplanation: The correct answer is A) because the primary motivation behind the data-mining project is to study the $\\Lambda p$ scattering cross section to test the $\\mathrm{SU}(3)_F$ symmetry, which is a key aspect of nuclear physics. The advantages of CLAS mentioned in the documentation are its good angular acceptance and momentum resolution, which make it well-suited for this analysis. The other options are incorrect because they do not accurately reflect the primary motivation or advantages of CLAS in this analysis."}, "46": {"documentation": {"title": "Estimating Doubly-Selective Channels for Hybrid mmWave Massive MIMO\n  Systems: A Doubly-Sparse Approach", "source": "Shijian Gao and Xiang Cheng and Liuqing Yang", "docs_id": "1912.08380", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimating Doubly-Selective Channels for Hybrid mmWave Massive MIMO\n  Systems: A Doubly-Sparse Approach. In mmWave massive multiple-input multiple-output (mMIMO) systems, hybrid digital/analog beamforming has been recognized as an economic means to overcome the severe mmWave propagation loss. To facilitate beamforming for mmWace mMIMO, there is a great urgency to acquire accurate channel state information. To this end, a novel doubly-sparse approach is proposed to estimate doubly-selective mmWave channels under hybrid mMIMO. Via the judiciously designed training pattern, the well-known beamspace sparsity along with the under-investigated delay-domain sparsity that mmWave channels exhibit can be jointly exploited to assist channel estimation. Thanks to our careful two-stage (random-probing and steering-probing) design, the proposed channel estimator possesses strong robustness against the double (frequency and time) selectivity whilst enjoying the benefits brought by the exploitation of double sparsity. Compared with existing alternatives, our proposed channel estimator not only proves to be more general, but also largely reduces the training overhead, storage demand as well as computational complexity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed doubly-sparse approach in estimating doubly-selective mmWave channels under hybrid mMIMO systems?\n\nA) It reduces the training overhead, storage demand, and computational complexity compared to existing alternatives.\nB) It exploits the delay-domain sparsity of mmWave channels to improve channel estimation accuracy.\nC) It utilizes the beamspace sparsity of mmWave channels to facilitate beamforming for mMIMO systems.\nD) It employs a two-stage design (random-probing and steering-probing) to enhance robustness against double selectivity.\n\n**Correct Answer:** A) It reduces the training overhead, storage demand, and computational complexity compared to existing alternatives.\n\n**Explanation:** The question requires the test-taker to understand the key benefits of the proposed doubly-sparse approach. The correct answer highlights the advantages of the approach in reducing training overhead, storage demand, and computational complexity, which are all mentioned in the documentation as benefits of the proposed channel estimator. The incorrect options (B, C, and D) are related to the approach, but they do not accurately capture the primary advantage of the proposed method."}, "47": {"documentation": {"title": "State Drug Policy Effectiveness: Comparative Policy Analysis of Drug\n  Overdose Mortality", "source": "Jarrod Olson and Po-Hsu Allen Chen and Marissa White and Nicole\n  Brennan and Ning Gong", "docs_id": "1909.01936", "section": ["stat.AP", "cs.LG", "econ.EM", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "State Drug Policy Effectiveness: Comparative Policy Analysis of Drug\n  Overdose Mortality. Opioid overdose rates have reached an epidemic level and state-level policy innovations have followed suit in an effort to prevent overdose deaths. State-level drug law is a set of policies that may reinforce or undermine each other, and analysts have a limited set of tools for handling the policy collinearity using statistical methods. This paper uses a machine learning method called hierarchical clustering to empirically generate \"policy bundles\" by grouping states with similar sets of policies in force at a given time together for analysis in a 50-state, 10-year interrupted time series regression with drug overdose deaths as the dependent variable. Policy clusters were generated from 138 binomial variables observed by state and year from the Prescription Drug Abuse Policy System. Clustering reduced the policies to a set of 10 bundles. The approach allows for ranking of the relative effect of different bundles and is a tool to recommend those most likely to succeed. This study shows that a set of policies balancing Medication Assisted Treatment, Naloxone Access, Good Samaritan Laws, Medication Assisted Treatment, Prescription Drug Monitoring Programs and legalization of medical marijuana leads to a reduced number of overdose deaths, but not until its second year in force."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the findings of the study on state drug policy effectiveness using hierarchical clustering. What is the key insight from the study regarding the policy bundles that lead to a reduction in overdose deaths, and what is the implication of this finding for policy makers?\n\n**A)** The study found that a single policy bundle, such as Medication Assisted Treatment, is sufficient to reduce overdose deaths, and therefore policy makers should focus on implementing this policy alone. (Candidate A)\n\n**B)** The study identified 10 policy bundles that are effective in reducing overdose deaths, and policy makers should consider implementing all of these bundles simultaneously to achieve the best results. (Candidate B)\n\n**C)** The study found that a policy bundle that balances Medication Assisted Treatment, Naloxone Access, Good Samaritan Laws, Medication Assisted Treatment, Prescription Drug Monitoring Programs, and legalization of medical marijuana is effective in reducing overdose deaths, but only after its second year in force. This suggests that policy makers should adopt a phased approach to policy implementation, starting with the most effective policies and gradually adding more. (Candidate C)\n\n**D)** The study found that the effectiveness of policy bundles in reducing overdose deaths is highly correlated with the number of policies implemented, and therefore policy makers should aim to implement the maximum number of policies possible to achieve the best results. (Candidate D)\n\n**Correct Answer:** C) The study found that a policy bundle that balances Medication Assisted Treatment, Naloxone Access, Good Samaritan Laws, Medication Assisted Treatment, Prescription Drug Monitoring Programs, and legalization of medical marijuana is effective in reducing overdose deaths, but only after its second year in force. This suggests that policy makers should adopt a phased approach to policy implementation, starting with the most effective policies and gradually adding more."}, "48": {"documentation": {"title": "Direct observation of the effects of spin dependent momentum of light in\n  optical tweezers", "source": "Debapriya Pal, Subhasish Dutta Gupta, Nirmalya Ghosh, and Ayan\n  Banerjee", "docs_id": "2004.04064", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Direct observation of the effects of spin dependent momentum of light in\n  optical tweezers. We demonstrate that tight focusing of a circularly polarized Gaussian beam in optical tweezers leads to spin-momentum locking - with the transverse spin angular momentum density being independent of helicity, while the transverse momentum (Poynting vector) becomes helicity dependent. Our theoretical calculations, numerical simulations, and experiments reveal that the presence of a stratified medium in the path of the trapping beam significantly enhances the magnitude of transverse momentum in the radial direction with respect to the beam axis, and likewise, also leads to high off-axial intensity. This overlap allows us to experimentally observe the circular motion of a birefringent particle, trapped off-axis, in response to an input circularly polarized fundamental Gaussian beam carrying no intrinsic orbital angular momentum. The circular motion is dependent on the helicity of the input beam, so that we can identify it to be the signature of the elusive Belinfante spin in propagating light beams obtained in our optical tweezers setup. Our work can be extended to higher-order beams carrying intrinsic orbital angular momentum leading to simple routes of achieving complex particle manipulation using optical tweezers."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the study on spin-dependent momentum of light in optical tweezers, what is the primary effect observed in the transverse spin angular momentum density, and how does it relate to the helicity of the input beam?\n\n**A)** The transverse spin angular momentum density is independent of the helicity of the input beam, and its magnitude is enhanced by the presence of a stratified medium in the path of the trapping beam.\n\n**B)** The transverse spin angular momentum density is dependent on the helicity of the input beam, and its magnitude is reduced by the presence of a stratified medium in the path of the trapping beam.\n\n**C)** The transverse spin angular momentum density is independent of the helicity of the input beam, and its magnitude is unaffected by the presence of a stratified medium in the path of the trapping beam.\n\n**D)** The transverse spin angular momentum density is dependent on the helicity of the input beam, and its magnitude is increased by the presence of a stratified medium in the path of the trapping beam.\n\n**Correct Answer:** A) The transverse spin angular momentum density is independent of the helicity of the input beam, and its magnitude is enhanced by the presence of a stratified medium in the path of the trapping beam.\n\n**Explanation:** According to the study, the transverse spin angular momentum density is independent of the helicity of the input beam, meaning that the spin-momentum locking effect is not dependent on the handedness of the circularly polarized Gaussian beam. However, the presence of a stratified medium in the path of the trapping beam significantly enhances the magnitude of the transverse momentum in the radial direction, leading to high off-axial intensity and the ability to observe the circular motion of a birefringent particle."}, "49": {"documentation": {"title": "Boundary Guidance Hierarchical Network for Real-Time Tongue Segmentation", "source": "Xinyi Zeng, Qian Zhang, Jia Chen, Guixu Zhang, Aimin Zhou and Yiqin\n  Wang", "docs_id": "2003.06529", "section": ["eess.IV", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Boundary Guidance Hierarchical Network for Real-Time Tongue Segmentation. Automated tongue image segmentation in tongue images is a challenging task for two reasons: 1) there are many pathological details on the tongue surface, which affect the extraction of the boundary; 2) the shapes of the tongues captured from various persons (with different diseases) are quite different. To deal with the challenge, a novel end-to-end Boundary Guidance Hierarchical Network (BGHNet) with a new hybrid loss is proposed in this paper. In the new approach, firstly Context Feature Encoder Module (CFEM) is built upon the bottomup pathway to confront with the shrinkage of the receptive field. Secondly, a novel hierarchical recurrent feature fusion module (HRFFM) is adopt to progressively and hierarchically refine object maps to recover image details by integrating local context information. Finally, the proposed hybrid loss in a four hierarchy-pixel, patch, map and boundary guides the network to effectively segment the tongue regions and accurate tongue boundaries. BGHNet is applied to a set of tongue images. The experimental results suggest that the proposed approach can achieve the latest tongue segmentation performance. And in the meantime, the lightweight network contains only 15.45M parameters and performs only 11.22GFLOPS."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary challenge in automated tongue image segmentation, and how does the proposed Boundary Guidance Hierarchical Network (BGHNet) address this challenge?\n\nA) The shapes of tongues captured from various persons are similar, making segmentation easier. BGHNet addresses this by using a hierarchical recurrent feature fusion module to refine object maps.\nB) The shapes of tongues captured from various persons are quite different, making segmentation challenging. BGHNet addresses this by using a Context Feature Encoder Module to confront with the shrinkage of the receptive field.\nC) The lack of pathological details on the tongue surface makes segmentation easier. BGHNet addresses this by using a novel hybrid loss that guides the network to effectively segment tongue regions and accurate tongue boundaries.\nD) The shapes of tongues captured from various persons are similar, but the tongue surface is too small to segment. BGHNet addresses this by using a lightweight network with only 15.45M parameters.\n\nCorrect Answer: B) The shapes of tongues captured from various persons are quite different, making segmentation challenging. BGHNet addresses this by using a Context Feature Encoder Module to confront with the shrinkage of the receptive field.\n\nExplanation: The question requires the test-taker to understand the primary challenge in automated tongue image segmentation, which is the diversity of tongue shapes captured from various persons. The correct answer, B, highlights this challenge and explains how BGHNet addresses it by using a Context Feature Encoder Module to mitigate the shrinkage of the receptive field. The other options are incorrect because they either misstate the challenge or incorrectly describe how BGHNet addresses it."}, "50": {"documentation": {"title": "Phase transitions on a class of generalized Vicsek-like models of\n  collective motion", "source": "Pau Clusella and Romualdo Pastor-Satorras", "docs_id": "2102.04715", "section": ["nlin.AO", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phase transitions on a class of generalized Vicsek-like models of\n  collective motion. Systems composed of interacting self-propelled particles (SPPs) display different forms of order-disorder phase transitions relevant to collective motion. In this paper we propose a generalization of the Vicsek model characterized by an angular noise term following an arbitrary probability density function, which might depend on the state of the system and thus have a multiplicative character. We show that the well established vectorial Vicsek model can be expressed in this general formalism by deriving the corresponding angular probability density function, as well as we propose two new multiplicative models consisting on a bivariate Gaussian and a wrapped Gaussian distributions. With the proposed formalism, the mean-field system can be solved using the mean resultant length of the angular stochastic term. Accordingly, when the SPPs interact globally, the character of the phase transition depends on the choice of the noise distribution, being first-order with an hybrid scaling for the vectorial and wrapped Gaussian distributions, and second order for the bivariate Gaussian distribution. Numerical simulations reveal that this scenario also holds when the interactions among SPPs are given by a static complex network. On the other hand, using spatial short-range interactions displays, in all the considered instances, a discontinuous transition with a coexistence region, consistent with the original formulation of the Vicsek model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary difference in the phase transition behavior of the generalized Vicsek-like models of collective motion when the interactions among self-propelled particles (SPPs) are given by a static complex network versus spatial short-range interactions?\n\n**A)** The phase transition is always first-order for static complex networks and second-order for spatial short-range interactions.\n**B)** The phase transition is always second-order for static complex networks and first-order for spatial short-range interactions.\n**C)** The phase transition is first-order for both static complex networks and spatial short-range interactions, with a coexistence region in the latter case.\n**D)** The phase transition is second-order for both static complex networks and spatial short-range interactions, with a discontinuous transition in the latter case.\n\n**Correct Answer:** C) The phase transition is first-order for both static complex networks and spatial short-range interactions, with a coexistence region in the latter case.\n\n**Explanation:** According to the documentation, the phase transition behavior of the generalized Vicsek-like models of collective motion is as follows:\n\n* For static complex networks, the phase transition is first-order.\n* For spatial short-range interactions, the phase transition is discontinuous with a coexistence region.\n\nTherefore, option C is the correct answer. Options A and B are incorrect because they state that the phase transition is always first-order or second-order, respectively, which is not the case. Option D is also incorrect because it states that the phase transition is second-order for both static complex networks and spatial short-range interactions, which is not true."}, "51": {"documentation": {"title": "Ab-initio Studies of (Li$_{0.8}$Fe$_{0.2}$)OHFeSe Superconductors:\n  Revealing the Dual Roles of Fe$_{0.2}$ in Structural Stability and Charge\n  Transfer", "source": "Wei Chen and Changgan Zeng and Efthimios Kaxiras and Zhenyu Zhang", "docs_id": "1509.07191", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ab-initio Studies of (Li$_{0.8}$Fe$_{0.2}$)OHFeSe Superconductors:\n  Revealing the Dual Roles of Fe$_{0.2}$ in Structural Stability and Charge\n  Transfer. The recently discovered (Li$_{0.8}$Fe$_{0.2}$)OHFeSe superconductor provides a new platform for exploiting the microscopic mechanisms of high-$T_c$ superconductivity in FeSe-derived systems. Using density functional theory calculations, we first show that substitution of Li by Fe not only significantly strengthens the attraction between the (Li$_{0.8}$Fe$_{0.2}$)OH spacing layers and the FeSe superconducting layers along the \\emph{c} axis, but also minimizes the lattice mismatch between the two in the \\emph{ab} plane, both favorable for stabilizing the overall structure. Next we explore the electron injection into FeSe from the spacing layers, and unambiguously identify the Fe$_{0.2}$ components to be the dominant atomic origin of the dramatically enhanced interlayer charge transfer. We further reveal that the system strongly favors collinear antiferromagnetic ordering in the FeSe layers, but the spacing layers can be either antiferromagnetic or ferromagnetic depending on the Fe$_{0.2}$ spatial distribution. Based on these understandings, we also predict (Li$_{0.8}$Co$_{0.2}$)OHFeSe to be structurally stable with even larger electron injection and potentially higher $T_c$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary role of the Fe$_{0.2}$ component in the (Li$_{0.8}$Fe$_{0.2}$)OHFeSe superconductor, as revealed by the density functional theory calculations?\n\nA) To enhance the lattice mismatch between the spacing layers and the FeSe superconducting layers in the ab plane.\nB) To minimize the attraction between the spacing layers and the FeSe superconducting layers along the c axis.\nC) To dominate the atomic origin of the dramatically enhanced interlayer charge transfer.\nD) To stabilize the overall structure by strengthening the attraction between the spacing layers and the FeSe superconducting layers along the c axis.\n\n**Correct Answer:** C) To dominate the atomic origin of the dramatically enhanced interlayer charge transfer.\n\n**Explanation:** The correct answer is C) To dominate the atomic origin of the dramatically enhanced interlayer charge transfer. The text states that \"we unambiguously identify the Fe$_{0.2}$ components to be the dominant atomic origin of the dramatically enhanced interlayer charge transfer.\" This indicates that the Fe$_{0.2}$ component plays a crucial role in facilitating the interlayer charge transfer, which is a key mechanism for high-Tc superconductivity in FeSe-derived systems."}, "52": {"documentation": {"title": "Social dilemma in traffic with heterogeneous drivers", "source": "Ricardo Sim\\~ao, and Lucas Wardil", "docs_id": "2004.03483", "section": ["physics.soc-ph", "nlin.CG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Social dilemma in traffic with heterogeneous drivers. There is a tragedy of the traffic analogous to the tragedy of the commons that can be caused by overtaking. We analyze the effect of overtaking in a minimal model of vehicular traffic, the model proposed by Nagel and Schreckenberg, with two types of drivers: drivers that overtake and drivers that do not. We show that, under certain circumstances, overtaking is good because it increases the road capacity and minimizes the mean time spent by the driver on the road. However, when these conditions are not met, overtaking is harmful to all. More specifically, we found that a social dilemma emerges in the vicinity of the transition to the congested traffic if the probability of random deceleration is low, which can also happen in more realistic single-lane models. The essential mechanism creating the social dilemma is the abrupt deceleration when the overtaking car returns to its lane. We analyze how the payoffs depend on the frequency of strategies in the population to conclude that the drivers that overtake are defectors and the ones that do not are cooperators, analogous to the strategies in tragedy of the commons class of games."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Nagel and Schreckenberg model of vehicular traffic, what is the essential mechanism creating the social dilemma that arises when overtaking occurs, and how does it relate to the frequency of strategies in the population?\n\n**A)** The mechanism is the increased road capacity that overtaking provides, which leads to a decrease in the frequency of overtaking among drivers who do not overtake.\n\n**B)** The mechanism is the abrupt deceleration when the overtaking car returns to its lane, which creates a negative externality that affects all drivers, leading to a decrease in the frequency of overtaking among drivers who overtake.\n\n**C)** The mechanism is the increased mean time spent by drivers on the road when overtaking occurs, which leads to a decrease in the frequency of overtaking among drivers who do not overtake.\n\n**D)** The mechanism is the increased road capacity that overtaking provides, which leads to an increase in the frequency of overtaking among drivers who overtake, and a decrease in the frequency of overtaking among drivers who do not overtake.\n\n**Correct Answer:** B) The mechanism is the abrupt deceleration when the overtaking car returns to its lane, which creates a negative externality that affects all drivers, leading to a decrease in the frequency of overtaking among drivers who overtake.\n\n**Explanation:** The correct answer is B) because the documentation states that the essential mechanism creating the social dilemma is the abrupt deceleration when the overtaking car returns to its lane, which creates a negative externality that affects all drivers. This leads to a decrease in the frequency of overtaking among drivers who overtake, as they realize that their actions are harming others. The other options are incorrect because they do not accurately describe the mechanism creating the social dilemma. Option A is incorrect because the increased road capacity provided by overtaking does not lead to a decrease in the frequency of overtaking among drivers who do not overtake. Option C is incorrect because the mean time spent by drivers on the road is not directly affected by overtaking. Option D is incorrect because the increased frequency of overtaking among drivers who overtake does not lead to a decrease in the frequency of overtaking among drivers who do not overtake."}, "53": {"documentation": {"title": "Exponential Kernels with Latency in Hawkes Processes: Applications in\n  Finance", "source": "Marcos Costa Santos Carreira", "docs_id": "2101.06348", "section": ["stat.ML", "cs.LG", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exponential Kernels with Latency in Hawkes Processes: Applications in\n  Finance. The Tick library allows researchers in market microstructure to simulate and learn Hawkes process in high-frequency data, with optimized parametric and non-parametric learners. But one challenge is to take into account the correct causality of order book events considering latency: the only way one order book event can influence another is if the time difference between them (by the central order book timestamps) is greater than the minimum amount of time for an event to be (i) published in the order book, (ii) reach the trader responsible for the second event, (iii) influence the decision (processing time at the trader) and (iv) the 2nd event reach the order book and be processed. For this we can use exponential kernels shifted to the right by the latency amount. We derive the expression for the log-likelihood to be minimized for the 1-D and the multidimensional cases, and test this method with simulated data and real data. On real data we find that, although not all decays are the same, the latency itself will determine most of the decays. We also show how the decays are related to the latency. Code is available on GitHub at https://github.com/MarcosCarreira/Hawkes-With-Latency."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Hawkes processes with latency, what is the primary challenge in modeling order book events, and how can exponential kernels be used to address this challenge?\n\n**A)** The primary challenge is to account for the non-stationarity of the process, and exponential kernels can be used to model the time-varying intensity function.\n\n**B)** The primary challenge is to capture the correct causality of order book events, considering latency, and exponential kernels can be used to shift the kernel to the right by the latency amount.\n\n**C)** The primary challenge is to estimate the parameters of the Hawkes process, and exponential kernels can be used to model the decay of the process.\n\n**D)** The primary challenge is to account for the non-Gaussian nature of the data, and exponential kernels can be used to model the conditional distribution of the process.\n\n**Correct Answer:** B) The primary challenge is to capture the correct causality of order book events, considering latency, and exponential kernels can be used to shift the kernel to the right by the latency amount.\n\n**Explanation:** The correct answer is B) because the question specifically mentions the challenge of taking into account the correct causality of order book events considering latency. The use of exponential kernels shifted to the right by the latency amount is a key technique to address this challenge, as it allows for the modeling of the time difference between events. The other options are incorrect because they do not directly address the challenge of capturing causality with latency."}, "54": {"documentation": {"title": "Symmetries of $\\mathcal{N} = (1,0)$ supergravity backgrounds in six\n  dimensions", "source": "Sergei M. Kuzenko, Ulf Lindstr\\\"om, Emmanouil S. N. Raptakis and\n  Gabriele Tartaglino-Mazzucchelli", "docs_id": "2012.08159", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Symmetries of $\\mathcal{N} = (1,0)$ supergravity backgrounds in six\n  dimensions. General $\\mathcal{N}=(1,0)$ supergravity-matter systems in six dimensions may be described using one of the two fully fledged superspace formulations for conformal supergravity: (i) $\\mathsf{SU}(2)$ superspace; and (ii) conformal superspace. With motivation to develop rigid supersymmetric field theories in curved space, this paper is devoted to the study of the geometric symmetries of supergravity backgrounds. In particular, we introduce the notion of a conformal Killing spinor superfield $\\epsilon^\\alpha$, which proves to generate extended superconformal transformations. Among its cousins are the conformal Killing vector $\\xi^a$ and tensor $\\zeta^{a(n)}$ superfields. The former parametrise conformal isometries of supergravity backgrounds, which in turn yield symmetries of every superconformal field theory. Meanwhile, the conformal Killing tensors of a given background are associated with higher symmetries of the hypermultiplet. By studying the higher symmetries of a non-conformal vector multiplet we introduce the concept of a Killing tensor superfield. We also analyse the problem of computing higher symmetries for the conformal d'Alembertian in curved space and demonstrate that, beyond the first-order case, these operators are defined only on conformally flat backgrounds."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the relationship between the conformal Killing spinor superfield $\\epsilon^\\alpha$ and the conformal Killing vector $\\xi^a$ in the context of six-dimensional $\\mathcal{N} = (1,0)$ supergravity backgrounds?\n\nA) The conformal Killing spinor superfield $\\epsilon^\\alpha$ is a linear combination of the conformal Killing vector $\\xi^a$.\n\nB) The conformal Killing spinor superfield $\\epsilon^\\alpha$ is a higher-order derivative of the conformal Killing vector $\\xi^a$.\n\nC) The conformal Killing spinor superfield $\\epsilon^\\alpha$ is a gauge parameter for the conformal Killing vector $\\xi^a$.\n\nD) The conformal Killing spinor superfield $\\epsilon^\\alpha$ is a tensor that transforms under the same conformal isometries as the conformal Killing vector $\\xi^a$.\n\nCorrect Answer: D) The conformal Killing spinor superfield $\\epsilon^\\alpha$ is a tensor that transforms under the same conformal isometries as the conformal Killing vector $\\xi^a$.\n\nExplanation: The conformal Killing spinor superfield $\\epsilon^\\alpha$ is introduced in the paper as a way to generate extended superconformal transformations. It is shown to transform under the same conformal isometries as the conformal Killing vector $\\xi^a$, which parametrises conformal isometries of supergravity backgrounds. This relationship is crucial in understanding the geometric symmetries of supergravity backgrounds and their implications for superconformal field theories."}, "55": {"documentation": {"title": "Catch bond mechanism in Dynein motor driven collective transport", "source": "Anil Nair, Sameep Chandel, Mithun K Mitra, Sudipto Muhuri and Abhishek\n  Chaudhuri", "docs_id": "1602.02423", "section": ["physics.bio-ph", "cond-mat.soft", "q-bio.SC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Catch bond mechanism in Dynein motor driven collective transport. Recent experiments have demonstrated that dynein motor exhibits catch bonding behaviour, in which the unbinding rate of a single dynein decreases with increasing force, for a certain range of force. Motivated by these experiments, we propose a model for catch bonding in dynein using a threshold force bond deformation (TFBD) model wherein catch bonding sets in beyond a critical applied load force. We study the effect of catch bonding on unidirectional transport properties of cellular cargo carried by multiple dynein motors within the framework of this model. We find catch bonding can result in dramatic changes in the transport properties, which are in sharp contrast to kinesin driven unidirectional transport, where catch bonding is absent. We predict that, under certain conditions, the average velocity of the cellular cargo can actually increase as applied load is increased. We characterize the transport properties in terms of a velocity profile phase plot in the parameter space of the catch bond strength and the stall force of the motor. This phase plot yields predictions that may be experimentally accessed by suitable modifications of motor transport and binding properties. Our work necessitates a reexamination of existing theories of collective bidirectional transport of cellular cargo where the catch bond effect of dynein described in this paper is expected to play a crucial role."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary effect of catch bonding in dynein motors on the unidirectional transport properties of cellular cargo, and how does it differ from the behavior of kinesin motors?\n\n**A)** Catch bonding increases the unbinding rate of a single dynein, leading to a decrease in the average velocity of cellular cargo with increasing applied load.\n**B)** Catch bonding decreases the unbinding rate of a single dynein, resulting in a decrease in the average velocity of cellular cargo with increasing applied load, and is absent in kinesin motors.\n**C)** Catch bonding has no effect on the unidirectional transport properties of cellular cargo, and its behavior is identical to that of kinesin motors.\n**D)** Catch bonding increases the average velocity of cellular cargo with increasing applied load, and is a characteristic feature of kinesin motors.\n\n**Correct Answer:** B) Catch bonding decreases the unbinding rate of a single dynein, resulting in a decrease in the average velocity of cellular cargo with increasing applied load, and is absent in kinesin motors.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"catch bonding sets in beyond a critical applied load force\" and that \"we find catch bonding can result in dramatic changes in the transport properties, which are in sharp contrast to kinesin driven unidirectional transport, where catch bonding is absent.\" This indicates that catch bonding decreases the unbinding rate of a single dynein, leading to a decrease in the average velocity of cellular cargo with increasing applied load, and is a characteristic feature of dynein motors, not kinesin motors."}, "56": {"documentation": {"title": "Proposal for measuring out-of-time-ordered correlators at finite\n  temperature with coupled spin chains", "source": "Bhuvanesh Sundar, Andreas Elben, Lata Kh Joshi, Torsten V. Zache", "docs_id": "2107.02196", "section": ["quant-ph", "cond-mat.quant-gas", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Proposal for measuring out-of-time-ordered correlators at finite\n  temperature with coupled spin chains. Information scrambling, which is the spread of local information through a system's many-body degrees of freedom, is an intrinsic feature of many-body dynamics. In quantum systems, the out-of-time-ordered correlator (OTOC) quantifies information scrambling. Motivated by experiments that have measured the OTOC at infinite temperature and a theory proposal to measure the OTOC at finite temperature using the thermofield double state, we describe a protocol to measure the OTOC in a finite temperature spin chain that is realized approximately as one half of the ground state of two moderately-sized coupled spin chains. We consider a spin Hamiltonian with particle-hole symmetry, for which we show that the OTOC can be measured without needing sign-reversal of the Hamiltonian. We describe a protocol to mitigate errors in the estimated OTOC, arising from the finite approximation of the system to the thermofield double state. We show that our protocol is also robust to main sources of decoherence in experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation behind the proposal to measure the out-of-time-ordered correlator (OTOC) at finite temperature using the thermofield double state, and how does this relate to the concept of information scrambling in quantum systems?\n\nA) To study the behavior of quantum systems at finite temperature and understand the role of information scrambling in their dynamics.\nB) To measure the OTOC at infinite temperature and compare it with the results obtained at finite temperature.\nC) To develop a new theoretical framework for understanding the thermodynamic properties of quantum systems.\nD) To investigate the effects of decoherence on the OTOC in finite-temperature quantum systems.\n\nCorrect Answer: A) To study the behavior of quantum systems at finite temperature and understand the role of information scrambling in their dynamics.\n\nExplanation: The correct answer is A) because the proposal to measure the OTOC at finite temperature using the thermofield double state is motivated by the need to study the behavior of quantum systems at finite temperature and understand the role of information scrambling in their dynamics. The thermofield double state is a theoretical framework that allows for the study of quantum systems at finite temperature, and the OTOC is a quantity that quantifies information scrambling in these systems. The other options are incorrect because they do not accurately capture the motivation behind the proposal.\n\nNote: The other options are designed to be plausible but incorrect, requiring the test-taker to carefully read and understand the documentation to arrive at the correct answer."}, "57": {"documentation": {"title": "Plausible \"faster-than-light\" displacements in a two-sheeted spacetime", "source": "Fabrice Petit, Michael Sarrazin", "docs_id": "0706.4025", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Plausible \"faster-than-light\" displacements in a two-sheeted spacetime. In this paper, we explore the implications of a two-point discretization of an extra-dimension in a five-dimensional quantum setup. We adopt a pragmatic attitude by considering the dynamics of spin-half particles through the simplest possible extension of the existing Dirac and Pauli equations. It is shown that the benefit of this approach is to predict new physical phenomena while maintaining the number of constitutive hypothesis at minimum. As the most striking feature of the model, we demonstrate the possibility of fermionic matter oscillations between the two four-dimensional sections and hyper-fast displacements in case of asymmetric warping (without conflicting special relativity). This result, similar to previous reported ones in brane-world theories, is completely original as it is derived by using quantum mechanics only without recourse to general relativity and bulk geodesics calculation. The model allows causal contact between normally disconnected regions. If it proves to be physically founded, its practical aspects could have deep implications for the search of extra-dimensions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the paper \"Plausible 'faster-than-light' displacements in a two-sheeted spacetime\", what is the primary benefit of adopting a pragmatic attitude by considering the dynamics of spin-half particles through the simplest possible extension of the existing Dirac and Pauli equations?\n\nA) To introduce new physical phenomena that require additional constitutive hypotheses\nB) To maintain the number of constitutive hypotheses at minimum while predicting new physical phenomena\nC) To derive a model that conflicts with special relativity\nD) To simplify the calculation of bulk geodesics in general relativity\n\nCorrect Answer: B) To maintain the number of constitutive hypotheses at minimum while predicting new physical phenomena\n\nExplanation: The correct answer is B) because the paper states that the benefit of this approach is to predict new physical phenomena while maintaining the number of constitutive hypothesis at minimum. This is a key aspect of the model, as it allows for the prediction of new physical phenomena without introducing unnecessary complexity. The other options are incorrect because they do not accurately reflect the primary benefit of the approach described in the paper. Option A is incorrect because the paper actually aims to minimize constitutive hypotheses, not introduce new ones. Option C is incorrect because the paper does not conflict with special relativity, but rather demonstrates the possibility of fermionic matter oscillations between the two four-dimensional sections without conflicting with special relativity. Option D is incorrect because the paper does not focus on simplifying the calculation of bulk geodesics in general relativity, but rather uses quantum mechanics to derive the results."}, "58": {"documentation": {"title": "Densely related groups", "source": "Yves Cornulier, Adrien Le Boudec", "docs_id": "1610.09469", "section": ["math.GR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Densely related groups. We study the class of densely related groups. These are finitely generated (or more generally, compactly generated locally compact) groups satisfying a strong negation of being finitely presented, in the sense that new relations appear at all scales. Here, new relations means relations that do not follow from relations of smaller size. Being densely related is a quasi-isometry invariant among finitely generated groups. We check that a densely related group has none of its asymptotic cones simply connected. In particular a lacunary hyperbolic group cannot be densely related. We prove that the Grigorchuk group is densely related. We also show that a finitely generated group that is (infinite locally finite)-by-cyclic and which satisfies a law must be densely related. Given a class $\\mathcal{C}$ of finitely generated groups, we consider the following dichotomy: every group in $\\mathcal{C}$ is either finitely presented or densely related. We show that this holds within the class of nilpotent-by-cyclic groups and the class of metabelian groups. In contrast, this dichotomy is no longer true for the class of $3$-step solvable groups."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the class of finitely generated groups that are either finitely presented or densely related. Determine which of the following classes of groups satisfy this dichotomy.\n\nA) The class of all finitely generated groups\nB) The class of nilpotent-by-cyclic groups\nC) The class of metabelian groups\nD) The class of 3-step solvable groups\n\nCorrect Answer: B) The class of nilpotent-by-cyclic groups\n\nExplanation: The documentation states that the dichotomy holds within the class of nilpotent-by-cyclic groups. This means that every nilpotent-by-cyclic group is either finitely presented or densely related. The other options do not satisfy this dichotomy, as the documentation explicitly states that the dichotomy fails for the class of 3-step solvable groups and that the class of metabelian groups is not explicitly mentioned as satisfying the dichotomy. Option A is incorrect because the documentation does not claim that the dichotomy holds for all finitely generated groups."}, "59": {"documentation": {"title": "Random matrices associated with general barrier billiards", "source": "Eugene Bogomolny", "docs_id": "2111.00198", "section": ["nlin.CD", "math-ph", "math.MP", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Random matrices associated with general barrier billiards. The paper is devoted to the derivation of random unitary matrices whose spectral statistics is the same as statistics of quantum eigenvalues of certain deterministic two-dimensional barrier billiards. These random matrices are extracted from the exact billiard quantisation condition by applying a random phase approximation for high-excited states. An important ingredient of the method is the calculation of $S$-matrix for the scattering in the slab with a half-plane inside by the Wiener-Hopf method. It appears that these random matrices have the form similar to the one obtained by the author in [arXiv:2107.03364] for a particular case of symmetric barrier billiards but with different choices of parameters. The local correlation functions of the resulting random matrices are well approximated by the semi-Poisson distribution which is a characteristic feature of various models with intermediate statistics. Consequently, local spectral statistics of the considered barrier billiards is (i) universal for almost all values of parameters and (ii) well described by the semi-Poisson statistics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The paper \"Random matrices associated with general barrier billiards\" discusses the derivation of random unitary matrices whose spectral statistics is the same as the statistics of quantum eigenvalues of certain deterministic two-dimensional barrier billiards. What is the key ingredient of the method used to calculate the $S$-matrix for the scattering in the slab with a half-plane inside, and what is the characteristic feature of the local spectral statistics of the resulting random matrices?\n\nA) The key ingredient is the calculation of the $S$-matrix using the Wiener-Hopf method, and the characteristic feature is the Poisson distribution.\nB) The key ingredient is the calculation of the $S$-matrix using the Wiener-Hopf method, and the characteristic feature is the semi-Poisson distribution.\nC) The key ingredient is the calculation of the $S$-matrix using the Fourier transform method, and the characteristic feature is the Gaussian distribution.\nD) The key ingredient is the calculation of the $S$-matrix using the Wiener-Hopf method, and the characteristic feature is the exponential distribution.\n\n**Correct Answer:** B) The key ingredient is the calculation of the $S$-matrix using the Wiener-Hopf method, and the characteristic feature is the semi-Poisson distribution.\n\n**Explanation:** The correct answer is B) because the paper states that the local correlation functions of the resulting random matrices are well approximated by the semi-Poisson distribution, which is a characteristic feature of various models with intermediate statistics. The Wiener-Hopf method is mentioned as the method used to calculate the $S$-matrix, which is also mentioned in the paper. The other options are incorrect because the paper does not mention the Fourier transform method or the exponential distribution as the characteristic feature of the local spectral statistics."}}