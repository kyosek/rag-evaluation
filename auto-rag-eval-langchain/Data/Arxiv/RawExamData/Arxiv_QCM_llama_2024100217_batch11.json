{"0": {"documentation": {"title": "Composite Cyclotomic Fourier Transforms with Reduced Complexities", "source": "Xuebin Wu, Meghanad Wagh, Ning Chen, Zhiyuan Yan, and Ying Wang", "docs_id": "1007.1213", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Composite Cyclotomic Fourier Transforms with Reduced Complexities. Discrete Fourier transforms~(DFTs) over finite fields have widespread applications in digital communication and storage systems. Hence, reducing the computational complexities of DFTs is of great significance. Recently proposed cyclotomic fast Fourier transforms (CFFTs) are promising due to their low multiplicative complexities. Unfortunately, there are two issues with CFFTs: (1) they rely on efficient short cyclic convolution algorithms, which has not been investigated thoroughly yet, and (2) they have very high additive complexities when directly implemented. In this paper, we address both issues. One of the main contributions of this paper is efficient bilinear 11-point cyclic convolution algorithms, which allow us to construct CFFTs over GF$(2^{11})$. The other main contribution of this paper is that we propose composite cyclotomic Fourier transforms (CCFTs). In comparison to previously proposed fast Fourier transforms, our CCFTs achieve lower overall complexities for moderate to long lengths, and the improvement significantly increases as the length grows. Our 2047-point and 4095-point CCFTs are also first efficient DFTs of such lengths to the best of our knowledge. Finally, our CCFTs are also advantageous for hardware implementations due to their regular and modular structure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the two main issues with cyclotomic fast Fourier transforms (CFFTs) that the authors of the paper \"Composite Cyclotomic Fourier Transforms with Reduced Complexities\" aim to address?\n\n**A)** CFFTs have high multiplicative complexities and are not suitable for hardware implementations.\n**B)** CFFTs rely on efficient short cyclic convolution algorithms and have very high additive complexities when directly implemented.\n**C)** CFFTs are not efficient for large lengths and are not suitable for digital communication systems.\n**D)** CFFTs are not regular and modular structures, making them difficult to implement in hardware.\n\n**Correct Answer:** B) CFFTs rely on efficient short cyclic convolution algorithms and have very high additive complexities when directly implemented.\n\n**Explanation:** The authors of the paper identify two main issues with CFFTs: (1) they rely on efficient short cyclic convolution algorithms, which has not been investigated thoroughly yet, and (2) they have very high additive complexities when directly implemented. The authors address these issues by proposing efficient bilinear 11-point cyclic convolution algorithms and composite cyclotomic Fourier transforms (CCFTs), which achieve lower overall complexities for moderate to long lengths."}, "1": {"documentation": {"title": "A New Formal Approach for Predicting Period Doubling Bifurcations in\n  Switching Converters", "source": "A. El Aroudi", "docs_id": "1204.5140", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A New Formal Approach for Predicting Period Doubling Bifurcations in\n  Switching Converters. Period doubling bifurcation leading to subharmonic oscillations are undesired phenomena in switching converters. In past studies, their prediction has been mainly tackled by explicitly deriving a discrete time model and then linearizing it in the vicinity of the operating point. However, the results obtained from such an approach cannot be applied for design purpose. Alternatively, in this paper, the subharmonic oscillations in voltage mode controlled DC-DC buck converters are predicted by using a formal symbolic approach. This approach is based on expressing the subharmonic oscillation conditions in the frequency domain and then converting the results to generalized hypergeometric functions. The obtained expressions depend explicitly on the system parameters and the operating duty cycle making the results directly applicable for design purpose. Under certain practical conditions concerning these parameters, the hypergeometric functions can be approximated by polylogarithm and standard functions. The new approach is demonstrated using an example of voltage-mode-controlled buck converters. It is found that the stability of the converter is strongly dependent upon a polynomial function of the duty cycle."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of predicting period doubling bifurcations in switching converters, what is the primary limitation of the traditional approach of explicitly deriving a discrete time model and linearizing it in the vicinity of the operating point?\n\nA) The approach is too computationally intensive.\nB) The results are not directly applicable for design purpose.\nC) The approach is limited to only voltage-mode controlled converters.\nD) The approach is only applicable for buck converters.\n\n**Correct Answer:** B) The results are not directly applicable for design purpose.\n\n**Explanation:** The traditional approach of explicitly deriving a discrete time model and linearizing it in the vicinity of the operating point is limited because the resulting expressions are not directly applicable for design purpose. This is because the approach does not take into account the system parameters and operating duty cycle, making it difficult to use the results for design purposes. In contrast, the new formal symbolic approach presented in the paper explicitly depends on the system parameters and operating duty cycle, making the results directly applicable for design purpose.\n\n**Additional question:** What is the relationship between the stability of the converter and the polynomial function of the duty cycle, as demonstrated in the example of voltage-mode-controlled buck converters?\n\nA) The stability of the converter is directly proportional to the polynomial function of the duty cycle.\nB) The stability of the converter is inversely proportional to the polynomial function of the duty cycle.\nC) The stability of the converter is independent of the polynomial function of the duty cycle.\nD) The stability of the converter is dependent on the polynomial function of the duty cycle, but only for certain values of the duty cycle.\n\n**Correct Answer:** D) The stability of the converter is dependent on the polynomial function of the duty cycle, but only for certain values of the duty cycle.\n\n**Explanation:** The stability of the converter is strongly dependent on a polynomial function of the duty cycle, but only under certain practical conditions. This means that the relationship between stability and the polynomial function is not always straightforward and depends on the specific values of the duty cycle."}, "2": {"documentation": {"title": "Finding a promising venture capital project with todim under\n  probabilistic hesitant fuzzy circumstance", "source": "Weike Zhang, Jiang Du, Xiaoli Tian", "docs_id": "1809.00128", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Finding a promising venture capital project with todim under\n  probabilistic hesitant fuzzy circumstance. Considering the risk aversion for gains and the risk seeking for losses of venture capitalists, the TODIM has been chosen as the decision-making method. Moreover, group decision is an available way to avoid the limited ability and knowledge etc. of venture capitalists.Simultaneously, venture capitalists may be hesitant among several assessed values with different probabilities to express their real perceptionbecause of the uncertain decision-making environment. However, the probabilistic hesitant fuzzy information can solve such problems effectively. Therefore, the TODIM has been extended to probabilistic hesitant fuzzy circumstance for the sake of settling the decision-making problem of venture capitalists in this paper. Moreover, due to the uncertain investment environment, the criteria weights are considered as probabilistic hesitant fuzzyinformation as well. Then, a case study has been used to verify the feasibility and validity of the proposed TODIM.Also, the TODIM with hesitant fuzzy information has been carried out to analysis the same case.From the comparative analysis, the superiority of the proposed TODIM in this paper has already appeared."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of venture capital decision-making, what is the primary advantage of extending the TODIM method to probabilistic hesitant fuzzy circumstance, as proposed in the paper?\n\n**A)** It allows for more accurate evaluation of uncertain investment environments.\n**B)** It enables venture capitalists to express their hesitant perceptions more effectively.\n**C)** It provides a more comprehensive framework for group decision-making under probabilistic hesitant fuzzy circumstances.\n**D)** It facilitates the calculation of probabilistic hesitant fuzzy criteria weights.\n\n**Correct Answer:** C) It provides a more comprehensive framework for group decision-making under probabilistic hesitant fuzzy circumstances.\n\n**Explanation:** The correct answer is C) because the paper proposes extending the TODIM method to probabilistic hesitant fuzzy circumstance to settle the decision-making problem of venture capitalists in uncertain investment environments. This extension enables a more comprehensive framework for group decision-making, taking into account the probabilistic hesitant fuzzy information of both the criteria and the criteria weights. The other options are incorrect because they do not accurately capture the primary advantage of the proposed TODIM method."}, "3": {"documentation": {"title": "Using invariant manifolds to construct symbolic dynamics for\n  three-dimensional volume-preserving maps", "source": "Bryan Maelfeyt, Spencer A. Smith, Kevin A. Mitchell", "docs_id": "1607.07346", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Using invariant manifolds to construct symbolic dynamics for\n  three-dimensional volume-preserving maps. Topological techniques are powerful tools for characterizing the complexity of many dynamical systems, including the commonly studied area-preserving maps of the plane. However, the extension of many topological techniques to higher dimensions is filled with roadblocks preventing their application. This article shows how to extend the homotopic lobe dynamics (HLD) technique, previously developed for 2D maps, to volume-preserving maps of a three-dimensional phase space. Such maps are physically relevant to particle transport by incompressible fluid flows or by magnetic field lines. Specifically, this manuscript shows how to utilize two-dimensional stable and unstable invariant manifolds, intersecting in a heteroclinic tangle, to construct a symbolic representation of the topological dynamics of the map. This symbolic representation can be used to classify system trajectories and to compute topological entropy. We illustrate the salient ideas through a series of examples with increasing complexity. These examples highlight new features of the HLD technique in 3D. Ultimately, in the final example, our technique detects a difference between the 2D stretching rate of surfaces and the 1D stretching rate of curves, illustrating the truly 3D nature of our approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Using invariant manifolds to construct symbolic dynamics for three-dimensional volume-preserving maps, what is the primary advantage of utilizing two-dimensional stable and unstable invariant manifolds, intersecting in a heteroclinic tangle, in the homotopic lobe dynamics (HLD) technique?\n\nA) It allows for the computation of topological entropy in higher dimensions.\nB) It enables the classification of system trajectories based on their topological properties.\nC) It facilitates the detection of heteroclinic bifurcations in three-dimensional maps.\nD) It provides a symbolic representation of the topological dynamics of the map, enabling the analysis of system behavior.\n\nCorrect Answer: D) It provides a symbolic representation of the topological dynamics of the map, enabling the analysis of system behavior.\n\nExplanation: The correct answer is D) because the HLD technique utilizes two-dimensional stable and unstable invariant manifolds to construct a symbolic representation of the topological dynamics of the map. This symbolic representation enables the analysis of system behavior, including the classification of system trajectories and the computation of topological entropy. The other options are incorrect because while they may be related to the topic, they are not the primary advantage of utilizing invariant manifolds in the HLD technique.\n\nNote: The other options are designed to be plausible but incorrect, requiring the test-taker to carefully read and understand the documentation to arrive at the correct answer."}, "4": {"documentation": {"title": "Wealth share analysis with \"fundamentalist/chartist\" heterogeneous\n  agents", "source": "Hai-Chuan Xu (TJU), Wei Zhang (TJU), Xiong Xiong (TJU), Wei-Xing Zhou\n  (ECUST)", "docs_id": "1405.5939", "section": ["q-fin.TR", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Wealth share analysis with \"fundamentalist/chartist\" heterogeneous\n  agents. We build a multiassets heterogeneous agents model with fundamentalists and chartists, who make investment decisions by maximizing the constant relative risk aversion utility function. We verify that the model can reproduce the main stylized facts in real markets, such as fat-tailed return distribution and long-term memory in volatility. Based on the calibrated model, we study the impacts of the key strategies' parameters on investors' wealth shares. We find that, as chartists' exponential moving average periods increase, their wealth shares also show an increasing trend. This means that higher memory length can help to improve their wealth shares. This effect saturates when the exponential moving average periods are sufficiently long. On the other hand, the mean reversion parameter has no obvious impacts on wealth shares of either type of traders. It suggests that no matter whether fundamentalists take moderate strategy or aggressive strategy on the mistake of stock prices, it will have no different impact on their wealth shares in the long run."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the study on the impacts of chartists' exponential moving average periods on their wealth shares, and what does this suggest about the optimal strategy for chartists?\n\nA) As chartists' exponential moving average periods increase, their wealth shares decrease, indicating that shorter periods are more effective.\nB) The study finds that chartists' wealth shares increase with longer exponential moving average periods, suggesting that higher memory length can improve their wealth shares.\nC) The mean reversion parameter has a significant impact on the wealth shares of chartists, and longer periods lead to higher wealth shares.\nD) The study reveals that the optimal strategy for chartists is to use shorter exponential moving average periods, as longer periods lead to decreased wealth shares.\n\nCorrect Answer: B) The study finds that chartists' wealth shares increase with longer exponential moving average periods, suggesting that higher memory length can improve their wealth shares.\n\nExplanation: The correct answer is B) because the study finds that as chartists' exponential moving average periods increase, their wealth shares also show an increasing trend. This suggests that higher memory length can help to improve their wealth shares. The other options are incorrect because they either misinterpret the results of the study or suggest the opposite conclusion."}, "5": {"documentation": {"title": "Painlev\\'e analysis for nonlinear partial differential equations", "source": "M. Musette (VUB, Brussels)", "docs_id": "solv-int/9804003", "section": ["nlin.SI", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Painlev\\'e analysis for nonlinear partial differential equations. The Painlev\\'e analysis introduced by Weiss, Tabor and Carnevale (WTC) in 1983 for nonlinear partial differential equations (PDE's) is an extension of the method initiated by Painlev\\'e and Gambier at the beginning of this century for the classification of algebraic nonlinear differential equations (ODE's) without movable critical points. In these lectures we explain the WTC method in its invariant version introduced by Conte in 1989 and its application to solitonic equations in order to find algorithmically their associated B\\\"acklund transformation. A lot of remarkable properties are shared by these so-called ``integrable'' equations but they are generically no more valid for equations modelising physical phenomema. Belonging to this second class, some equations called ``partially integrable'' sometimes keep remnants of integrability. In that case, the singularity analysis may also be useful for building closed form analytic solutions, which necessarily % Conte agree with the singularity structure of the equations. We display the privileged role played by the Riccati equation and systems of Riccati equations which are linearisable, as well as the importance of the Weierstrass elliptic function, for building solitary waves or more elaborate solutions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Painlev\u00e9 analysis, what is the significance of the Weierstrass elliptic function in the construction of solitary waves or more elaborate solutions for nonlinear partial differential equations, and how does it relate to the classification of integrable and partially integrable equations?\n\n**A)** The Weierstrass elliptic function is a fundamental tool in the construction of solitary waves, but its role is limited to linearizable Riccati equations. In contrast, partially integrable equations often rely on other mathematical structures, such as the Riccati equation itself.\n\n**B)** The Weierstrass elliptic function plays a crucial role in the classification of integrable equations, as it is essential for determining the singularity structure of these equations. However, its significance is diminished in partially integrable equations, where other mathematical structures, such as the Riccati equation, take center stage.\n\n**C)** The Weierstrass elliptic function is a key component in the construction of solitary waves for both integrable and partially integrable equations. Its importance lies in its ability to capture the underlying mathematical structure of these equations, which is essential for building closed-form analytic solutions.\n\n**D)** The Weierstrass elliptic function is a fundamental tool in the construction of solitary waves, but its significance is limited to the context of Painlev\u00e9 analysis. In other areas of mathematics, such as algebraic geometry or number theory, the Weierstrass elliptic function plays a much more prominent role.\n\n**Correct Answer:** C) The Weierstrass elliptic function is a key component in the construction of solitary waves for both integrable and partially integrable equations. Its importance lies in its ability to capture the underlying mathematical structure of these equations, which is essential for building closed-form analytic solutions.\n\n**Explanation:** The correct answer, C, highlights the significance of the Weierstrass elliptic function in the construction of solitary waves for both integrable and partially integrable equations. This function plays a crucial role in capturing the underlying mathematical structure of these equations, which is essential for building closed-form analytic solutions. The other options are incorrect because they either limit the significance of the Weierstrass elliptic function to specific contexts (A and B) or downplay its importance altogether (D)."}, "6": {"documentation": {"title": "Quasi-continuum approximation to the Nonlinear Schr\\\"odinger equation\n  with Long-range dispersions", "source": "Alain M. Dikand\\'e", "docs_id": "nlin/0402020", "section": ["nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quasi-continuum approximation to the Nonlinear Schr\\\"odinger equation\n  with Long-range dispersions. The long-wavelength, weak-dispersion limit of the discrete nonlinear Schr\\\"odinger equation with long-range dispersion is analytically considered. This continuum approximation is carried out irrespective of the dispersion range and hence can be assumed exact in the weak dispersion regime. For nonlinear Schr\\\"odinger equations showing finite dispersion extents, the long-range parameter is still a relevant control parameter allowing to tune the dispersion from short-range to long-range regimes with respect to the dispersion extent. The long-range Kac-Baker potential becomes unappropriate in this context owing to an \"edge anomaly\" consisting of vanishing maximum dispersion frequency and group velocity(and in turn soliton width) in the \"Debye\" limit. An improved Kac-Baker potential is then considered which gives rise to a non-zero maximum frequency, and allows for soliton excitations with finite widths in the nonlinear Schr\\\"odinger system subjected to the long-range but finite-extent dispersion."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the long-wavelength, weak-dispersion limit of the discrete nonlinear Schr\u00f6dinger equation with long-range dispersion. What is the main advantage of using the improved Kac-Baker potential in this context, and how does it differ from the traditional long-range Kac-Baker potential?\n\nA) The improved Kac-Baker potential allows for soliton excitations with finite widths in the nonlinear Schr\u00f6dinger system subjected to the long-range but finite-extent dispersion, whereas the traditional long-range Kac-Baker potential does not. However, it also leads to an \"edge anomaly\" consisting of vanishing maximum dispersion frequency and group velocity.\n\nB) The improved Kac-Baker potential is more suitable for systems with finite dispersion extents, whereas the traditional long-range Kac-Baker potential is more suitable for systems with infinite dispersion extents.\n\nC) The improved Kac-Baker potential is a more general form of the traditional long-range Kac-Baker potential, allowing for soliton excitations with finite widths in both systems with finite and infinite dispersion extents.\n\nD) The improved Kac-Baker potential is a more complex form of the traditional long-range Kac-Baker potential, requiring more computational resources to solve.\n\nCorrect Answer: A) The improved Kac-Baker potential allows for soliton excitations with finite widths in the nonlinear Schr\u00f6dinger system subjected to the long-range but finite-extent dispersion, whereas the traditional long-range Kac-Baker potential does not. However, it also leads to an \"edge anomaly\" consisting of vanishing maximum dispersion frequency and group velocity.\n\nExplanation: The correct answer is A) because the improved Kac-Baker potential is specifically designed to address the \"edge anomaly\" issue in the traditional long-range Kac-Baker potential, allowing for soliton excitations with finite widths in systems with finite dispersion extents. The other options are incorrect because they either oversimplify the relationship between the two potentials or introduce incorrect information."}, "7": {"documentation": {"title": "Radio astronomy in Africa: the case of Ghana", "source": "Bernard Duah Asabere, Michael Gaylard, Cathy Horellou, Hartmut Winkler\n  and Thomas Jarrett", "docs_id": "1503.08850", "section": ["astro-ph.IM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Radio astronomy in Africa: the case of Ghana. South Africa has played a leading role in radio astronomy in Africa with the Hartebeesthoek Radio Astronomy Observatory (HartRAO). It continues to make strides with the current seven-dish MeerKAT precursor array (KAT-7), leading to the 64-dish MeerKAT and the giant Square Kilometer Array (SKA), which will be used for transformational radio astronomy research. Ghana, an African partner to the SKA, has been mentored by South Africa over the past six years and will soon emerge in the field of radio astronomy. The country will soon have a science-quality 32m dish converted from a redundant satellite communication antenna. Initially, it will be fitted with 5 GHz and 6.7 GHz receivers to be followed later by a 1.4 - 1.7 GHz receiver. The telescope is being designed for use as a single dish observatory and for participation in the developing African Very Long Baseline Interferometry (VLBI) Network (AVN) and the European VLBI Network. Ghana is earmarked to host a remote station during a possible SKA Phase 2. The location of the country on 5 degree north of the Equator gives it the distinct advantage of viewing the entire plane of the Milky Way galaxy and nearly the whole sky. In this article, we present the case of Ghana in the radio astronomy scene and the science/technology that will soon be carried out by engineers and astronomers."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of Ghana's location in terms of viewing the sky for radio astronomy research?\n\nA) Its proximity to the South Pole\nB) Its location on 5 degrees north of the Equator, which allows it to view the entire plane of the Milky Way galaxy and nearly the whole sky\nC) Its distance from the equator, which provides a unique perspective on the sky\nD) Its location in the Southern Hemisphere, which offers a distinct advantage in terms of astronomical observations\n\nCorrect Answer: B) Its location on 5 degrees north of the Equator, which allows it to view the entire plane of the Milky Way galaxy and nearly the whole sky\n\nExplanation: The correct answer is B) Its location on 5 degrees north of the Equator, which allows it to view the entire plane of the Milky Way galaxy and nearly the whole sky. This is stated in the documentation as a distinct advantage of Ghana's location, making it an ideal location for radio astronomy research. The other options are incorrect because they do not accurately reflect the advantages of Ghana's location for radio astronomy research."}, "8": {"documentation": {"title": "Causal Inference for Spatial Treatments", "source": "Michael Pollmann", "docs_id": "2011.00373", "section": ["econ.EM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Causal Inference for Spatial Treatments. I propose a framework, estimators, and inference procedures for the analysis of causal effects in a setting with spatial treatments. Many events and policies (treatments), such as opening of businesses, building of hospitals, and sources of pollution, occur at specific spatial locations, with researchers interested in their effects on nearby individuals or businesses (outcome units). However, the existing treatment effects literature primarily considers treatments that could be assigned directly at the level of the outcome units, potentially with spillover effects. I approach the spatial treatment setting from a similar experimental perspective: What ideal experiment would we design to estimate the causal effects of spatial treatments? This perspective motivates a comparison between individuals near realized treatment locations and individuals near unrealized candidate locations, which is distinct from current empirical practice. Furthermore, I show how to find such candidate locations and apply the proposed methods with observational data. I apply the proposed methods to study the causal effects of grocery stores on foot traffic to nearby businesses during COVID-19 lockdowns."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of causal inference for spatial treatments, what is the primary limitation of the existing treatment effects literature, and how does the proposed framework address this limitation?\n\nA) The existing literature primarily considers treatments that can be assigned directly at the level of the outcome units, but the proposed framework focuses on spillover effects, whereas the literature focuses on direct assignment. (Incorrect)\nB) The existing literature primarily considers treatments that can be assigned directly at the level of the outcome units, which may lead to biased estimates of causal effects, whereas the proposed framework compares individuals near realized treatment locations to individuals near unrealized candidate locations. (Correct)\nC) The existing literature primarily considers treatments that are not spatially localized, whereas the proposed framework focuses on spatial treatments that occur at specific locations. (Incorrect)\nD) The existing literature primarily considers treatments that are not causal, whereas the proposed framework focuses on causal effects of spatial treatments. (Incorrect)\n\n**Explanation:** The correct answer is B) because the existing treatment effects literature primarily considers treatments that can be assigned directly at the level of the outcome units, which may lead to biased estimates of causal effects. The proposed framework addresses this limitation by comparing individuals near realized treatment locations to individuals near unrealized candidate locations, which is distinct from current empirical practice."}, "9": {"documentation": {"title": "High precision wavelength estimation method for integrated optics", "source": "R.M. Oldenbeuving, H. Song, G. Schitter, M. Verhaegen, E.J. Klein,\n  C.J. Lee, H.L. Offerhaus, K.-J. Boller", "docs_id": "1304.6063", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "High precision wavelength estimation method for integrated optics. A novel and simple approach to optical wavelength measurement is presented in this paper. The working principle is demonstrated using a tunable waveguide micro ring resonator and single photodiode. The initial calibration is done with a set of known wavelengths and resonator tunings. The combined spectral sensitivity function of the resonator and photodiode at each tuning voltage was modeled by a neural network. For determining the unknown wavelengths, the resonator was tuned with a set of heating voltages and the corresponding photodiode signals are collected. The unknown wavelength was estimated, based on the collected photodiode signals, the calibrated neural networks, and an optimization algorithm. The wavelength estimate method provides a high spectral precision of about 8 pm (5*10^(-6) at 1550 nm) in the wavelength range between 1549 nm to 1553 nm. A higher precision of 5 pm (3*10^(-6)) is achieved in the range between 1550.3 nm to 1550.8 nm, which is a factor of five improved compared to a simple lookup of data. The importance of our approach is that it strongly simplifies the optical system and enables optical integration. The approach is also of general importance, because it may be applicable to all wavelength monitoring devices which show an adjustable wavelength response."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed wavelength estimation method in the context of integrated optics?\n\nA) It requires a complex optical system with multiple components.\nB) It simplifies the optical system and enables optical integration.\nC) It relies solely on a lookup table for wavelength data.\nD) It is limited to a specific wavelength range.\n\n**Correct Answer:** B) It simplifies the optical system and enables optical integration.\n\n**Explanation:** The correct answer is B) It simplifies the optical system and enables optical integration. The proposed method simplifies the optical system by using a tunable waveguide micro ring resonator and a single photodiode, which reduces the complexity of the system. This simplification enables optical integration, making the system more compact and efficient. The other options are incorrect because they do not accurately reflect the advantages of the proposed method. Option A is incorrect because the method actually simplifies the system, not increases its complexity. Option C is incorrect because the method uses a neural network to model the spectral sensitivity function, not a lookup table. Option D is incorrect because the method is applicable to a range of wavelength values, not limited to a specific range."}, "10": {"documentation": {"title": "Shear Power Spectrum Reconstruction using Pseudo-Spectrum Method", "source": "Chiaki Hikage, Masahiro Takada, Takashi Hamana, David Spergel", "docs_id": "1004.3542", "section": ["astro-ph.CO", "physics.data-an"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Shear Power Spectrum Reconstruction using Pseudo-Spectrum Method. We develop a pseudo power spectrum technique for measuring the lensing power spectrum from weak lensing surveys in both the full sky and flat sky limits. The power spectrum approaches have a number of advantages over the traditional correlation function approach. We test the pseudo spectrum method by using numerical simulations with square-shape boundary that include masked regions with complex configuration due to bright stars and saturated spikes. Even when 25% of total area of the survey is masked, the method recovers the E-mode power spectrum at a sub-percent precision over a wide range of multipoles 100<l<10000. The systematic error is smaller than the statistical errors expected for a 2000 square degree survey. The residual B-mode spectrum is well suppressed in the amplitudes at less than a percent level relative to the E-mode. We also find that the correlated errors of binned power spectra caused by the survey geometry effects are not significant. Our method is applicable to the current and upcoming wide-field lensing surveys."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the key advantages of the pseudo-spectrum method over the traditional correlation function approach for measuring the lensing power spectrum from weak lensing surveys?\n\nA) The pseudo-spectrum method is more computationally intensive and requires more data.\nB) The pseudo-spectrum method is more sensitive to systematics and requires more careful masking.\nC) The pseudo-spectrum method has a smaller systematic error and is more robust against survey geometry effects.\nD) The pseudo-spectrum method is only applicable to flat sky limits and not suitable for full sky surveys.\n\nCorrect Answer: C) The pseudo-spectrum method has a smaller systematic error and is more robust against survey geometry effects.\n\nExplanation: The question requires the test-taker to analyze the advantages of the pseudo-spectrum method mentioned in the documentation. The correct answer, C, highlights the key benefit of the pseudo-spectrum method, which is its ability to reduce systematic errors and be more robust against survey geometry effects. The other options are incorrect because they either misrepresent the advantages of the pseudo-spectrum method or are not supported by the documentation."}, "11": {"documentation": {"title": "Bayesian Estimation and Comparison of Conditional Moment Models", "source": "Siddhartha Chib, Minchul Shin, Anna Simoni", "docs_id": "2110.13531", "section": ["math.ST", "econ.EM", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian Estimation and Comparison of Conditional Moment Models. We consider the Bayesian analysis of models in which the unknown distribution of the outcomes is specified up to a set of conditional moment restrictions. The nonparametric exponentially tilted empirical likelihood function is constructed to satisfy a sequence of unconditional moments based on an increasing (in sample size) vector of approximating functions (such as tensor splines based on the splines of each conditioning variable). For any given sample size, results are robust to the number of expanded moments. We derive Bernstein-von Mises theorems for the behavior of the posterior distribution under both correct and incorrect specification of the conditional moments, subject to growth rate conditions (slower under misspecification) on the number of approximating functions. A large-sample theory for comparing different conditional moment models is also developed. The central result is that the marginal likelihood criterion selects the model that is less misspecified. We also introduce sparsity-based model search for high-dimensional conditioning variables, and provide efficient MCMC computations for high-dimensional parameters. Along with clarifying examples, the framework is illustrated with real-data applications to risk-factor determination in finance, and causal inference under conditional ignorability."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a Bayesian analysis of a conditional moment model where the unknown distribution of the outcomes is specified up to a set of conditional moment restrictions. Suppose we have a sample of size n = 100, and we use a nonparametric exponentially tilted empirical likelihood function to satisfy a sequence of unconditional moments based on an increasing vector of approximating functions (such as tensor splines based on the splines of each conditioning variable). The number of approximating functions is k = 10.\n\n**Part A:** What is the growth rate condition on the number of approximating functions required for the Bernstein-von Mises theorem to hold under correct specification of the conditional moments?\n\nA) k grows linearly with n\nB) k grows polynomially with n\nC) k grows exponentially with n\nD) k grows at a rate slower than n^2\n\n**Part B:** Suppose we have two conditional moment models, Model A and Model B, with different sets of approximating functions. Model A uses k = 5 approximating functions, while Model B uses k = 20 approximating functions. Which model is less misspecified, and why?\n\nA) Model A is less misspecified\nB) Model B is less misspecified\nC) The models are equally misspecified\nD) The question is not sufficient to determine which model is less misspecified\n\n**Part C:** What is the central result of the paper regarding the marginal likelihood criterion for comparing different conditional moment models?\n\nA) The marginal likelihood criterion selects the model with the highest marginal likelihood\nB) The marginal likelihood criterion selects the model that is most parsimonious\nC) The marginal likelihood criterion selects the model that is less misspecified\nD) The marginal likelihood criterion selects the model with the lowest marginal likelihood\n\n**Part D:** Suppose we have a high-dimensional conditioning variable with p = 100 features. What is the sparsity-based model search approach used in the paper to handle this high dimensionality?\n\nA) Lasso regularization\nB) Ridge regression\nC) Elastic net regularization\nD) Sparse tensor splines\n\n**Correct Answer:**\n\nA) Part A: k grows at a rate slower than n^2\nB) Part B: Model B is less misspecified (since it uses more approximating functions, which allows for a more accurate approximation of the true distribution)\nC) Part C: The marginal likelihood criterion selects the model that is less misspecified\nD) Part D: Sparse tensor splines\n\nNote: The correct answers require a good understanding of the concepts and techniques discussed in the paper, as well as the ability to apply them to different scenarios."}, "12": {"documentation": {"title": "Efficient Treatment Effect Estimation in Observational Studies under\n  Heterogeneous Partial Interference", "source": "Zhaonan Qu, Ruoxuan Xiong, Jizhou Liu, Guido Imbens", "docs_id": "2107.12420", "section": ["stat.ME", "econ.EM", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient Treatment Effect Estimation in Observational Studies under\n  Heterogeneous Partial Interference. In many observational studies in social science and medical applications, subjects or individuals are connected, and one unit's treatment and attributes may affect another unit's treatment and outcome, violating the stable unit treatment value assumption (SUTVA) and resulting in interference. To enable feasible inference, many previous works assume the ``exchangeability'' of interfering units, under which the effect of interference is captured by the number or ratio of treated neighbors. However, in many applications with distinctive units, interference is heterogeneous. In this paper, we focus on the partial interference setting, and restrict units to be exchangeable conditional on observable characteristics. Under this framework, we propose generalized augmented inverse propensity weighted (AIPW) estimators for general causal estimands that include direct treatment effects and spillover effects. We show that they are consistent, asymptotically normal, semiparametric efficient, and robust to heterogeneous interference as well as model misspecifications. We also apply our method to the Add Health dataset and find that smoking behavior exhibits interference on academic outcomes."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of the paper \"Efficient Treatment Effect Estimation in Observational Studies under Heterogeneous Partial Interference\", what is the primary assumption that the authors restrict units to be exchangeable conditional on observable characteristics, and how does this assumption impact the estimation of causal effects?\n\n**A)** The authors assume that units are exchangeable conditional on unobservable characteristics, which allows for the estimation of causal effects under heterogeneous interference. However, this assumption may lead to biased estimates if the unobservable characteristics are correlated with the treatment assignment.\n\n**B)** The authors assume that units are exchangeable conditional on observable characteristics, which enables the estimation of causal effects under partial interference. This assumption is robust to model misspecifications and allows for the estimation of direct treatment effects and spillover effects.\n\n**C)** The authors assume that units are exchangeable conditional on both observable and unobservable characteristics, which is a stronger assumption than the one stated in the paper. This assumption is necessary for the estimation of causal effects under heterogeneous interference.\n\n**D)** The authors assume that units are not exchangeable conditional on any characteristics, which would lead to biased estimates of causal effects under partial interference.\n\n**Correct Answer:** B) The authors assume that units are exchangeable conditional on observable characteristics, which enables the estimation of causal effects under partial interference. This assumption is robust to model misspecifications and allows for the estimation of direct treatment effects and spillover effects.\n\n**Explanation:** The correct answer is B) because the paper states that the authors restrict units to be exchangeable conditional on observable characteristics, which is a key assumption in the proposed generalized augmented inverse propensity weighted (AIPW) estimators. This assumption is crucial for the estimation of causal effects under partial interference and is robust to model misspecifications. The other options are incorrect because they either assume a stronger or weaker assumption than stated in the paper, or assume that units are not exchangeable conditional on any characteristics."}, "13": {"documentation": {"title": "A Link Generator for Increasing the Utility of OpenAPI-to-GraphQL\n  Translations", "source": "Dominik Adam Kus, Istv\\'an Koren, Ralf Klamma", "docs_id": "2005.08708", "section": ["cs.DC", "cs.DB", "cs.SE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Link Generator for Increasing the Utility of OpenAPI-to-GraphQL\n  Translations. Standardized interfaces are the connecting link of today's distributed systems, facilitating access to data services in the cloud. REST APIs have been prevalent over the last years, despite several issues like over- and underfetching of resources. GraphQL enjoys rapid adoption, resolving these problems by using statically typed queries. However, the redevelopment of services to the new paradigm is costly. Therefore, several approaches for the successive migration from REST to GraphQL have been proposed, many leveraging OpenAPI service descriptions. In this article, we present the findings of our empirical evaluation on the APIs.guru directory and identify several schema translation challenges. These include less expressive schema types in GraphQL, as well as missing meta information about related resources in OpenAPI. To this end, we developed the open source Link Generator, that analyzes OpenAPI documents and automatically adds links to increase translation utility. This fundamentally benefits around 34% of APIs in the APIs.guru directory. Our findings and tool support contribute to the ongoing discussion about the migration of REST APIs to GraphQL, and provide developers with valuable insights into common pitfalls, to reduce friction during API transformation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary motivation behind the redevelopment of services to the GraphQL paradigm, and how does the proposed approach using OpenAPI service descriptions address this challenge?\n\nA) The high cost of redevelopment is the primary motivation, and the approach uses OpenAPI to facilitate a seamless transition.\nB) The rapid adoption of GraphQL is the primary motivation, and the approach uses OpenAPI to provide a standardized interface for data services.\nC) The need for statically typed queries is the primary motivation, and the approach uses OpenAPI to resolve issues like over- and underfetching of resources.\nD) The lack of meta information about related resources is the primary motivation, and the approach uses OpenAPI to automatically add links to increase translation utility.\n\nCorrect Answer: D) The lack of meta information about related resources is the primary motivation, and the approach uses OpenAPI to automatically add links to increase translation utility.\n\nExplanation: The question requires the candidate to understand the motivations behind the redevelopment of services to the GraphQL paradigm and the role of OpenAPI in addressing this challenge. The correct answer, D, highlights the importance of meta information about related resources in OpenAPI and how the Link Generator tool addresses this challenge by automatically adding links to increase translation utility. The other options are incorrect because they either focus on secondary benefits (A and B) or a different challenge (C)."}, "14": {"documentation": {"title": "Phase-Space Volume of Regions of Trapped Motion: Multiple Ring\n  Components and Arcs", "source": "L. Benet, O. Merlo", "docs_id": "0801.2030", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phase-Space Volume of Regions of Trapped Motion: Multiple Ring\n  Components and Arcs. The phase--space volume of regions of regular or trapped motion, for bounded or scattering systems with two degrees of freedom respectively, displays universal properties. In particular, sudden reductions in the phase-space volume or gaps are observed at specific values of the parameter which tunes the dynamics; these locations are approximated by the stability resonances. The latter are defined by a resonant condition on the stability exponents of a central linearly stable periodic orbit. We show that, for more than two degrees of freedom, these resonances can be excited opening up gaps, which effectively separate and reduce the regions of trapped motion in phase space. Using the scattering approach to narrow rings and a billiard system as example, we demonstrate that this mechanism yields rings with two or more components. Arcs are also obtained, specifically when an additional (mean-motion) resonance condition is met. We obtain a complete representation of the phase-space volume occupied by the regions of trapped motion."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the phase-space volume of regions of trapped motion, what is the significance of the stability resonances in systems with more than two degrees of freedom?\n\nA) They are the points of maximum phase-space volume, where the dynamics are most stable.\nB) They are the points of minimum phase-space volume, where the dynamics are most unstable.\nC) They are the points where the central linearly stable periodic orbit is excited, opening up gaps in the phase space and reducing the regions of trapped motion.\nD) They are the points where the mean-motion resonance condition is met, resulting in the formation of arcs in the phase space.\n\n**Correct Answer:** C) They are the points where the central linearly stable periodic orbit is excited, opening up gaps in the phase space and reducing the regions of trapped motion.\n\n**Explanation:** The correct answer is C) because the stability resonances are defined by a resonant condition on the stability exponents of a central linearly stable periodic orbit. This means that when these resonances are excited, they open up gaps in the phase space, reducing the regions of trapped motion. The other options are incorrect because they do not accurately describe the significance of stability resonances in systems with more than two degrees of freedom. Option A is incorrect because stability resonances are not points of maximum phase-space volume. Option B is incorrect because stability resonances are not points of minimum phase-space volume. Option D is incorrect because the mean-motion resonance condition is a separate condition that results in the formation of arcs in the phase space, not directly related to the stability resonances."}, "15": {"documentation": {"title": "Production of $b\\bar{b}$ at forward rapidity in $p$+$p$ collisions at\n  $\\sqrt{s}=510$ GeV", "source": "U. Acharya, A. Adare, C. Aidala, N.N. Ajitanand, Y. Akiba, R. Akimoto,\n  M. Alfred, N. Apadula, Y. Aramaki, H. Asano, E.T. Atomssa, T.C. Awes, B.\n  Azmoun, V. Babintsev, M. Bai, N.S. Bandara, B. Bannier, K.N. Barish, S.\n  Bathe, A. Bazilevsky, M. Beaumier, S. Beckman, R. Belmont, A. Berdnikov, Y.\n  Berdnikov, L. Bichon, D. Black, B. Blankenship, J.S. Bok, V. Borisov, K.\n  Boyle, M.L. Brooks, J. Bryslawskyj, H. Buesching, V. Bumazhnov, S. Campbell,\n  V. Canoa Roman, C.-H. Chen, C.Y. Chi, M. Chiu, I.J. Choi, J.B. Choi, T.\n  Chujo, Z. Citron, M. Connors, M. Csan\\'ad, T. Cs\\\"org\\H{o}, A. Datta, M.S.\n  Daugherity, G. David, K. DeBlasio, K. Dehmelt, A. Denisov, A. Deshpande, E.J.\n  Desmond, L. Ding, A. Dion, J.H. Do, A. Drees, K.A. Drees, J.M. Durham, A.\n  Durum, A. Enokizono, H. En'yo, R. Esha, S. Esumi, B. Fadem, W. Fan, N. Feege,\n  D.E. Fields, M. Finger, M. Finger, Jr., D. Firak, D. Fitzgerald, S.L. Fokin,\n  J.E. Frantz, A. Franz, A.D. Frawley, C. Gal, P. Gallus, P. Garg, H. Ge, F.\n  Giordano, A. Glenn, Y. Goto, N. Grau, S.V. Greene, M. Grosse Perdekamp, Y.\n  Gu, T. Gunji, H. Guragain, T. Hachiya, J.S. Haggerty, K.I. Hahn, H. Hamagaki,\n  S.Y. Han, J. Hanks, S. Hasegawa, X. He, T.K. Hemmick, J.C. Hill, A. Hodges,\n  R.S. Hollis, K. Homma, B. Hong, T. Hoshino, J. Huang, S. Huang, Y. Ikeda, K.\n  Imai, Y. Imazu, M. Inaba, A. Iordanova, D. Isenhower, D. Ivanishchev, B.V.\n  Jacak, S.J. Jeon, M. Jezghani, Z. Ji, J. Jia, X. Jiang, B.M. Johnson, E. Joo,\n  K.S. Joo, D. Jouan, D.S. Jumper, J.H. Kang, J.S. Kang, D. Kawall, A.V.\n  Kazantsev, J.A. Key, V. Khachatryan, A. Khanzadeev, A. Khatiwada, K. Kihara,\n  C. Kim, D.H. Kim, D.J. Kim, E.-J. Kim, H.-J. Kim, M. Kim, Y.K. Kim, D.\n  Kincses, E. Kistenev, J. Klatsky, D. Kleinjan, P. Kline, T. Koblesky, M.\n  Kofarago, J. Koster, D. Kotov, B. Kurgyis, K. Kurita, M. Kurosawa, Y. Kwon,\n  R. Lacey, J.G. Lajoie, D. Larionova, M. Larionova, A. Lebedev, K.B. Lee, S.H.\n  Lee, M.J. Leitch, M. Leitgab, N.A. Lewis, X. Li, S.H. Lim, M.X. Liu, S.\n  L\\\"ok\\\"os, D. Lynch, T. Majoros, Y.I. Makdisi, M. Makek, A. Manion, V.I.\n  Manko, E. Mannel, M. McCumber, P.L. McGaughey, D. McGlinchey, C. McKinney, A.\n  Meles, M. Mendoza, B. Meredith, W.J. Metzger, Y. Miake, A.C. Mignerey, A.J.\n  Miller, A. Milov, D.K. Mishra, J.T. Mitchell, Iu. Mitrankov, S. Miyasaka, S.\n  Mizuno, P. Montuenga, T. Moon, D.P. Morrison, S.I. Morrow, T.V. Moukhanova,\n  B. Mulilo, T. Murakami, J. Murata, A. Mwai, S. Nagamiya, J.L. Nagle, M.I.\n  Nagy, I. Nakagawa, H. Nakagomi, K. Nakano, C. Nattrass, S. Nelson, P.K.\n  Netrakanti, M. Nihashi, T. Niida, R. Nouicer, N. Novitzky, A.S. Nyanin, E.\n  O'Brien, C.A. Ogilvie, J.D. Orjuela Koop, J.D. Osborn, A. Oskarsson, K.\n  Ozawa, R. Pak, V. Pantuev, V. Papavassiliou, S. Park, S.F. Pate, L. Patel, M.\n  Patel, J.-C. Peng, W. Peng, D.V. Perepelitsa, G.D.N. Perera, D.Yu.\n  Peressounko, C.E. PerezLara, J. Perry, R. Petti, C. Pinkenburg, R. Pinson,\n  R.P. Pisani, A. Pun, M.L. Purschke, P.V. Radzevich, J. Rak, N.\n  Ramasubramanian, I. Ravinovich, K.F. Read, D. Reynolds, V. Riabov, Y. Riabov,\n  D. Richford, T. Rinn, N. Riveli, D. Roach, S.D. Rolnick, M. Rosati, Z. Rowan,\n  J.G. Rubin, J. Runchey, N. Saito, T. Sakaguchi, H. Sako, V. Samsonov, M.\n  Sarsour, S. Sato, S. Sawada, B. Schaefer, B.K. Schmoll, K. Sedgwick, J.\n  Seele, R. Seidl, A. Sen, R. Seto, P. Sett, A. Sexton, D. Sharma, I. Shein,\n  T.-A. Shibata, K. Shigaki, M. Shimomura, P. Shukla, A. Sickles, C.L. Silva,\n  D. Silvermyr, B.K. Singh, C.P. Singh, V. Singh, M. Slune\\v{c}ka, K.L. Smith,\n  R.A. Soltz, W.E. Sondheim, S.P. Sorensen, I.V. Sourikova, P.W. Stankus, M.\n  Stepanov, S.P. Stoll, T. Sugitate, A. Sukhanov, T. Sumita, J. Sun, X. Sun, Z.\n  Sun, J. Sziklai, A. Takahara, A. Taketani, K. Tanida, M.J. Tannenbaum, S.\n  Tarafdar, A. Taranenko, A. Timilsina, T. Todoroki, M. Tom\\'a\\v{s}ek, H.\n  Torii, M. Towell, R. Towell, R.S. Towell, I. Tserruya, Y. Ueda, B. Ujvari,\n  H.W. van Hecke, M. Vargyas, J. Velkovska, M. Virius, V. Vrba, E. Vznuzdaev,\n  X.R. Wang, D. Watanabe, Y. Watanabe, Y.S. Watanabe, F. Wei, S. Whitaker, S.\n  Wolin, C.P. Wong, C.L. Woody, Y. Wu, M. Wysocki, B. Xia, Q. Xu, L. Xue, S.\n  Yalcin, Y.L. Yamaguchi, A. Yanovich, I. Yoon, I. Younus, I.E. Yushmanov, W.A.\n  Zajc, A. Zelenski, Y. Zhai, S. Zharko, L. Zou", "docs_id": "2005.14276", "section": ["hep-ex", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Production of $b\\bar{b}$ at forward rapidity in $p$+$p$ collisions at\n  $\\sqrt{s}=510$ GeV. The cross section of bottom quark-antiquark ($b\\bar{b}$) production in $p$+$p$ collisions at $\\sqrt{s}=510$ GeV is measured with the PHENIX detector at the Relativistic Heavy Ion Collider. The results are based on the yield of high mass, like-sign muon pairs measured within the PHENIX muon arm acceptance ($1.2<|y|<2.2$). The $b\\bar{b}$ signal is extracted from like-sign dimuons by utilizing the unique properties of neutral $B$ meson oscillation. We report a differential cross section of $d\\sigma_{b\\bar{b}\\rightarrow \\mu^\\pm\\mu^\\pm}/dy = 0.16 \\pm 0.01~(\\mbox{stat}) \\pm 0.02~(\\mbox{syst}) \\pm 0.02~(\\mbox{global})$ nb for like-sign muons in the rapidity and $p_T$ ranges $1.2<|y|<2.2$ and $p_T>1$ GeV/$c$, and dimuon mass of 5--10 GeV/$c^2$. The extrapolated total cross section at this energy for $b\\bar{b}$ production is $13.1 \\pm 0.6~(\\mbox{stat}) \\pm 1.5~(\\mbox{syst}) \\pm 2.7~(\\mbox{global})~\\mu$b. The total cross section is compared to a perturbative quantum chromodynamics calculation and is consistent within uncertainties. The azimuthal opening angle between muon pairs from $b\\bar{b}$ decays and their $p_T$ distributions are compared to distributions generated using {\\sc ps pythia 6}, which includes next-to-leading order processes. The azimuthal correlations and pair $p_T$ distribution are not very well described by {\\sc pythia} calculations, but are still consistent within uncertainties. Flavor creation and flavor excitation subprocesses are favored over gluon splitting."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: The authors of the paper report a differential cross section of $d\\sigma_{b\\bar{b}\\rightarrow \\mu^\\pm\\mu^\\pm}/dy$ for like-sign muons in the rapidity and $p_T$ ranges $1.2<|y|<2.2$ and $p_T>1$ GeV/$c$, and dimuon mass of 5--10 GeV/$c^2$. What is the main conclusion about the flavor creation and flavor excitation subprocesses in $b\\bar{b}$ production, based on the comparison with {\\sc pythia} calculations?\n\nA){The flavor creation and flavor excitation subprocesses are not favored over gluon splitting.}\nB){The flavor creation and flavor excitation subprocesses are favored over gluon splitting.}\nC){The flavor creation and flavor excitation subprocesses are not well described by {\\sc pythia} calculations.}\nD){The flavor creation and flavor excitation subprocesses are consistent with {\\sc pythia} calculations.}\n\nCorrect Answer: B){The flavor creation and flavor excitation subprocesses are favored over gluon splitting.}\n\nExplanation: The question requires the test-taker to understand the main conclusion drawn from the comparison between the experimental results and {\\sc pythia} calculations. The correct answer is based on the statement in the paper that \"Flavor creation and flavor excitation subprocesses are favored over gluon splitting..\". This indicates that the experimental results are consistent with the idea that flavor creation and flavor excitation subprocesses are more important than gluon splitting in $b\\bar{b}$ production."}, "16": {"documentation": {"title": "Beta decay of the very neutron-deficient $^{60}$Ge and $^{62}$Ge nuclei", "source": "S.E.A. Orrigo, B. Rubio, W. Gelletly, P. Aguilera, A. Algora, A.I.\n  Morales, J. Agramunt, D.S. Ahn, P. Ascher, B. Blank, C. Borcea, A. Boso, R.B.\n  Cakirli, J. Chiba, G. de Angelis, G. de France, F. Diel, P. Doornenbal, Y.\n  Fujita, N. Fukuda, E. Ganio\\u{g}lu, M. Gerbaux, J. Giovinazzo, S. Go, T.\n  Goigoux, S. Gr\\'evy, V. Guadilla, N. Inabe, G. Kiss, T. Kubo, S. Kubono, T.\n  Kurtukian-Nieto, D. Lubos, C. Magron, F. Molina, A. Montaner-Piz\\'a, D.\n  Napoli, D. Nishimura, S. Nishimura, H. Oikawa, Y. Shimizu, C. Sidong, P.-A.\n  S\\\"oderstr\\\"om, T. Sumikama, H. Suzuki, H. Takeda, Y. Takei, M. Tanaka, P.\n  Vi, J. Wu, S. Yagi", "docs_id": "2008.10576", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Beta decay of the very neutron-deficient $^{60}$Ge and $^{62}$Ge nuclei. We report here the results of a study of the $\\beta$ decay of the proton-rich Ge isotopes, $^{60}$Ge and $^{62}$Ge, produced in an experiment at the RIKEN Nishina Center. We have improved our knowledge of the half-lives of $^{62}$Ge (73.5(1) ms), $^{60}$Ge (25.0(3) ms) and its daughter nucleus, $^{60}$Ga (69.4(2) ms). We measured individual $\\beta$-delayed proton and $\\gamma$ emissions and their related branching ratios. Decay schemes and absolute Fermi and Gamow-Teller transition strengths have been determined. The mass excesses of the nuclei under study have been deduced. A total $\\beta$-delayed proton-emission branching ratio of 67(3)% has been obtained for $^{60}$Ge. New information has been obtained on the energy levels populated in $^{60}$Ga and on the 1/2$^-$ excited state in the $\\beta p$ daughter $^{59}$Zn. We extracted a ground state to ground state feeding of 85.3(3)% for the decay of $^{62}$Ge. Eight new $\\gamma$ lines have been added to the de-excitation of levels populated in the $^{62}$Ga daughter."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the energy levels of the $^{60}$Ga daughter nucleus from the given data?\n\nA) The energy levels of $^{60}$Ga are populated by a single $\\gamma$ transition.\nB) The 1/2$^-$ excited state in the $\\beta p$ daughter $^{59}$Zn is not populated in the decay of $^{60}$Ge.\nC) The ground state to ground state feeding ratio of the decay of $^{62}$Ge is not relevant to the energy levels of $^{60}$Ga.\nD) Eight new $\\gamma$ lines have been added to the de-excitation of levels populated in $^{60}$Ga, indicating a complex energy level structure.\n\nCorrect Answer: D) Eight new $\\gamma$ lines have been added to the de-excitation of levels populated in $^{60}$Ga, indicating a complex energy level structure.\n\nExplanation: The correct answer can be inferred from the statement \"Eight new $\\gamma$ lines have been added to the de-excitation of levels populated in the $^{62}$Ga daughter.\" This suggests that the energy levels of $^{60}$Ga are complex and have multiple $\\gamma$ transitions, which is consistent with option D. Options A and B are incorrect because the text does not provide information about the number of $\\gamma$ transitions or the population of the 1/2$^-$ excited state in $^{60}$Ga. Option C is also incorrect because the ground state to ground state feeding ratio of the decay of $^{62}$Ge is relevant to the energy levels of $^{62}$Ge, not $^{60}$Ga."}, "17": {"documentation": {"title": "Complexity without chaos: Plasticity within random recurrent networks\n  generates robust timing and motor control", "source": "Rodrigo Laje and Dean V. Buonomano", "docs_id": "1210.2104", "section": ["nlin.CD", "cond-mat.dis-nn", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Complexity without chaos: Plasticity within random recurrent networks\n  generates robust timing and motor control. It is widely accepted that the complex dynamics characteristic of recurrent neural circuits contributes in a fundamental manner to brain function. Progress has been slow in understanding and exploiting the computational power of recurrent dynamics for two main reasons: nonlinear recurrent networks often exhibit chaotic behavior and most known learning rules do not work in robust fashion in recurrent networks. Here we address both these problems by demonstrating how random recurrent networks (RRN) that initially exhibit chaotic dynamics can be tuned through a supervised learning rule to generate locally stable neural patterns of activity that are both complex and robust to noise. The outcome is a novel neural network regime that exhibits both transiently stable and chaotic trajectories. We further show that the recurrent learning rule dramatically increases the ability of RRNs to generate complex spatiotemporal motor patterns, and accounts for recent experimental data showing a decrease in neural variability in response to stimulus onset."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary challenge in understanding and exploiting the computational power of recurrent dynamics in neural circuits, and how do the authors of the paper address this challenge?\n\nA) Nonlinear recurrent networks often exhibit chaotic behavior, making it difficult to understand their dynamics, and most known learning rules are not effective in recurrent networks.\nB) Recurrent neural circuits are too simple to generate complex dynamics, and most learning rules are not robust enough to handle the complexity of these dynamics.\nC) The authors of the paper argue that the primary challenge is the lack of effective learning rules for recurrent networks, and that their proposed supervised learning rule can address this challenge.\nD) The authors of the paper claim that the primary challenge is the non-linearity of recurrent networks, and that their proposed random recurrent networks (RRN) can overcome this challenge through their ability to generate locally stable neural patterns of activity.\n\nCorrect Answer: A) Nonlinear recurrent networks often exhibit chaotic behavior, making it difficult to understand their dynamics, and most known learning rules are not effective in recurrent networks.\n\nExplanation: The question requires the test-taker to understand the main challenge in understanding and exploiting the computational power of recurrent dynamics in neural circuits, as stated in the introduction of the paper. The correct answer, A, highlights the two main reasons mentioned in the introduction: nonlinear recurrent networks often exhibit chaotic behavior, and most known learning rules do not work in robust fashion in recurrent networks. The other options are incorrect because they either oversimplify the challenge or misrepresent the authors' argument."}, "18": {"documentation": {"title": "Inertial Sensor Aided mmWave Beam Tracking to Support Cooperative\n  Autonomous Driving", "source": "Mattia Brambilla, Monica Nicoli, Sergio Savaresi, Umberto Spagnolini", "docs_id": "1903.11849", "section": ["eess.SP", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inertial Sensor Aided mmWave Beam Tracking to Support Cooperative\n  Autonomous Driving. This paper presents an inertial sensor aided technique for beam alignment and tracking in massive multiple-input multiple-output (MIMO) vehicle-to-vehicle (V2V) communications based on millimeter waves (mmWave). Since directional communications in vehicular scenarios are severely hindered by beam pointing issues, a beam alignment procedure has to be periodically carried out to guarantee the communication reliability. When dealing with massive MIMO links, the beam sweeping approach is known to be time consuming and often unfeasible due to latency constraints. To speed up the process, we propose a method that exploits a-priori information on array dynamics provided by an inertial sensor on transceivers to assist the beam alignment procedure. The proposed inertial sensor aided technique allows a continuous tracking of the beam while transmitting, avoiding frequent realignment phases. Numerical results based on real measurements of on-transceiver accelerometers demonstrate a significant gain in terms of V2V communication throughput with respect to conventional beam alignment protocols."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of the proposed inertial sensor aided technique for beam alignment and tracking in massive MIMO V2V communications?\n\nA) To reduce the computational complexity of beam alignment algorithms\nB) To improve the accuracy of beam alignment in the presence of high-speed vehicle movements\nC) To speed up the beam alignment process and reduce latency constraints in vehicular scenarios\nD) To enhance the security of V2V communications by introducing additional authentication mechanisms\n\n**Correct Answer:** C) To speed up the beam alignment process and reduce latency constraints in vehicular scenarios\n\n**Explanation:** The question requires the test-taker to understand the context and motivation behind the proposed technique. The correct answer, C, is supported by the text, which states that the beam sweeping approach is \"time consuming and often unfeasible due to latency constraints\". The other options are incorrect because they do not accurately reflect the primary motivation behind the proposed technique. Option A is incorrect because the technique is not primarily designed to reduce computational complexity. Option B is incorrect because the technique is not specifically designed to address the challenges of high-speed vehicle movements. Option D is incorrect because the technique is not related to security mechanisms."}, "19": {"documentation": {"title": "Empirical non-parametric estimation of the Fisher Information", "source": "Visar Berisha and Alfred O. Hero", "docs_id": "1408.1182", "section": ["stat.CO", "cs.IT", "math.IT", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Empirical non-parametric estimation of the Fisher Information. The Fisher information matrix (FIM) is a foundational concept in statistical signal processing. The FIM depends on the probability distribution, assumed to belong to a smooth parametric family. Traditional approaches to estimating the FIM require estimating the probability distribution function (PDF), or its parameters, along with its gradient or Hessian. However, in many practical situations the PDF of the data is not known but the statistician has access to an observation sample for any parameter value. Here we propose a method of estimating the FIM directly from sampled data that does not require knowledge of the underlying PDF. The method is based on non-parametric estimation of an $f$-divergence over a local neighborhood of the parameter space and a relation between curvature of the $f$-divergence and the FIM. Thus we obtain an empirical estimator of the FIM that does not require density estimation and is asymptotically consistent. We empirically evaluate the validity of our approach using two experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Empirical non-parametric estimation of the Fisher Information: A statistical signal processing concept.\n\n**A)** The Fisher Information Matrix (FIM) is a measure of the amount of information that a statistical model has about a parameter, and it is typically estimated using the gradient or Hessian of the log-likelihood function.\n\n**B)** The proposed method for estimating the FIM directly from sampled data does not require knowledge of the underlying probability distribution function (PDF), but rather uses a non-parametric estimation of an $f$-divergence over a local neighborhood of the parameter space.\n\n**C)** The FIM is a fundamental concept in statistical signal processing, and its estimation is crucial for many applications, including parameter estimation and hypothesis testing.\n\n**D)** The empirical estimator of the FIM proposed in the paper is asymptotically consistent, meaning that it converges to the true FIM as the sample size increases.\n\n**Correct Answer:** B) The proposed method for estimating the FIM directly from sampled data does not require knowledge of the underlying probability distribution function (PDF), but rather uses a non-parametric estimation of an $f$-divergence over a local neighborhood of the parameter space.\n\n**Explanation:** The correct answer is B) because the paper proposes a method for estimating the FIM directly from sampled data without requiring knowledge of the underlying PDF. This is achieved through the use of non-parametric estimation of an $f$-divergence over a local neighborhood of the parameter space. The other options are incorrect because they either describe traditional approaches to estimating the FIM (A), provide a general statement about the FIM (C), or describe a property of the empirical estimator (D), but do not accurately capture the main contribution of the paper."}, "20": {"documentation": {"title": "Inclusive, prompt and non-prompt J/$\\psi$ production at mid-rapidity in\n  Pb-Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 2.76 TeV", "source": "ALICE Collaboration", "docs_id": "1504.07151", "section": ["nucl-ex", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inclusive, prompt and non-prompt J/$\\psi$ production at mid-rapidity in\n  Pb-Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 2.76 TeV. The transverse momentum ($p_{\\rm T}$) dependence of the nuclear modification factor $R_{\\rm AA}$ and the centrality dependence of the average transverse momentum $\\langle p_{\\rm T}\\rangle$ for inclusive J/$\\psi$ have been measured with ALICE for Pb-Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 2.76 TeV in the e$^+$e$^-$ decay channel at mid-rapidity ($|y|<0.8$). The $\\langle p_{\\rm T}\\rangle$ is significantly smaller than the one observed for pp collisions at the same centre-of-mass energy. Consistently, an increase of $R_{\\rm AA}$ is observed towards low $p_{\\rm T}$. These observations might be indicative of a sizable contribution of charm quark coalescence to the J/$\\psi$ production. Additionally, the fraction of non-prompt J/$\\psi$ from beauty hadron decays, $f_{\\rm B}$, has been determined in the region $1.5 < p_{\\rm T} < 10$ GeV/c in three centrality intervals. No significant centrality dependence of $f_{\\rm B}$ is observed. Finally, the $R_{\\rm AA}$ of non-prompt J/$\\psi$ is discussed and compared with model predictions. The nuclear modification in the region $4.5 < p_{\\rm T} < 10$ GeV/c is found to be stronger than predicted by most models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the results of the ALICE experiment on inclusive J/$\\psi$ production in Pb-Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 2.76 TeV. What can be inferred about the contribution of charm quark coalescence to J/$\\psi$ production, and how does it compare to the results observed in pp collisions?\n\nA) The results indicate that charm quark coalescence is negligible, and the J/$\\psi$ production is primarily due to open charm decays. This is consistent with the smaller $\\langle p_{\\rm T}\\rangle$ observed in Pb-Pb collisions compared to pp collisions.\n\nB) The data suggest that charm quark coalescence is a significant contributor to J/$\\psi$ production, and the nuclear modification factor $R_{\\rm AA}$ is stronger at low $p_{\\rm T}$, indicating a larger contribution from charm quark coalescence.\n\nC) The results indicate that the fraction of non-prompt J/$\\psi$ from beauty hadron decays, $f_{\\rm B}$, is independent of centrality, and the nuclear modification in the region $4.5 < p_{\\rm T} < 10$ GeV/c is stronger than predicted by most models.\n\nD) The data suggest that the average transverse momentum $\\langle p_{\\rm T}\\rangle$ for inclusive J/$\\psi$ is significantly smaller than in pp collisions, and the nuclear modification factor $R_{\\rm AA}$ is stronger at low $p_{\\rm T}$, indicating a larger contribution from charm quark coalescence.\n\nCorrect Answer: B) The data suggest that charm quark coalescence is a significant contributor to J/$\\psi$ production, and the nuclear modification factor $R_{\\rm AA}$ is stronger at low $p_{\\rm T}$, indicating a larger contribution from charm quark coalescence.\n\nExplanation: The correct answer is B) because the data show that the $\\langle p_{\\rm T}\\rangle$ for inclusive J/$\\psi$ is significantly smaller than in pp collisions, and the nuclear modification factor $R_{\\rm AA}$ is stronger at low $p_{\\rm T}$. This suggests that charm quark coalescence is a significant contributor to J/$\\psi$ production, as it would lead to a smaller $\\langle p_{\\rm T}\\rangle$ and a stronger nuclear modification at low $p_{\\rm T}$."}, "21": {"documentation": {"title": "Convective Instabilities of Bunched Beams with Space Charge", "source": "Alexey Burov", "docs_id": "1807.04887", "section": ["physics.acc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Convective Instabilities of Bunched Beams with Space Charge. For a single hadron bunch in a circular accelerator at zero chromaticity, without multi-turn wakes and without electron clouds and other beams, only one transverse collective instability is possible, the mode-coupling instability, or TMCI. For sufficiently strong space charge (SC), the instability threshold of the wake-driven coherent tune shift normally increases linearly with the SC tune shift, as independently concluded by several authors using different methods. This stability condition has, however, a very strange feature: at strong SC, it is totally insensitive to the number of particles. Thus, were it correct, such a beam with sufficiently strong SC, being stable at some intensity, would remain stable at higher intensity, regardless of how much higher! This paper suggests a resolution of this conundrum: while SC suppresses TMCI, it introduces head-to-tail convective amplifications, which could make the beam even less stable than without SC, even if all the coherent tunes are real, i.e. all the modes are stable in the conventional {\\it absolute} meaning of the word. This is done using an effective new method of analysis of the beam's transverse spectrum for arbitrary space charge and wake fields. Two new types of beam instabilities are introduced: the {\\it saturating convective instability}, SCI, and the {\\it absolute-convective instability}, ACI."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the stability conditions of a single hadron bunch in a circular accelerator at zero chromaticity, without multi-turn wakes and without electron clouds and other beams. What is the resolution to the apparent paradox that the instability threshold of the wake-driven coherent tune shift increases linearly with the space charge tune shift, yet remains insensitive to the number of particles?\n\n**A)** The resolution lies in the fact that the space charge tune shift is a measure of the beam's transverse spectrum, and the instability threshold is determined by the effective new method of analysis of the beam's transverse spectrum for arbitrary space charge and wake fields.\n\n**B)** The resolution lies in the fact that the saturating convective instability (SCI) and absolute-convective instability (ACI) are two new types of beam instabilities that are introduced by the effective new method of analysis, which can make the beam even less stable than without space charge.\n\n**C)** The resolution lies in the fact that the instability threshold of the wake-driven coherent tune shift is actually independent of the number of particles, and the apparent paradox is resolved by considering the head-to-tail convective amplifications introduced by the space charge.\n\n**D)** The resolution lies in the fact that the beam's stability is determined by the conventional absolute meaning of the word, and the apparent paradox is resolved by considering the real coherence of the beam's modes.\n\n**Correct Answer:** B) The resolution lies in the fact that the saturating convective instability (SCI) and absolute-convective instability (ACI) are two new types of beam instabilities that are introduced by the effective new method of analysis, which can make the beam even less stable than without space charge.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the resolution to the paradox presented in the documentation. The saturating convective instability (SCI) and absolute-convective instability (ACI) are two new types of beam instabilities that are introduced by the effective new method of analysis, which can make the beam even less stable than without space charge. This resolution is a key finding of the paper and is essential to understanding the stability conditions of a single hadron bunch in a circular accelerator."}, "22": {"documentation": {"title": "The Heuristic Dynamic Programming Approach in Boost Converters", "source": "Sepehr Saadatmand, Pourya Shamsi, and Mehdi Ferdowsi", "docs_id": "2002.00822", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Heuristic Dynamic Programming Approach in Boost Converters. In this study, a heuristic dynamic programming controller is proposed to control a boost converter. Conventional controllers such as proportional integral-derivative (PID) or proportional integral (PI) are designed based on the linearized small-signal model near the operating point. Therefore, the performance of the controller during the start-up, the load change, or the input voltage variation is not optimal since the system model changes by varying the operating point. The heuristic dynamic programming controller optimally controls the boost converter by following the approximate dynamic programming. The advantage of the HDP is that the neural network based characteristic of the proposed controller enables boost converters to easily cope with large disturbances. An HDP with a well trained critic and action networks can perform as an optimal controller for the boost converter. To compare the effectiveness of the traditional PI-based and the HDP boost converter, the simulation results are provided."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the Heuristic Dynamic Programming (HDP) approach in controlling a boost converter, as compared to traditional controllers such as PID or PI?\n\n**A)** HDP can only handle small disturbances and is not suitable for large disturbances.\n**B)** HDP can easily cope with large disturbances due to its neural network-based characteristic.\n**C)** HDP requires a more complex control structure compared to traditional controllers.\n**D)** HDP is only suitable for boost converters with a specific operating point.\n\n**Correct Answer:** B) HDP can easily cope with large disturbances due to its neural network-based characteristic.\n\n**Explanation:** The HDP approach is proposed as a solution to the limitations of traditional controllers, which are designed based on the linearized small-signal model near the operating point. The HDP approach, on the other hand, uses a neural network-based characteristic, which enables it to handle large disturbances and perform optimally. This is the primary advantage of the HDP approach, as stated in the documentation."}, "23": {"documentation": {"title": "Sensitivity to a possible variation of the Proton-to-Electron Mass Ratio\n  of Torsion-Wagging-Rotation Transitions in Methylamine (CH3NH2)", "source": "Vadim V. Ilyushin and Paul Jansen and Mikhail G. Kozlov and Sergei A.\n  Levshakov and Isabelle Kleiner and Wim Ubachs and Hendrick L. Bethlem", "docs_id": "1201.2090", "section": ["physics.chem-ph", "astro-ph.CO", "physics.atom-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sensitivity to a possible variation of the Proton-to-Electron Mass Ratio\n  of Torsion-Wagging-Rotation Transitions in Methylamine (CH3NH2). We determine the sensitivity to a possible variation of the proton-to-electron mass ratio \\mu for torsion-wagging-rotation transitions in the ground state of methylamine (CH3NH2). Our calculation uses an effective Hamiltonian based on a high-barrier tunneling formalism combined with extended-group ideas. The \\mu-dependence of the molecular parameters that are used in this model are derived and the most important ones of these are validated using the spectroscopic data of different isotopologues of methylamine. We find a significant enhancement of the sensitivity coefficients due to energy cancellations between internal rotational, overall rotational and inversion energy splittings. The sensitivity coefficients of the different transitions range from -19 to +24. The sensitivity coefficients of the 78.135, 79.008, and 89.956 GHz transitions that were recently observed in the disk of a z = 0.89 spiral galaxy located in front of the quasar PKS 1830-211 [S. Muller et al. Astron. Astrophys. 535, A103 (2011)] were calculated to be -0.87 for the first two and -1.4 for the third transition, respectively. From these transitions a preliminary upper limit for a variation of the proton to electron mass ratio of \\Delta \\mu/\\mu< 9 x 10^{-6} is deduced."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: A study on the sensitivity of torsion-wagging-rotation transitions in methylamine (CH3NH2) to a possible variation of the proton-to-electron mass ratio \u03bc has been conducted. The researchers used an effective Hamiltonian based on a high-barrier tunneling formalism combined with extended-group ideas. They found that the sensitivity coefficients of the different transitions range from -19 to +24.\n\nWhich of the following statements accurately describes the main finding of this study?\n\nA) The sensitivity coefficients of the transitions are independent of the proton-to-electron mass ratio \u03bc.\nB) The sensitivity coefficients of the transitions are significantly enhanced due to energy cancellations between internal rotational, overall rotational, and inversion energy splittings.\nC) The sensitivity coefficients of the transitions are only dependent on the vibrational frequency of the methylamine molecule.\nD) The sensitivity coefficients of the transitions are not affected by the presence of the quasar PKS 1830-211.\n\nCorrect Answer: B) The sensitivity coefficients of the transitions are significantly enhanced due to energy cancellations between internal rotational, overall rotational, and inversion energy splittings.\n\nExplanation: The correct answer is B because the study found that the sensitivity coefficients of the transitions are significantly enhanced due to energy cancellations between internal rotational, overall rotational, and inversion energy splittings. This is stated in the abstract of the study: \"We find a significant enhancement of the sensitivity coefficients due to energy cancellations between internal rotational, overall rotational and inversion energy splittings.\" The other options are incorrect because they do not accurately describe the main finding of the study. Option A is incorrect because the sensitivity coefficients are not independent of the proton-to-electron mass ratio \u03bc. Option C is incorrect because the sensitivity coefficients are not only dependent on the vibrational frequency of the methylamine molecule. Option D is incorrect because the presence of the quasar PKS 1830-211 is not mentioned as affecting the sensitivity coefficients."}, "24": {"documentation": {"title": "Non-coboundary Poisson-Lie structures on the book group", "source": "Angel Ballesteros, Alfonso Blasco, Fabio Musso", "docs_id": "1112.2623", "section": ["math-ph", "math.MP", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-coboundary Poisson-Lie structures on the book group. All possible Poisson-Lie (PL) structures on the 3D real Lie group generated by a dilation and two commuting translations are obtained. Its classification is fully performed by relating these PL groups with the corresponding Lie bialgebra structures on the corresponding \"book\" Lie algebra. By construction, all these Poisson structures are quadratic Poisson-Hopf algebras for which the group multiplication is a Poisson map. In contrast to the case of simple Lie groups, it turns out that most of the PL structures on the book group are non-coboundary ones. Moreover, from the viewpoint of Poisson dynamics, the most interesting PL book structures are just some of these non-coboundaries, which are explicitly analysed. In particular, we show that the two different q-deformed Poisson versions of the sl(2,R) algebra appear as two distinguished cases in this classification, as well as the quadratic Poisson structure that underlies the integrability of a large class of 3D Lotka-Volterra equations. Finally, the quantization problem for these PL groups is sketched."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Classify the Poisson-Lie structures on the 3D real Lie group generated by a dilation and two commuting translations, and explain the significance of the non-coboundary Poisson-Lie structures in the context of Poisson dynamics.\n\nA) The Poisson-Lie structures on the book group can be classified into two types: coboundary and non-coboundary. The non-coboundary structures are of particular interest, as they are related to the q-deformed Poisson versions of the sl(2,R) algebra and the integrability of 3D Lotka-Volterra equations.\n\nB) The classification of Poisson-Lie structures on the book group is performed by relating these structures to the corresponding Lie bialgebra structures on the \"book\" Lie algebra. The non-coboundary Poisson-Lie structures are of interest in the context of Poisson dynamics, as they are related to the q-deformed Poisson versions of the sl(2,R) algebra.\n\nC) The Poisson-Lie structures on the book group are quadratic Poisson-Hopf algebras, and the group multiplication is a Poisson map. The non-coboundary Poisson-Lie structures are of particular interest, as they are related to the integrability of 3D Lotka-Volterra equations and the q-deformed Poisson versions of the sl(2,R) algebra.\n\nD) The classification of Poisson-Lie structures on the book group is performed by relating these structures to the corresponding Lie bialgebra structures on the \"book\" Lie algebra. The non-coboundary Poisson-Lie structures are of interest in the context of Poisson dynamics, as they are related to the q-deformed Poisson versions of the sl(2,R) algebra and the integrability of 3D Lotka-Volterra equations.\n\nCorrect Answer: B) The classification of Poisson-Lie structures on the book group is performed by relating these structures to the corresponding Lie bialgebra structures on the \"book\" Lie algebra. The non-coboundary Poisson-Lie structures are of interest in the context of Poisson dynamics, as they are related to the q-deformed Poisson versions of the sl(2,R) algebra.\n\nExplanation: The correct answer is B) because it accurately summarizes the main point of the documentation, which is that the classification of Poisson-Lie structures on the book group is performed by relating these structures to the corresponding Lie bialgebra structures on the \"book\" Lie algebra. Additionally, the correct answer highlights the significance of non-coboundary Poisson-Lie structures in the context of Poisson dynamics, which is a key aspect of the documentation."}, "25": {"documentation": {"title": "Monodromy Transform Approach to Solution of Some Field Equations in\n  General Relativity and String Theory", "source": "G. A. Alekseev", "docs_id": "gr-qc/9911045", "section": ["gr-qc", "hep-th", "nlin.SI", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Monodromy Transform Approach to Solution of Some Field Equations in\n  General Relativity and String Theory. A monodromy transform approach, presented in this communication, provides a general base for solution of space-time symmetry reductions of Einstein equations in all known integrable cases, which include vacuum, electrovacuum, massless Weyl spinor field and stiff matter fluid, as well as some string theory induced gravity models. It was found a special finite set of functional parameters, defined as the monodromy data for the fundamental solution of associated spectral problem. Similarly to the scattering data in the inverse scattering transform, these monodromy data can be used for characterization of any local solution of the field equations. A \"direct\" and \"inverse\" problems of such monodromy transform admit unambiguous solutions. For the linear singular integral equation with a scalar (i.e. non-matrix) kernel, which solves the inverse problem of this monodromy transform, an equivalent regularization -- a Fredholm linear integral equation of the second kind is constrcuted in several convenient forms. For arbitrary choice of the monodromy data a simple iterative method leads to an effective construction of the solution in terms of homogeneously convergent functional series."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a monodromy transform approach to solve the Einstein equations in general relativity and string theory. What is the primary advantage of using this approach, and how does it relate to the characterization of local solutions of the field equations?\n\nA) The monodromy transform approach provides a direct solution to the Einstein equations, whereas the inverse problem allows for the characterization of local solutions.\nB) The monodromy transform approach is limited to vacuum, electrovacuum, and massless Weyl spinor field cases, but can be extended to stiff matter fluid and string theory induced gravity models.\nC) The monodromy transform approach admits unambiguous solutions for both the direct and inverse problems, and can be used to construct solutions in terms of homogeneously convergent functional series.\nD) The monodromy transform approach is equivalent to the inverse scattering transform, but requires a different regularization, such as a Fredholm linear integral equation of the second kind.\n\nCorrect Answer: C) The monodromy transform approach admits unambiguous solutions for both the direct and inverse problems, and can be used to construct solutions in terms of homogeneously convergent functional series.\n\nExplanation: The correct answer highlights the key advantage of the monodromy transform approach, which is its ability to provide unambiguous solutions for both the direct and inverse problems. This allows for the construction of solutions in terms of homogeneously convergent functional series, making it a powerful tool for solving the Einstein equations in general relativity and string theory. The other options are incorrect because they either limit the scope of the approach (B), misrepresent the relationship between the direct and inverse problems (A), or incorrectly describe the equivalence to the inverse scattering transform (D)."}, "26": {"documentation": {"title": "Proportionate vs disproportionate distribution of wealth of two\n  individuals in a tempered Paretian ensemble", "source": "G. Oshanin, Yu. Holovatch and G. Schehr", "docs_id": "1106.4710", "section": ["q-fin.GN", "math.PR", "math.ST", "physics.data-an", "q-fin.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Proportionate vs disproportionate distribution of wealth of two\n  individuals in a tempered Paretian ensemble. We study the distribution P(\\omega) of the random variable \\omega = x_1/(x_1 + x_2), where x_1 and x_2 are the wealths of two individuals selected at random from the same tempered Paretian ensemble characterized by the distribution \\Psi(x) \\sim \\phi(x)/x^{1 + \\alpha}, where \\alpha > 0 is the Pareto index and $\\phi(x)$ is the cut-off function. We consider two forms of \\phi(x): a bounded function \\phi(x) = 1 for L \\leq x \\leq H, and zero otherwise, and a smooth exponential function \\phi(x) = \\exp(-L/x - x/H). In both cases \\Psi(x) has moments of arbitrary order. We show that, for \\alpha > 1, P(\\omega) always has a unimodal form and is peaked at \\omega = 1/2, so that most probably x_1 \\approx x_2. For 0 < \\alpha < 1 we observe a more complicated behavior which depends on the value of \\delta = L/H. In particular, for \\delta < \\delta_c - a certain threshold value - P(\\omega) has a three-modal (for a bounded \\phi(x)) and a bimodal M-shape (for an exponential \\phi(x)) form which signifies that in such ensembles the wealths x_1 and x_2 are disproportionately different."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In a tempered Paretian ensemble, consider two individuals with wealths x1 and x2, where the distribution of their wealths is characterized by the probability density function \u03a8(x) = \u03c6(x)/x^(1 + \u03b1), with \u03b1 > 0 being the Pareto index and \u03c6(x) being a cut-off function. Analyze the distribution P(\u03c9) of the random variable \u03c9 = x1/(x1 + x2) for the two forms of \u03c6(x): (A) a bounded function \u03c6(x) = 1 for L \u2264 x \u2264 H, and zero otherwise, and (B) a smooth exponential function \u03c6(x) = exp(-L/x - x/H). How does the distribution P(\u03c9) behave for different values of \u03b1 and \u03c6(x), and what are the implications for the distribution of wealths x1 and x2?\n\n**A)** For \u03b1 > 1, P(\u03c9) is unimodal and peaked at \u03c9 = 1/2, indicating that most probably x1 \u2248 x2. For 0 < \u03b1 < 1, the distribution P(\u03c9) exhibits a more complicated behavior that depends on the value of \u03b4 = L/H, leading to three-modal or bimodal forms for certain values of \u03b4.\n\n**B)** The distribution P(\u03c9) is always bimodal and peaked at \u03c9 = 0 and \u03c9 = 1, indicating that most probably x1 and x2 are disproportionately different.\n\n**C)** For \u03b1 > 1, P(\u03c9) is unimodal and peaked at \u03c9 = 1/2, but for 0 < \u03b1 < 1, the distribution P(\u03c9) exhibits a three-modal form for bounded \u03c6(x) and a bimodal M-shape for exponential \u03c6(x).\n\n**D)** The distribution P(\u03c9) is always unimodal and peaked at \u03c9 = 1/2, regardless of the value of \u03b1 and \u03c6(x).\n\n**Correct Answer:** C) For \u03b1 > 1, P(\u03c9) is unimodal and peaked at \u03c9 = 1/2, but for 0 < \u03b1 < 1, the distribution P(\u03c9) exhibits a three-modal form for bounded \u03c6(x) and a bimodal M-shape for exponential \u03c6(x)."}, "27": {"documentation": {"title": "Shrinking the Sample Covariance Matrix using Convex Penalties on the\n  Matrix-Log Transformation", "source": "David E. Tyler and Mengxi Yi", "docs_id": "1903.08281", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Shrinking the Sample Covariance Matrix using Convex Penalties on the\n  Matrix-Log Transformation. For $q$-dimensional data, penalized versions of the sample covariance matrix are important when the sample size is small or modest relative to $q$. Since the negative log-likelihood under multivariate normal sampling is convex in $\\Sigma^{-1}$, the inverse of its covariance matrix, it is common to add to it a penalty which is also convex in $\\Sigma^{-1}$. More recently, Deng-Tsui (2013) and Yu et al.(2017) have proposed penalties which are functions of the eigenvalues of $\\Sigma$, and are convex in $\\log \\Sigma$, but not in $\\Sigma^{-1}$. The resulting penalized optimization problem is not convex in either $\\log \\Sigma$ or $\\Sigma^{-1}$. In this paper, we note that this optimization problem is geodesically convex in $\\Sigma$, which allows us to establish the existence and uniqueness of the corresponding penalized covariance matrices. More generally, we show the equivalence of convexity in $\\log \\Sigma$ and geodesic convexity for penalties on $\\Sigma$ which are strictly functions of their eigenvalues. In addition, when using such penalties, we show that the resulting optimization problem reduces to to a $q$-dimensional convex optimization problem on the eigenvalues of $\\Sigma$, which can then be readily solved via Newton-Raphson. Finally, we argue that it is better to apply these penalties to the shape matrix $\\Sigma/(\\det \\Sigma)^{1/q}$ rather than to $\\Sigma$ itself. A simulation study and an example illustrate the advantages of applying the penalty to the shape matrix."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a multivariate normal distribution with $q$-dimensional data, where the sample covariance matrix $\\Sigma$ is subject to a penalty that is a strictly increasing function of its eigenvalues. The penalty is added to the negative log-likelihood under multivariate normal sampling, which is convex in $\\Sigma^{-1}$. Show that the resulting optimization problem is geodesically convex in $\\Sigma$, and explain why this allows for the existence and uniqueness of the corresponding penalized covariance matrices.\n\n**A)** The penalty is a strictly increasing function of the trace of $\\Sigma$, and the negative log-likelihood is convex in $\\Sigma^{-1}$. Therefore, the resulting optimization problem is convex in $\\Sigma^{-1}$, and hence geodesically convex in $\\Sigma$.\n\n**B)** The penalty is a strictly increasing function of the determinant of $\\Sigma$, and the negative log-likelihood is convex in $\\Sigma^{-1}$. Therefore, the resulting optimization problem is convex in $\\Sigma^{-1}$, and hence geodesically convex in $\\Sigma$.\n\n**C)** The penalty is a strictly increasing function of the eigenvalues of $\\Sigma$, and the negative log-likelihood is convex in $\\Sigma^{-1}$. Since the penalty is strictly increasing in the eigenvalues, the resulting optimization problem is geodesically convex in $\\Sigma$, and hence the existence and uniqueness of the corresponding penalized covariance matrices can be established.\n\n**D)** The penalty is a strictly increasing function of the inverse of $\\Sigma$, and the negative log-likelihood is convex in $\\Sigma^{-1}$. Therefore, the resulting optimization problem is convex in $\\Sigma^{-1}$, and hence geodesically convex in $\\Sigma$.\n\n**Correct Answer:** C) The penalty is a strictly increasing function of the eigenvalues of $\\Sigma$, and the negative log-likelihood is convex in $\\Sigma^{-1}$. Since the penalty is strictly increasing in the eigenvalues, the resulting optimization problem is geodesically convex in $\\Sigma$, and hence the existence and uniqueness of the corresponding penalized covariance matrices can be established.\n\n**Explanation:** The correct answer is C) because the penalty is a strictly increasing function of the eigenvalues of $\\Sigma$, and the negative log-likelihood is convex in $\\Sigma^{-1}$. This allows for the existence and uniqueness of the corresponding penalized covariance matrices, as the optimization problem is geodesically convex in $\\Sigma$. The other options are incorrect because they do not accurately describe the relationship between the penalty and the negative log-likelihood. Option A is incorrect because the penalty is not strictly increasing in the trace of $\\Sigma$. Option B is incorrect because the penalty is not strictly increasing in the determinant of $\\Sigma$. Option D is incorrect because the penalty is not strictly increasing in the inverse of $\\Sigma$."}, "28": {"documentation": {"title": "Large number of receptors may reduce cellular response time variation", "source": "Xiang Cheng, Lina Merchan, Martin Tchernookov, Ilya Nemenman", "docs_id": "1212.1229", "section": ["q-bio.MN", "q-bio.CB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Large number of receptors may reduce cellular response time variation. Cells often have tens of thousands of receptors, even though only a few activated receptors can trigger full cellular responses. Reasons for the overabundance of receptors remain unclear. We suggest that, in certain conditions, the large number of receptors results in a competition among receptors to be the first to activate the cell. The competition decreases the variability of the time to cellular activation, and hence results in a more synchronous activation of cells. We argue that, in simple models, this variability reduction does not necessarily interfere with the receptor specificity to ligands achieved by the kinetic proofreading mechanism. Thus cells can be activated accurately in time and specifically to certain signals. We predict the minimum number of receptors needed to reduce the coefficient of variation for the time to activation following binding of a specific ligand. Further, we predict the maximum number of receptors so that the kinetic proofreading mechanism still can improve the specificity of the activation. These predictions fall in line with experimentally reported receptor numbers for multiple systems."}, "answer": "Here is the exam question:\n\nWhat is the primary reason for the large number of receptors on cells, according to the proposed mechanism, and how does it affect the variability of cellular response time?\n\nA) The large number of receptors allows for more precise control over cellular responses, reducing variability.\nB) The large number of receptors results in a competition among receptors to be the first to activate the cell, decreasing the variability of the time to cellular activation.\nC) The large number of receptors is a result of the kinetic proofreading mechanism, which improves receptor specificity to ligands.\nD) The large number of receptors is a byproduct of cellular evolution, with no specific functional significance.\n\nCorrect Answer: B) The large number of receptors results in a competition among receptors to be the first to activate the cell, decreasing the variability of the time to cellular activation.\n\nExplanation: The correct answer is B) because the text states that the large number of receptors results in a competition among receptors to be the first to activate the cell, which decreases the variability of the time to cellular activation. This is the primary reason for the large number of receptors, according to the proposed mechanism. The other options are incorrect because they either misrepresent the mechanism (A and C) or provide an unrelated explanation (D)."}, "29": {"documentation": {"title": "The Mittag-Leffler Fitting of the Phillips Curve", "source": "Tomas Skovranek", "docs_id": "1604.00369", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Mittag-Leffler Fitting of the Phillips Curve. In this paper, a mathematical model based on the one-parameter Mittag-Leffler function is proposed to be used for the first time to describe the relation between unemployment rate and inflation rate, also known as the Phillips curve. The Phillips curve is in the literature often represented by an exponential-like shape. On the other hand, Phillips in his fundamental paper used a power function in the model definition. Considering that the ordinary as well as generalised Mittag-Leffler function behaves between a purely exponential function and a power function it is natural to implement it in the definition of the model used to describe the relation between the data representing the Phillips curve. For the modelling purposes the data of two different European economies, France and Switzerland, were used and an \"out-of-sample\" forecast was done to compare the performance of the Mittag-Leffler model to the performance of the power-type and exponential-type model. The results demonstrate that the ability of the Mittag-Leffler function to fit data that manifest signs of stretched exponentials, oscillations or even damped oscillations can be of use when describing economic relations and phenomenons, such as the Phillips curve."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"The Mittag-Leffler Fitting of the Phillips Curve\" propose a mathematical model based on the one-parameter Mittag-Leffler function to describe the relation between unemployment rate and inflation rate. What is the primary motivation behind using the Mittag-Leffler function in this context, and how does it relate to the traditional exponential and power function representations of the Phillips curve?\n\n**A)** The Mittag-Leffler function is used to capture the non-linear relationships between economic variables, which are often better represented by a stretched exponential or oscillatory behavior.\n\n**B)** The Mittag-Leffler function is used to provide a more accurate fit to the data, as it can handle cases where the data exhibits damped oscillations or stretched exponentials.\n\n**C)** The Mittag-Leffler function is used to simplify the model, as it can be used to replace the traditional exponential and power functions in the Phillips curve model.\n\n**D)** The Mittag-Leffler function is used to introduce a new parameter that can be tuned to improve the fit of the model to the data.\n\n**Correct Answer:** B) The Mittag-Leffler function is used to provide a more accurate fit to the data, as it can handle cases where the data exhibits damped oscillations or stretched exponentials.\n\n**Explanation:** The correct answer is B) because the paper states that the Mittag-Leffler function can be used to fit data that manifest signs of stretched exponentials, oscillations, or even damped oscillations, which is exactly what the authors claim the Mittag-Leffler function can do. This is the primary motivation behind using the Mittag-Leffler function in this context. The other options are incorrect because they do not accurately reflect the motivation behind using the Mittag-Leffler function in the paper."}, "30": {"documentation": {"title": "The polarisation of remote work", "source": "Fabian Braesemann, Fabian Stephany, Ole Teutloff, Otto K\\\"assi, Mark\n  Graham, Vili Lehdonvirta", "docs_id": "2108.13356", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The polarisation of remote work. The Covid-19 pandemic has led to the rise of remote work with consequences for the global division of work. Remote work could connect labour markets, but it could also increase spatial polarisation. However, our understanding of the geographies of remote work is limited. Specifically, does remote work bring jobs to rural areas or is it concentrating in large cities, and how do skill requirements affect competition for jobs and wages? We use data from a fully remote labour market - an online labour platform - to show that remote work is polarised along three dimensions. First, countries are globally divided: North American, European, and South Asian remote workers attract most jobs, while many Global South countries participate only marginally. Secondly, remote jobs are pulled to urban regions; rural areas fall behind. Thirdly, remote work is polarised along the skill axis: workers with in-demand skills attract profitable jobs, while others face intense competition and obtain low wages. The findings suggest that remote work is shaped by agglomerative forces, which are deepening the gap between urban and rural areas. To make remote work an effective tool for rural development, it needs to be embedded in local skill-building and labour market programmes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the findings of the study on the polarisation of remote work, as presented in the Arxiv documentation. How do the results suggest that remote work is shaping the global division of work, and what implications do these findings have for rural development?\n\n**A)** The study suggests that remote work is polarising along the skill axis, with workers having in-demand skills attracting profitable jobs, while others face intense competition and obtain low wages. However, this polarisation is not necessarily a result of remote work itself, but rather a reflection of existing skill disparities.\n\n**B)** The findings indicate that remote work is concentrating in large cities, with rural areas falling behind, and that countries are globally divided, with North American, European, and South Asian remote workers attracting most jobs. This polarisation is exacerbated by agglomerative forces, which are deepening the gap between urban and rural areas.\n\n**C)** The study reveals that remote work is not bringing jobs to rural areas, but rather concentrating in urban regions, and that the skill requirements of remote jobs are driving competition for jobs and wages. This polarisation is a result of the online labour platform's ability to connect labour markets, but it also perpetuates existing inequalities.\n\n**D)** The results suggest that remote work is an effective tool for rural development, as it can connect rural workers with global job opportunities and provide them with access to in-demand skills. However, this requires embedding remote work in local skill-building and labour market programmes to address the existing skill disparities.\n\n**Correct Answer:** B) The findings indicate that remote work is concentrating in large cities, with rural areas falling behind, and that countries are globally divided, with North American, European, and South Asian remote workers attracting most jobs. This polarisation is exacerbated by agglomerative forces, which are deepening the gap between urban and rural areas.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the three dimensions of polarisation identified in the study: countries are globally divided, remote jobs are concentrated in urban regions, and remote work is polarised along the skill axis. The study also highlights the exacerbating effect of agglomerative forces on the gap between urban and rural areas."}, "31": {"documentation": {"title": "Semisupervised Clustering by Queries and Locally Encodable Source Coding", "source": "Arya Mazumdar, Soumyabrata Pal", "docs_id": "1904.00507", "section": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semisupervised Clustering by Queries and Locally Encodable Source Coding. Source coding is the canonical problem of data compression in information theory. In a locally encodable source coding, each compressed bit depends on only few bits of the input. In this paper, we show that a recently popular model of semi-supervised clustering is equivalent to locally encodable source coding. In this model, the task is to perform multiclass labeling of unlabeled elements. At the beginning, we can ask in parallel a set of simple queries to an oracle who provides (possibly erroneous) binary answers to the queries. The queries cannot involve more than two (or a fixed constant number of) elements. Now the labeling of all the elements (or clustering) must be performed based on the noisy query answers. The goal is to recover all the correct labelings while minimizing the number of such queries. The equivalence to locally encodable source codes leads us to find lower bounds on the number of queries required in a variety of scenarios. We provide querying schemes based on pairwise `same cluster' queries - and pairwise AND queries and show provable performance guarantees for each of the schemes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Semisupervised Clustering by Queries and Locally Encodable Source Coding, what is the primary goal of the task, and how does it relate to the concept of locally encodable source coding?\n\n**A)** The primary goal is to compress data into a smaller representation while minimizing the number of queries to an oracle, and locally encodable source coding is a method to achieve this goal.\n\n**B)** The primary goal is to perform multiclass labeling of unlabeled elements based on noisy query answers from an oracle, and locally encodable source coding is a model that is equivalent to this task.\n\n**C)** The primary goal is to recover all correct labelings while minimizing the number of queries, and locally encodable source coding is a method to achieve this goal by providing lower bounds on the number of queries required.\n\n**D)** The primary goal is to compress data into a smaller representation while minimizing the number of queries to an oracle, and locally encodable source coding is a method to achieve this goal by using pairwise AND queries.\n\n**Correct Answer:** B) The primary goal is to perform multiclass labeling of unlabeled elements based on noisy query answers from an oracle, and locally encodable source coding is a model that is equivalent to this task.\n\n**Explanation:** The correct answer is B) because the task described in the paper is to perform multiclass labeling of unlabeled elements based on noisy query answers from an oracle, and the equivalence to locally encodable source coding is established by showing that this task is equivalent to a locally encodable source coding problem. The other options are incorrect because they either misstate the primary goal of the task or incorrectly describe the relationship between the task and locally encodable source coding."}, "32": {"documentation": {"title": "Closed Form Analytical Model for Airflow around 2-Dimensional Composite\n  Airfoil Via Conformal Mapping", "source": "Rita Gitik and William B. Ribbens", "docs_id": "1712.09730", "section": ["physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Closed Form Analytical Model for Airflow around 2-Dimensional Composite\n  Airfoil Via Conformal Mapping. This paper presents a method of computing section lift characteristics for a 2-dimensional airfoil with a second 2-dimensional object at a position at or ahead of the leading edge of the airfoil. Since both objects are 2-dimensional, the analysis yields a closed form solution to calculation of the airflow over the airfoil and second object, using conformal mapping of analytically closed form airflow velocity vector past two circular shaped objects in initial complex plane, using a standard air flow model for each object individually. The combined airflow velocity vector is obtained by linear superposition of the velocity vector for the two objects, computed individually. The lift characteristics are obtained from the circulation around the airfoil and second object which is computed from the combined closed form velocity vector and the geometry along the contour integral for circulation. The illustrative example considered in this paper shows that the second object which is essentially a cylinder whose diameter is approximately 9% of the chord length of the airfoil reduces the section lift coefficient by approximately 6:3% from that of the airfoil alone. 1."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a 2-dimensional airfoil with a circular object of diameter 0.09c (where c is the chord length of the airfoil) placed at a position ahead of the leading edge of the airfoil. Using the conformal mapping method described in the paper, compute the section lift coefficient of the airfoil with the circular object.\n\n**A)** 0.93c^2\n**B)** 0.96c^2\n**C)** 0.99c^2\n**D)** 1.03c^2\n\n**Correct Answer:** B) 0.96c^2\n\n**Explanation:** According to the paper, the second object (cylinder) reduces the section lift coefficient by approximately 6.3% from that of the airfoil alone. This means that the lift coefficient of the airfoil with the cylinder is 96.7% of the lift coefficient of the airfoil alone, or 0.96c^2."}, "33": {"documentation": {"title": "Noise Robust Generative Adversarial Networks", "source": "Takuhiro Kaneko, Tatsuya Harada", "docs_id": "1911.11776", "section": ["cs.CV", "cs.LG", "eess.IV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Noise Robust Generative Adversarial Networks. Generative adversarial networks (GANs) are neural networks that learn data distributions through adversarial training. In intensive studies, recent GANs have shown promising results for reproducing training images. However, in spite of noise, they reproduce images with fidelity. As an alternative, we propose a novel family of GANs called noise robust GANs (NR-GANs), which can learn a clean image generator even when training images are noisy. In particular, NR-GANs can solve this problem without having complete noise information (e.g., the noise distribution type, noise amount, or signal-noise relationship). To achieve this, we introduce a noise generator and train it along with a clean image generator. However, without any constraints, there is no incentive to generate an image and noise separately. Therefore, we propose distribution and transformation constraints that encourage the noise generator to capture only the noise-specific components. In particular, considering such constraints under different assumptions, we devise two variants of NR-GANs for signal-independent noise and three variants of NR-GANs for signal-dependent noise. On three benchmark datasets, we demonstrate the effectiveness of NR-GANs in noise robust image generation. Furthermore, we show the applicability of NR-GANs in image denoising. Our code is available at https://github.com/takuhirok/NR-GAN/."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of Noise Robust Generative Adversarial Networks (NR-GANs), and how do they address the limitations of traditional GANs in reproducing images with fidelity in the presence of noise?\n\n**A)** NR-GANs are designed to learn a clean image generator by minimizing the difference between the generated images and the training images, without considering the noise present in the data. This approach is motivated by the need to improve the robustness of GANs to noise.\n\n**B)** NR-GANs are proposed to learn a clean image generator by introducing a noise generator that captures only the noise-specific components of the training images, thereby reducing the impact of noise on the generated images. This approach is motivated by the need to develop GANs that can learn to generate images with fidelity in the presence of noise.\n\n**C)** NR-GANs are designed to learn a clean image generator by training a noise generator and a clean image generator simultaneously, without any constraints on the noise generator. This approach is motivated by the need to improve the efficiency of GAN training.\n\n**D)** NR-GANs are proposed to learn a clean image generator by minimizing the difference between the generated images and the training images, while also considering the noise present in the data. This approach is motivated by the need to improve the robustness of GANs to noise.\n\n**Correct Answer:** B) NR-GANs are proposed to learn a clean image generator by introducing a noise generator that captures only the noise-specific components of the training images, thereby reducing the impact of noise on the generated images. This approach is motivated by the need to develop GANs that can learn to generate images with fidelity in the presence of noise.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the primary motivation behind the development of NR-GANs, which is to learn a clean image generator that can produce images with fidelity in the presence of noise. The other options are incorrect because they either oversimplify or misrepresent the approach used in NR-GANs. Option A is incorrect because it implies that NR-GANs learn a clean image generator by minimizing the difference between the generated images and the training images, without considering the noise present in the data. Option C is incorrect because it implies that NR-GANs train a noise generator and a clean image generator simultaneously without any constraints on the noise generator, which is not the case. Option D is incorrect because it implies that NR-GANs learn a clean image generator by minimizing the difference between the generated images and the training images, while also considering the noise present in the data, which is not the primary motivation behind NR-GANs."}, "34": {"documentation": {"title": "Site-Occupation Embedding Theory using Bethe Ansatz Local Density\n  Approximations", "source": "Bruno Senjean, Naoki Nakatani, Masahisa Tsuchiizu, Emmanuel Fromager", "docs_id": "1710.03125", "section": ["cond-mat.str-el", "physics.chem-ph", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Site-Occupation Embedding Theory using Bethe Ansatz Local Density\n  Approximations. Site-occupation embedding theory (SOET) is an alternative formulation of density-functional theory (DFT) for model Hamiltonians where the fully-interacting Hubbard problem is mapped, in principle exactly, onto an impurity-interacting (rather than a non-interacting) one. It provides a rigorous framework for combining wavefunction (or Green function) based methods with DFT. In this work, exact expressions for the per-site energy and double occupation of the uniform Hubbard model are derived in the context of SOET. As readily seen from these derivations, the so-called bath contribution to the per-site correlation energy is, in addition to the latter, the key density functional quantity to model in SOET. Various approximations based on Bethe ansatz and perturbative solutions to the Hubbard and single impurity Anderson models are constructed and tested on a one-dimensional ring. The self-consistent calculation of the embedded impurity wavefunction has been performed with the density matrix renormalization group method. It has been shown that promising results are obtained in specific regimes of correlation and density. Possible further developments have been proposed in order to provide reliable embedding functionals and potentials."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Site-Occupation Embedding Theory (SOET), what is the key density functional quantity that is modeled in addition to the per-site correlation energy, and how is it related to the bath contribution to the per-site correlation energy?\n\n**A)** The double occupation of the uniform Hubbard model is the key density functional quantity, and it is related to the bath contribution through a linear relationship.\n\n**B)** The bath contribution to the per-site correlation energy is the key density functional quantity, and it is related to the double occupation through a non-linear relationship.\n\n**C)** The per-site energy of the uniform Hubbard model is the key density functional quantity, and it is related to the bath contribution through a quadratic relationship.\n\n**D)** The double occupation of the uniform Hubbard model is the key density functional quantity, and it is related to the bath contribution through a non-linear relationship.\n\n**Correct Answer:** B) The bath contribution to the per-site correlation energy is the key density functional quantity, and it is related to the double occupation through a non-linear relationship.\n\n**Explanation:** According to the documentation, the bath contribution to the per-site correlation energy is, in addition to the per-site correlation energy, the key density functional quantity to model in SOET. Furthermore, the derivations show that the double occupation of the uniform Hubbard model is related to the bath contribution through a non-linear relationship. Therefore, option B is the correct answer."}, "35": {"documentation": {"title": "Two-photon exchange from intermediate state resonances in elastic\n  electron-proton scattering", "source": "Jaseer Ahmed and P. G. Blunden and W. Melnitchouk", "docs_id": "2006.12543", "section": ["nucl-th", "hep-ph", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Two-photon exchange from intermediate state resonances in elastic\n  electron-proton scattering. We use a recently developed dispersive approach to compute the two-photon exchange (TPE) correction to elastic electron-proton scattering, including contributions from hadronic $J^P=1/2^\\pm$ and $3/2^\\pm$ resonant intermediate states below~1.8~GeV. For the transition amplitudes from the proton ground state to the resonant excited states we employ new exclusive meson electroproduction data from CLAS at $Q^2 \\lesssim 5$~GeV$^2$, and we explore the effects of both fixed and dynamic widths for the resonances. Among the resonant states, the $N(1520)~\\!3/2^-$ becomes dominant for $Q^2 \\gtrsim 2$~GeV$^2$, with a sign opposite to the comparably sized $\\Delta(1232)~\\!3/2^+$ contribution, leading to an overall increase in the size of the TPE correction to the cross section relative to the nucleon only contribution at higher $Q^2$ values. The results are in good overall agreement with recent $e^+ p$ to $e^- p$ cross section ratio and polarization transfer measurements, and provide compelling evidence for a resolution of the electric to magnetic form factor ratio discrepancy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of the two-photon exchange (TPE) correction to elastic electron-proton scattering at higher Q^2 values, and how does it compare to the nucleon-only contribution?\n\nA) The TPE correction decreases the cross section, while the nucleon-only contribution remains constant.\nB) The TPE correction increases the cross section, with a sign opposite to the comparably sized \u0394(1232) 3/2+ contribution.\nC) The TPE correction has no effect on the cross section, as the resonant states do not contribute significantly at higher Q^2 values.\nD) The TPE correction decreases the cross section, while the nucleon-only contribution increases, leading to a net decrease in the cross section.\n\nCorrect Answer: B) The TPE correction increases the cross section, with a sign opposite to the comparably sized \u0394(1232) 3/2+ contribution.\n\nExplanation: The correct answer is based on the statement in the documentation that \"the $N(1520)~\\!3/2^-$ becomes dominant for $Q^2 \\gtrsim 2$~GeV$^2$, with a sign opposite to the comparably sized $\\Delta(1232)~\\!3/2^+$ contribution, leading to an overall increase in the size of the TPE correction to the cross section relative to the nucleon only contribution at higher $Q^2$ values.\" This indicates that the TPE correction increases the cross section, with a specific sign and magnitude, making option B the correct answer."}, "36": {"documentation": {"title": "Longitudinal asymmetry and its effect on pseudorapidity distributions in\n  Pb-Pb collisions at $\\sqrt{s_{NN}}$ = 2.76 TeV", "source": "ALICE Collaboration", "docs_id": "1710.07975", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Longitudinal asymmetry and its effect on pseudorapidity distributions in\n  Pb-Pb collisions at $\\sqrt{s_{NN}}$ = 2.76 TeV. First results on the longitudinal asymmetry and its effect on the pseudorapidity distributions in Pb-Pb collisions at $\\sqrt{s_{\\mathrm{NN}}}$ = 2.76 TeV at the Large Hadron Collider are obtained with the ALICE detector. The longitudinal asymmetry arises because of an unequal number of participating nucleons from the two colliding nuclei, and is estimated for each event by measuring the energy in the forward neutron-Zero-Degree-Calorimeters (ZNs). The effect of the longitudinal asymmetry is measured on the pseudorapidity distributions of charged particles in the regions $|\\eta| < 0.9$, $2.8 < \\eta < 5.1$ and $-3.7 < \\eta < -1.7 $ by taking the ratio of the pseudorapidity distributions from events corresponding to different regions of asymmetry. The coefficients of a polynomial fit to the ratio characterise the effect of the asymmetry. A Monte Carlo simulation using a Glauber model for the colliding nuclei is tuned to reproduce the spectrum in the ZNs and provides a relation between the measurable longitudinal asymmetry and the shift in the rapidity ($y_{\\mathrm{0}}$) of the participant zone formed by the unequal number of participating nucleons. The dependence of the coefficient of the linear term in the polynomial expansion, $c_{\\rm 1}$, on the mean value of $y_{\\mathrm{0}}$ is investigated."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Pb-Pb collisions at $\\sqrt{s_{\\mathrm{NN}}}$ = 2.76 TeV, what is the relationship between the longitudinal asymmetry and the shift in the rapidity ($y_{\\mathrm{0}}$) of the participant zone formed by the unequal number of participating nucleons, as described in the ALICE detector's measurement of pseudorapidity distributions?\n\nA) The longitudinal asymmetry is directly proportional to the shift in $y_{\\mathrm{0}}$.\nB) The longitudinal asymmetry is inversely proportional to the shift in $y_{\\mathrm{0}}$.\nC) The longitudinal asymmetry is related to the shift in $y_{\\mathrm{0}}$ through a polynomial fit, with the coefficient of the linear term ($c_{\\rm 1}$) being dependent on the mean value of $y_{\\mathrm{0}}$.\nD) The longitudinal asymmetry is independent of the shift in $y_{\\mathrm{0}}$.\n\nCorrect Answer: C) The longitudinal asymmetry is related to the shift in $y_{\\mathrm{0}}$ through a polynomial fit, with the coefficient of the linear term ($c_{\\rm 1}$) being dependent on the mean value of $y_{\\mathrm{0}}$.\n\nExplanation: The correct answer is based on the description in the documentation, which states that the Monte Carlo simulation using a Glauber model for the colliding nuclei provides a relation between the measurable longitudinal asymmetry and the shift in the rapidity ($y_{\\mathrm{0}}$) of the participant zone formed by the unequal number of participating nucleons. The relationship is further described as being characterized by a polynomial fit, with the coefficient of the linear term ($c_{\\rm 1}$) being dependent on the mean value of $y_{\\mathrm{0}}$."}, "37": {"documentation": {"title": "A New Continuum-Based Thick Shell Finite Element for Soft Biological\n  Tissues in Dynamics: Part 1 - Preliminary Benchmarking Using Classic\n  Verification Experiments", "source": "Bahareh Momenan (1), Michel R. Labrosse (2) ((1,2) Department of\n  Mechanical Engineering, University of Ottawa)", "docs_id": "1801.04029", "section": ["math.NA", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A New Continuum-Based Thick Shell Finite Element for Soft Biological\n  Tissues in Dynamics: Part 1 - Preliminary Benchmarking Using Classic\n  Verification Experiments. For the finite element simulation of thin soft biological tissues in dynamics, shell elements, compared to volume elements, can capture the whole tissue thickness at once, and feature larger critical time steps. However, the capabilities of existing shell elements to account for irregular geometries, and hyperelastic, anisotropic 3D deformations characteristic of soft tissues are still limited. As improvement, we developed a new general nonlinear thick continuum-based (CB) shell finite element (FE) based on the Mindlin-Reissner shell theory, with large bending, large distortion and large strain capabilities, embedded in the updated Lagrangian formulation and explicit time integration. We performed numerical benchmark experiments available from the literature that focus on engineering linear elastic materials, which, verified and proved the new thick CB shell FE to: 1) be accurate an efficient 2) be powerful in handling large 3D deformations, curved geometries, 3) accommodate coarse distorted meshes, and 4) achieve comparatively fast computational times. The new element was also insensitive to three types of locking (shear, membrane and volumetric), and warping effects. The capabilities of the present thick CB shell FE in the biomedical realm are illustrated in a companion article (Part 2), in which anisotropic incompressible hyperelastic constitutive relations are implemented and verified."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the key advantages of the new thick continuum-based (CB) shell finite element (FE) developed for simulating soft biological tissues in dynamics, as demonstrated in the preliminary benchmarking experiments?\n\nA) It can only capture small deformations and is limited to linear elastic materials.\nB) It is accurate and efficient, but lacks the ability to handle large 3D deformations and curved geometries.\nC) It is powerful in handling large 3D deformations, curved geometries, and accommodates coarse distorted meshes, while achieving comparatively fast computational times.\nD) It is insensitive to locking effects, but has limited capabilities in handling anisotropic incompressible hyperelastic constitutive relations.\n\nCorrect Answer: C) It is powerful in handling large 3D deformations, curved geometries, and accommodates coarse distorted meshes, while achieving comparatively fast computational times.\n\nExplanation: The correct answer is C) because the new thick CB shell FE is demonstrated to be powerful in handling large 3D deformations, curved geometries, and accommodates coarse distorted meshes, while achieving comparatively fast computational times. This is explicitly stated in the preliminary benchmarking experiments, which verified the new element's capabilities in these areas. The other options are incorrect because they either contradict the findings of the benchmarking experiments (A and B) or are not mentioned as advantages of the new FE (D)."}, "38": {"documentation": {"title": "On the fixed-parameter tractability of the partial vertex cover problem\n  with a matching constraint in edge-weighted bipartite graphs", "source": "Vahan Mkrtchyan, Garik Petrosyan", "docs_id": "2104.11215", "section": ["cs.DM", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the fixed-parameter tractability of the partial vertex cover problem\n  with a matching constraint in edge-weighted bipartite graphs. In the classical partial vertex cover problem, we are given a graph $G$ and two positive integers $R$ and $L$. The goal is to check whether there is a subset $V'$ of $V$ of size at most $R$, such that $V'$ covers at least $L$ edges of $G$. The problem is NP-hard as it includes the Vertex Cover problem. Previous research has addressed the extension of this problem where one has weight-functions defined on sets of vertices and edges of $G$. In this paper, we consider the following version of the problem where on the input we are given an edge-weighted bipartite graph $G$, and three positive integers $R$, $S$ and $T$. The goal is to check whether $G$ has a subset $V'$ of vertices of $G$ of size at most $R$, such that the edges of $G$ covered by $V'$ have weight at least $S$ and they include a matching of weight at least $T$. In the paper, we address this problem from the perspective of fixed-parameter tractability. One of our hardness results is obtained via a reduction from the bi-objective knapsack problem, which we show to be W[1]-hard with respect to one of parameters. We believe that this problem might be useful in obtaining similar results in other situations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider an edge-weighted bipartite graph G with two sets of vertices V and W, where each edge (u, v) has a weight w(u, v) \u2265 0. The goal is to find a subset V' of V such that:\n\n1. The size of V' is at most R.\n2. The edges of G covered by V' have a total weight of at least S.\n3. The edges of G covered by V' form a matching, i.e., every vertex in W is incident to exactly one edge in V'.\n\nWhat is the complexity class of this problem, assuming the input graph G and parameters R, S, and T are given?\n\nA) NP-complete\nB) W[1]-hard\nC) FPT\nD) P\n\nCorrect Answer: B) W[1]-hard\n\nExplanation: The problem is W[1]-hard because it is a variant of the classical partial vertex cover problem, which is known to be NP-hard. The reduction from the bi-objective knapsack problem, which is W[1]-hard, to this problem provides evidence that it is also W[1]-hard. This means that the problem is fixed-parameter tractable (FPT) if and only if the parameterized version of the Vertex Cover problem is FPT, which is not the case."}, "39": {"documentation": {"title": "Temporal analysis of acoustic emission from a plunged granular bed", "source": "Daisuke Tsuji and Hiroaki Katsuragi", "docs_id": "1509.05675", "section": ["cond-mat.stat-mech", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Temporal analysis of acoustic emission from a plunged granular bed. The statistical property of acoustic emission (AE) events from a plunged granular bed is analyzed by means of actual time and natural time analyses. These temporal analysis methods allow us to investigate the details of AE events that follow a power-law distribution. In the actual time analysis, the calm time distribution and the decay of the event-occurrence density after the largest event (i.e., Omori-Utsu law) are measured. Although the former always shows a power-law form, the latter does not always obey a power law. Markovianity of the event-occurrence process is also verified using a scaling law by assuming that both of them exhibit power laws. We find that the effective shear strain rate is a key parameter to classify the emergence rate of power-law nature and Markovianity in the granular AE events. For the natural time analysis, the existence of self organized critical (SOC) states is revealed by calculating the variance of natural time $\\chi_k$, where $k$th natural time of N events is defined as $\\chi_k=k/N$. In addition, the energy difference distribution can be fitted by a $q$-Gaussian form, which is also consistent with the criticality of the system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the temporal analysis methods used to study acoustic emission (AE) events from a plunged granular bed. How do the actual time and natural time analyses differ in their conclusions about the power-law distribution of AE events, and what is the significance of the effective shear strain rate in classifying the emergence of power-law nature and Markovianity in the granular AE events?\n\n**A)** The actual time analysis reveals a power-law distribution in the calm time distribution, while the natural time analysis shows a non-power-law distribution in the decay of event-occurrence density after the largest event. The effective shear strain rate is a key parameter in distinguishing between power-law and non-power-law systems.\n\n**B)** The natural time analysis reveals a power-law distribution in the variance of natural time, while the actual time analysis shows a non-power-law distribution in the energy difference distribution. The effective shear strain rate is a key parameter in distinguishing between power-law and non-power-law systems.\n\n**C)** The actual time analysis reveals a power-law distribution in the decay of event-occurrence density after the largest event, while the natural time analysis shows a power-law distribution in the variance of natural time. The effective shear strain rate is a key parameter in classifying the emergence of power-law nature and Markovianity in the granular AE events.\n\n**D)** The natural time analysis reveals a non-power-law distribution in the variance of natural time, while the actual time analysis shows a power-law distribution in the energy difference distribution. The effective shear strain rate is a key parameter in distinguishing between power-law and non-power-law systems.\n\n**Correct Answer:** C) The actual time analysis reveals a power-law distribution in the calm time distribution, while the natural time analysis shows a power-law distribution in the variance of natural time. The effective shear strain rate is a key parameter in classifying the emergence of power-law nature and Markovianity in the granular AE events.\n\n**Explanation:**\n\nThe actual time analysis reveals a power-law distribution in the calm time distribution, indicating that the time between AE events follows a power-law distribution. In contrast, the natural time analysis reveals a power-law distribution in the variance of natural time, indicating that the time between AE events follows a power-law distribution in the natural time scale. The effective shear strain rate is a key parameter in classifying the emergence of power-law nature and Markovianity in the granular AE events, as it distinguishes between power-law and non-power-law systems. This is because the effective shear strain rate is a measure of the rate at which the granular bed is shearing, and it is a key factor in determining the power-law nature of the AE events."}, "40": {"documentation": {"title": "A unified view of LIBOR models", "source": "Kathrin Glau, Zorana Grbac, Antonis Papapantoleon", "docs_id": "1601.01352", "section": ["q-fin.MF", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A unified view of LIBOR models. We provide a unified framework for modeling LIBOR rates using general semimartingales as driving processes and generic functional forms to describe the evolution of the dynamics. We derive sufficient conditions for the model to be arbitrage-free which are easily verifiable, and for the LIBOR rates to be true martingales under the respective forward measures. We discuss when the conditions are also necessary and comment on further desirable properties such as those leading to analytical tractability and positivity of rates. This framework allows to consider several popular models in the literature, such as LIBOR market models driven by Brownian motion or jump processes, the L\\'evy forward price model as well as the affine LIBOR model, under one umbrella. Moreover, we derive structural results about LIBOR models and show, in particular, that only models where the forward price is an exponentially affine function of the driving process preserve their structure under different forward measures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the sufficient condition for a LIBOR model to be arbitrage-free, and how does it relate to the preservation of the model's structure under different forward measures?\n\nA) The model must have a deterministic driving process, and the forward price must be a linear function of the driving process.\nB) The model must have a stochastic driving process, and the forward price must be an exponentially affine function of the driving process.\nC) The model must have a deterministic driving process, and the forward price must be an exponentially affine function of the driving process.\nD) The model must have a stochastic driving process, and the forward price must be a linear function of the driving process.\n\nCorrect Answer: B) The model must have a stochastic driving process, and the forward price must be an exponentially affine function of the driving process.\n\nExplanation: According to the documentation, the sufficient condition for a LIBOR model to be arbitrage-free is that the forward price is an exponentially affine function of the driving process. This condition is also necessary for the model to preserve its structure under different forward measures. Therefore, option B is the correct answer. Options A and C are incorrect because they do not specify that the driving process must be stochastic, and option D is incorrect because it specifies that the forward price must be a linear function, which is not the correct condition."}, "41": {"documentation": {"title": "The 2-point angular correlation function of 20,000 galaxies to V<23.5\n  and I<22", "source": "Remi A. Cabanac (1), Valerie de Lapparent (1), Paul Hickson (2) ((1)\n  Institut d'astrophysique de Paris, (2) U.B.C., Vancouver)", "docs_id": "astro-ph/0007184", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The 2-point angular correlation function of 20,000 galaxies to V<23.5\n  and I<22. The UH8K wide field camera of the CFHT was used to image 0.68 deg^2 of sky. From these images, ~20,000 galaxies were detected to completeness magnitudes V<23.5 and I<22.5. The angular correlation function of these galaxies is well represented by the parameterization omega(theta) = A_W*theta^-delta. The slope delta=-0.8 shows no significant variation over the range of magnitude. The amplitude A_W decreases with increasing magnitude in a way that is most compatible with a Lambda-CDM model (Omega_0 = 0.2, Lambda=0.8) with a hierarchical clustering evolution parameter epsilon>0. We infer a best-fit spatial correlation length of r_00= 5.85+/-0.5 h^-1 Mpc at z=0. The peak redshift of the survey (I<22.5) is estimated to be z_peak~0.58, using the blue-evolving luminosity function from the CFRS and the flat Lambda cosmology, and r_0(z_peak)=3.5+/-0.5 h^-1 Mpc. We also detect a significant difference in clustering amplitude for the red and blue galaxies, quantitatively measured by correlation lengths of r_00=5.3+/-0.5 h^-1 Mpc and r_00=1.9+/-0.9 h^-1 Mpc respectively, at z=0."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The authors of the study report a significant difference in the clustering amplitude between red and blue galaxies. What is the estimated correlation length of the red galaxies at z=0, and what is the implication of this result for our understanding of galaxy evolution?\n\nA) r_00 = 5.3 \u00b1 0.5 h^-1 Mpc, suggesting that red galaxies are more clustered than blue galaxies, which is consistent with the idea that red galaxies are more massive and have undergone more mergers.\n\nB) r_00 = 1.9 \u00b1 0.9 h^-1 Mpc, suggesting that blue galaxies are more clustered than red galaxies, which is consistent with the idea that blue galaxies are more massive and have undergone less mergers.\n\nC) r_00 = 5.3 \u00b1 0.5 h^-1 Mpc, suggesting that blue galaxies are more clustered than red galaxies, which is consistent with the idea that blue galaxies are more massive and have undergone more mergers.\n\nD) r_00 = 1.9 \u00b1 0.9 h^-1 Mpc, suggesting that red galaxies are more clustered than blue galaxies, which is consistent with the idea that red galaxies are more massive and have undergone less mergers.\n\n**Correct Answer:** B) r_00 = 1.9 \u00b1 0.9 h^-1 Mpc, suggesting that blue galaxies are more clustered than red galaxies, which is consistent with the idea that blue galaxies are more massive and have undergone less mergers.\n\n**Explanation:** The correct answer is B) r_00 = 1.9 \u00b1 0.9 h^-1 Mpc, because the study reports a correlation length of r_00 = 1.9 \u00b1 0.9 h^-1 Mpc for the blue galaxies, which is significantly smaller than the correlation length of r_00 = 5.3 \u00b1 0.5 h^-1 Mpc for the red galaxies. This suggests that blue galaxies are more clustered than red galaxies, which is consistent with the idea that blue galaxies are more massive and have undergone less mergers."}, "42": {"documentation": {"title": "Analysis of a New Harmonically Enriched Multiscale Coarse Space for\n  Domain Decomposition Methods", "source": "Martin J. Gander, Atle Loneland, Talal Rahman", "docs_id": "1512.05285", "section": ["math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analysis of a New Harmonically Enriched Multiscale Coarse Space for\n  Domain Decomposition Methods. We propose a new, harmonically enriched multiscale coarse space (HEM) for domain decomposition methods. For a coercive high contrast model problem, we show how to enrich the coarse space so that the method is robust against any variations and discontinuities in the problem parameters both inside subdomains and across and along subdomain boundaries. We prove our results for an enrichment strategy based on solving simple, lower dimensional eigenvalue problems on the interfaces between subdomains, and we call the resulting coarse space the spectral harmonically enriched multiscale coarse space (SHEM). We then also give a variant that performs equally well in practice, and does not require the solve of eigenvalue problems, which we call non-spectral harmonically enriched multiscale coarse space (NSHEM). Our enrichment process naturally reaches the optimal coarse space represented by the full discrete harmonic space, which enables us to turn the method into a direct solver (OHEM). We also extensively test our new coarse spaces numerically, and the results confirm our analysis"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the Spectral Harmonically Enriched Multiscale Coarse Space (SHEM) over the Non-Spectral Harmonically Enriched Multiscale Coarse Space (NSHEM) in terms of computational efficiency?\n\nA) SHEM requires the solve of eigenvalue problems, whereas NSHEM does not.\nB) SHEM is more robust against variations and discontinuities in problem parameters, but NSHEM is more computationally efficient.\nC) SHEM is more computationally efficient than NSHEM, as it does not require the solve of eigenvalue problems.\nD) NSHEM is more robust against variations and discontinuities in problem parameters, but SHEM is more computationally efficient.\n\nCorrect Answer: C) SHEM is more computationally efficient than NSHEM, as it does not require the solve of eigenvalue problems.\n\nExplanation: The question requires the test-taker to understand the key differences between SHEM and NSHEM. The correct answer, C, highlights the advantage of SHEM in terms of computational efficiency, as it does not require the solve of eigenvalue problems. The other options are incorrect because they either misrepresent the advantages of SHEM and NSHEM (A and B) or incorrectly state the advantages of NSHEM (D)."}, "43": {"documentation": {"title": "Cross Section Measurement of 9Be(\\gamma,n)8Be and Implications for\n  \\alpha+\\alpha+n -> 9Be in the r-Process", "source": "C. W. Arnold, T. B. Clegg, C. Iliadis, H. J. Karwowski, G. C. Rich, J.\n  R. Tompkins, C. R. Howell", "docs_id": "1112.1148", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cross Section Measurement of 9Be(\\gamma,n)8Be and Implications for\n  \\alpha+\\alpha+n -> 9Be in the r-Process. Models of the r-process are sensitive to the production rate of 9Be because, in explosive environments rich in neutrons, alpha(alpha n,gamma)9Be is the primary mechanism for bridging the stability gaps at A=5 and A=8. The alpha(alpha n,gamma)9Be reaction represents a two-step process, consisting of alpha+alpha -> 8Be followed by 8Be(n,gamma)9Be. We report here on a new absolute cross section measurement for the 9Be(gamma,n)8Be reaction conducted using a highly-efficient, 3He-based neutron detector and nearly-monoenergetic photon beams, covering energies from E_gamma = 1.5 MeV to 5.2 MeV, produced by the High Intensity gamma-ray Source of Triangle Universities Nuclear Laboratory. In the astrophysically important threshold energy region, the present cross sections are 40% larger than those found in most previous measurements and are accurate to +/- 10% (95% confidence). The revised thermonuclear alpha(alpha n,gamma)9Be reaction rate could have implications for the r-process in explosive environments such as Type II supernovae."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the implications of the revised thermonuclear alpha(alpha n,gamma)9Be reaction rate for the r-process in explosive environments such as Type II supernovae?\n\nA) The reaction rate would lead to a decrease in the production of heavy elements, potentially affecting the abundance of certain elements in the universe.\nB) The reaction rate would have no significant impact on the r-process, as it is not a critical component of the process.\nC) The reaction rate could lead to an increase in the production of heavy elements, potentially affecting the abundance of certain elements in the universe, particularly those with atomic numbers greater than 92.\nD) The reaction rate would lead to a shift in the stability gaps at A=5 and A=8, potentially affecting the production of 9Be and other elements in the r-process.\n\nCorrect Answer: C) The reaction rate could lead to an increase in the production of heavy elements, potentially affecting the abundance of certain elements in the universe, particularly those with atomic numbers greater than 92.\n\nExplanation: The revised thermonuclear alpha(alpha n,gamma)9Be reaction rate could lead to an increase in the production of heavy elements, particularly those with atomic numbers greater than 92, due to the increased production of 9Be. This could have significant implications for the abundance of certain elements in the universe, potentially affecting the formation of heavy elements in explosive environments such as Type II supernovae."}, "44": {"documentation": {"title": "Effects of bacterial density on growth rate and characteristics of\n  microbial-induced CaCO3 precipitates: a particle-scale experimental study", "source": "Yuze Wang, Kenichi Soga, Jason T. DeJong, Alexandre J. Kabla", "docs_id": "2007.04094", "section": ["q-bio.QM", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effects of bacterial density on growth rate and characteristics of\n  microbial-induced CaCO3 precipitates: a particle-scale experimental study. Microbial-Induced Carbonate Precipitation (MICP) has been explored for more than a decade as a promising soil improvement technique. However, it is still challenging to predict and control the growth rate and characteristics of CaCO3 precipitates, which directly affect the engineering performance of MICP-treated soils. In this study, we employ a microfluidics-based pore scale model to observe the effect of bacterial density on the growth rate and characteristics of CaCO3 precipitates during MICP processes occurring at the sand particle scale. Results show that the precipitation rate of CaCO3 increases with bacterial density in the range between 0.6e8 and 5.2e8 cells/ml. Bacterial density also affects both the size and number of CaCO3 crystals. A low bacterial density of 0.6e8 cells/ml produced 1.1e6 crystals/ml with an average crystal volume of 8,000 um3, whereas a high bacterial density of 5.2e8 cells/ml resulted in more crystals (2.0e7 crystals/ml) but with a smaller average crystal volume of 450 um3. The produced CaCO3 crystals were stable when the bacterial density was 0.6e8 cells/ml. When the bacterial density was 4-10 times higher, the crystals were first unstable and then transformed into more stable CaCO3 crystals. This suggests that bacterial density should be an important consideration in the design of MICP protocols."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the optimal bacterial density for the growth rate and characteristics of CaCO3 precipitates in Microbial-Induced Carbonate Precipitation (MICP) processes?\n\nA) A bacterial density of 1.0e8 cells/ml is optimal for the growth rate and characteristics of CaCO3 precipitates.\nB) A bacterial density of 0.6e8 cells/ml is optimal for the growth rate and characteristics of CaCO3 precipitates, as it produces stable CaCO3 crystals.\nC) A bacterial density of 5.2e8 cells/ml is optimal for the growth rate and characteristics of CaCO3 precipitates, as it results in more crystals but with a smaller average crystal volume.\nD) A bacterial density of 1.0e9 cells/ml is optimal for the growth rate and characteristics of CaCO3 precipitates, as it produces the largest average crystal volume.\n\nCorrect Answer: B) A bacterial density of 0.6e8 cells/ml is optimal for the growth rate and characteristics of CaCO3 precipitates, as it produces stable CaCO3 crystals.\n\nExplanation: The study found that a low bacterial density of 0.6e8 cells/ml produced stable CaCO3 crystals, whereas higher bacterial densities (4-10 times higher) resulted in unstable crystals that transformed into more stable CaCO3 crystals. Therefore, the optimal bacterial density for the growth rate and characteristics of CaCO3 precipitates is 0.6e8 cells/ml."}, "45": {"documentation": {"title": "Current noise cross correlation mediated by Majorana bound states", "source": "Hai-Feng Lu, Hai-Zhou Lu, and Shun-Qing Shen", "docs_id": "1411.4260", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Current noise cross correlation mediated by Majorana bound states. We study the transport properties of a quantum dot-Majorana hybrid system, in which each of paired Majorana bound states is connected to one quantum dot. With the help of non-equilibrium Green's function method, we obtain an exact solution of the Green's functions and calculate the currents through the quantum dots and nonlocal noise cross correlation between the currents. As a function of dot energy levels $\\epsilon_{1}$ and $\\epsilon_{2}$, we find that for the symmetric level configuration $\\epsilon_{1}=\\epsilon_{2}$, the noise cross correlation is negative in the low lead voltage regime, while it becomes positive with the increase of the lead voltages. Due to the particle-hole symmetry, the cross correlation is always positive in the anti-symmetric case $\\epsilon_{1}=-\\epsilon_{2}$. In contrast, the cross correlation of non-Majorana setups is always positive. For comparison, we also perform the diagonalized master equation calculation to check its applicability. It is found that the diagonalized master equations work well in most regimes of system parameters. Nevertheless, it shows an obvious deviation from the exact solution by the non-equilibrium Green's function method when all eigenenergies of the dot-Majorana hybrid system and simultaneously the energy intervals are comparable to the dot-lead coupling strength."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the current noise cross correlation mediated by Majorana bound states, what is the relationship between the noise cross correlation and the lead voltage for a symmetric level configuration of the quantum dot energy levels?\n\n**A)** The noise cross correlation is always positive and independent of the lead voltage.\n**B)** The noise cross correlation is negative in the low lead voltage regime and becomes positive with the increase of the lead voltage.\n**C)** The noise cross correlation is positive for all lead voltages and does not depend on the energy levels of the quantum dot.\n**D)** The noise cross correlation is zero for all lead voltages and energy level configurations.\n\n**Correct Answer:** B) The noise cross correlation is negative in the low lead voltage regime and becomes positive with the increase of the lead voltage.\n\n**Explanation:** According to the documentation, for the symmetric level configuration (\u03b51 = \u03b52), the noise cross correlation is negative in the low lead voltage regime, while it becomes positive with the increase of the lead voltage. This is a key finding of the study, highlighting the importance of considering the lead voltage regime when analyzing the noise cross correlation in Majorana bound state systems."}, "46": {"documentation": {"title": "Kinetic Turbulence in Astrophysical Plasmas: Waves and/or Structures?", "source": "D. Groselj, C. H. K. Chen, A. Mallet, R. Samtaney, K. Schneider, F.\n  Jenko", "docs_id": "1806.05741", "section": ["physics.plasm-ph", "astro-ph.SR", "physics.space-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kinetic Turbulence in Astrophysical Plasmas: Waves and/or Structures?. The question of the relative importance of coherent structures and waves has for a long time attracted a great deal of interest in astrophysical plasma turbulence research, with a more recent focus on kinetic scale dynamics. Here we utilize high-resolution observational and simulation data to investigate the nature of waves and structures emerging in a weakly collisional, turbulent kinetic plasma. Observational results are based on in situ solar wind measurements from the Cluster and MMS spacecraft, and the simulation results are obtained from an externally driven, three-dimensional fully kinetic simulation. Using a set of novel diagnostic measures we show that both the large-amplitude structures and the lower-amplitude background fluctuations preserve linear features of kinetic Alfven waves to order unity. This quantitative evidence suggests that the kinetic turbulence cannot be described as a mixture of mutually exclusive waves and structures but may instead be pictured as an ensemble of localized, anisotropic wave packets or \"eddies\" of varying amplitudes, which preserve certain linear wave properties during their nonlinear evolution."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What can be inferred from the observational and simulation data presented in the study on kinetic turbulence in astrophysical plasmas, regarding the nature of waves and structures emerging in a weakly collisional, turbulent kinetic plasma?\n\n**A)** The study suggests that kinetic turbulence can be described as a mixture of mutually exclusive waves and structures, with no overlap between them.\n\n**B)** The data indicate that large-amplitude structures and lower-amplitude background fluctuations preserve linear features of kinetic Alfven waves to order unity, suggesting a wave-like behavior.\n\n**C)** The study concludes that kinetic turbulence is a chaotic, random process with no underlying wave-like structure.\n\n**D)** The data show that kinetic turbulence is characterized by a single, dominant wave mode that dominates the entire plasma.\n\n**Correct Answer:** B) The data indicate that large-amplitude structures and lower-amplitude background fluctuations preserve linear features of kinetic Alfven waves to order unity, suggesting a wave-like behavior.\n\n**Explanation:** The correct answer is supported by the text, which states that \"both the large-amplitude structures and the lower-amplitude background fluctuations preserve linear features of kinetic Alfven waves to order unity.\" This suggests that the kinetic turbulence exhibits wave-like behavior, with both large-amplitude structures and background fluctuations preserving linear features of kinetic Alfven waves. This is a key finding of the study, and it challenges the traditional view of kinetic turbulence as a mixture of mutually exclusive waves and structures."}, "47": {"documentation": {"title": "Image Segmentation and Processing for Efficient Parking Space Analysis", "source": "Chetan Sai Tutika, Charan Vallapaneni, Karthik R, Bharath KP, N Ruban\n  Rajesh Kumar Muthu", "docs_id": "1803.04620", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Image Segmentation and Processing for Efficient Parking Space Analysis. In this paper, we develop a method to detect vacant parking spaces in an environment with unclear segments and contours with the help of MATLAB image processing capabilities. Due to the anomalies present in the parking spaces, such as uneven illumination, distorted slot lines and overlapping of cars. The present-day conventional algorithms have difficulties processing the image for accurate results. The algorithm proposed uses a combination of image pre-processing and false contour detection techniques to improve the detection efficiency. The proposed method also eliminates the need to employ individual sensors to detect a car, instead uses real-time static images to consider a group of slots together, instead of the usual single slot method. This greatly decreases the expenses required to design an efficient parking system. We compare the performance of our algorithm to that of other techniques. These comparisons show that the proposed algorithm can detect the vacancies in the parking spots while ignoring the false data and other distortions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed algorithm for detecting vacant parking spaces, as described in the paper \"Image Segmentation and Processing for Efficient Parking Space Analysis\"?\n\nA) It eliminates the need for individual sensors to detect cars, reducing expenses.\nB) It uses real-time static images to consider a group of slots together, increasing detection efficiency.\nC) It employs a single slot method, allowing for more accurate results.\nD) It uses conventional algorithms, which have difficulties processing images with anomalies.\n\n**Correct Answer:** B) It uses real-time static images to consider a group of slots together, increasing detection efficiency.\n\n**Explanation:** The correct answer is B) because the paper states that the proposed algorithm uses real-time static images to consider a group of slots together, which greatly decreases the expenses required to design an efficient parking system. This is a key advantage of the proposed algorithm, as it allows for more efficient detection of vacant parking spaces.\n\n**Explanation of incorrect options:**\n\nA) While the paper does mention that the algorithm eliminates the need for individual sensors to detect cars, this is not the primary advantage of the algorithm.\n\nC) The paper actually states that the conventional algorithms have difficulties processing images with anomalies, and that the proposed algorithm addresses this issue.\n\nD) The paper describes the proposed algorithm as a combination of image pre-processing and false contour detection techniques, which is the opposite of using conventional algorithms."}, "48": {"documentation": {"title": "Inferring epidemic parameters for COVID-19 from fatality counts in\n  Mumbai", "source": "Sourendu Gupta", "docs_id": "2004.11677", "section": ["q-bio.PE", "physics.med-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inferring epidemic parameters for COVID-19 from fatality counts in\n  Mumbai. Epidemic parameters are estimated through Bayesian inference using the daily fatality counts in Mumbai during the period from March 31 to April 14. A doubling time of 5.5 days (median with 95% CrI of 4.6-6.9 days) is observed. In the SEIR model this gives the basic reproduction rate R_0 of 3.4 (median with 95% CrI of 2.4-4.8). Using as input the infection fatality rate and the interval between infection and death, the number of infections in Mumbai is inferred. It is found that the ratio of the number of test positives to the total infections is 0.13\\% (median), implying that tests are currently finding 1 out of 750 cases of infection. After correcting for different testing rates, this result is compatible with a measurement of the ratio made recently via serological testing in the USA. From the estimates of the number of infections we infer that the first COVID-19 cases were seeded in Mumbai between late December 2019 and early February 2020. provided the doubling times remained unchanged since then. We remark on some public health implications if the rate of growth cannot be controlled in about a week."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the implications of the estimated doubling time of 5.5 days for the control of the COVID-19 epidemic in Mumbai. If the rate of growth of cases cannot be controlled in about a week, what are the potential public health consequences?\n\n**A)** The city's healthcare system will be overwhelmed within 7 days, leading to a significant increase in hospitalizations and mortality.\n\n**B)** The estimated doubling time suggests that the epidemic is slowing down, and the number of new cases will decrease in the coming days.\n\n**C)** The city's population will be able to return to normal activities within 7 days, as the number of new cases will be under control.\n\n**D)** The estimated doubling time implies that the basic reproduction rate R_0 is high, and the epidemic will continue to spread rapidly unless interventions are taken to control the growth of cases.\n\n**Correct Answer:** D) The estimated doubling time implies that the basic reproduction rate R_0 is high, and the epidemic will continue to spread rapidly unless interventions are taken to control the growth of cases.\n\n**Explanation:** The estimated doubling time of 5.5 days suggests that the epidemic is spreading rapidly, with a basic reproduction rate R_0 of 3.4. This implies that the number of new cases will continue to increase unless interventions are taken to control the growth of cases. The correct answer requires an understanding of the relationship between the doubling time, R_0, and the potential for epidemic control.\n\n**Explanation for candidate B:** This answer is incorrect because the estimated doubling time of 5.5 days suggests that the epidemic is still spreading rapidly, and the number of new cases will continue to increase.\n\n**Explanation for candidate C:** This answer is incorrect because the estimated doubling time of 5.5 days suggests that the epidemic is still spreading rapidly, and the number of new cases will continue to increase. The city's population will not be able to return to normal activities within 7 days.\n\n**Explanation for candidate A:** This answer is partially correct, but it is not the most accurate answer. While the city's healthcare system may be overwhelmed within 7 days, this is not the primary implication of the estimated doubling time. The correct answer focuses on the potential for the epidemic to continue spreading rapidly unless interventions are taken."}, "49": {"documentation": {"title": "Recent developments in nuclear structure theory: an outlook on the\n  muonic atom program", "source": "Oscar Javier Hernandez, Sonia Bacca, Kyle Andrew Wendt", "docs_id": "1712.05187", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Recent developments in nuclear structure theory: an outlook on the\n  muonic atom program. The discovery of the proton-radius puzzle and the subsequent deuteron-radius puzzle is fueling an on-going debate on possible explanations for the difference in the observed radii obtained from muonic atoms and from electron-nucleus systems. Atomic nuclei have a complex internal structure that must be taken into account when analyzing experimental spectroscopic results. Ab initio nuclear structure theory provided the so far most precise estimates of important corrections to the Lamb shift in muonic atoms and is well poised to also investigate nuclear structure corrections to the hyperfine splitting in muonic atoms. Independently on whether the puzzle is due to beyond-the-standard-model physics or not, nuclear structure corrections are a necessary theoretical input to any experimental extraction of electric and magnetic radii from precise muonic atom measurements. Here, we review the status of the calculations performed by the TRIUMF-Hebrew University group, focusing on the deuteron, and discuss preliminary results on magnetic sum rules calculated with two-body currents at next-to-leading order. Two-body currents will be an important ingredient in future calculations of nuclear structure corrections to the hyperfine splitting in muonic atoms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary motivation for investigating nuclear structure corrections to the hyperfine splitting in muonic atoms, and how do these corrections relate to the proton-radius and deuteron-radius puzzles?\n\nA) Nuclear structure corrections are necessary to explain the observed differences in radii between muonic atoms and electron-nucleus systems, and are a key ingredient in resolving the proton-radius and deuteron-radius puzzles.\nB) Nuclear structure corrections are a consequence of beyond-the-standard-model physics, and are not directly related to the proton-radius and deuteron-radius puzzles.\nC) Nuclear structure corrections are a necessary input for experimental extraction of electric and magnetic radii from precise muonic atom measurements, but do not provide a direct explanation for the proton-radius and deuteron-radius puzzles.\nD) Nuclear structure corrections are not relevant to the proton-radius and deuteron-radius puzzles, as these puzzles are primarily driven by experimental uncertainties.\n\nCorrect Answer: A) Nuclear structure corrections are necessary to explain the observed differences in radii between muonic atoms and electron-nucleus systems, and are a key ingredient in resolving the proton-radius and deuteron-radius puzzles.\n\nExplanation: The correct answer is A) because the text states that nuclear structure corrections are a necessary theoretical input to any experimental extraction of electric and magnetic radii from precise muonic atom measurements, and that these corrections are a key ingredient in resolving the proton-radius and deuteron-radius puzzles. This suggests that nuclear structure corrections are motivated by the need to explain the observed differences in radii between muonic atoms and electron-nucleus systems, and are a crucial component in resolving the proton-radius and deuteron-radius puzzles."}, "50": {"documentation": {"title": "A Robust Determination of Milky Way Satellite Properties using\n  Hierarchical Mass Modeling", "source": "Gregory D. Martinez", "docs_id": "1309.2641", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Robust Determination of Milky Way Satellite Properties using\n  Hierarchical Mass Modeling. We introduce a new methodology to robustly determine the mass profile, as well as the overall distribution, of Local Group satellite galaxies. Specifically we employ a statistical multilevel modelling technique, Bayesian hierarchical modelling, to simultaneously constrain the properties of individual Local Group Milky Way satellite galaxies and the characteristics of the Milky Way satellite population. We show that this methodology reduces the uncertainty in individual dwarf galaxy mass measurements up to a factor of a few for the faintest galaxies. We find that the distribution of Milky Way satellites inferred by this analysis, with the exception of the apparent lack of high-mass haloes, is consistent with the Lambda cold dark matter (Lambda-CDM) paradigm. In particular we find that both the measured relationship between the maximum circular velocity and the radius at this velocity, as well as the inferred relationship between the mass within 300 pc and luminosity, match the values predicted by Lambda-CDM simulations for halos with maximum circular velocities below 20 km/sec. Perhaps more striking is that this analysis seems to suggest a more cusped \"average\" halo shape that is shared by these galaxies. While this study reconciles many of the observed properties of the Milky Way satellite distribution with that of Lambda-CDM simulations, we find that there is still a deficit of satellites with maximum circular velocities of 20-40 km/sec."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat is the primary conclusion of the study regarding the distribution of Milky Way satellites, and how does it relate to the Lambda cold dark matter (Lambda-CDM) paradigm?\n\nA) The study suggests that the distribution of Milky Way satellites is inconsistent with the Lambda-CDM paradigm, and that the observed properties of the satellite distribution cannot be explained by Lambda-CDM simulations.\n\nB) The study finds that the distribution of Milky Way satellites is consistent with the Lambda-CDM paradigm, and that the observed properties of the satellite distribution match the values predicted by Lambda-CDM simulations for halos with maximum circular velocities below 20 km/sec.\n\nC) The study suggests that the distribution of Milky Way satellites is consistent with the Lambda-CDM paradigm, but that there is a deficit of satellites with maximum circular velocities of 20-40 km/sec that cannot be explained by Lambda-CDM simulations.\n\nD) The study finds that the distribution of Milky Way satellites is inconsistent with the Lambda-CDM paradigm, and that the observed properties of the satellite distribution are better explained by an alternative model, such as a modified Newtonian dynamics (MOND) model.\n\nCorrect Answer: B) The study finds that the distribution of Milky Way satellites is consistent with the Lambda-CDM paradigm, and that the observed properties of the satellite distribution match the values predicted by Lambda-CDM simulations for halos with maximum circular velocities below 20 km/sec.\n\nExplanation: The study concludes that the distribution of Milky Way satellites is consistent with the Lambda-CDM paradigm, and that the observed properties of the satellite distribution match the values predicted by Lambda-CDM simulations for halos with maximum circular velocities below 20 km/sec. This is evident from the text, which states that the study finds that \"the distribution of Milky Way satellites inferred by this analysis, with the exception of the apparent lack of high-mass haloes, is consistent with the Lambda-CDM paradigm.\""}, "51": {"documentation": {"title": "Role of Activity in Human Dynamics", "source": "Tao Zhou, Hoang Anh Tuan Kiet, Beom Jun Kim, Bing-Hong Wang, and\n  Petter Holme", "docs_id": "0711.4168", "section": ["physics.soc-ph", "physics.data-an"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Role of Activity in Human Dynamics. The human society is a very complex system; still, there are several non-trivial, general features. One type of them is the presence of power-law distributed quantities in temporal statistics. In this Letter, we focus on the origin of power-laws in rating of movies. We present a systematic empirical exploration of the time between two consecutive ratings of movies (the interevent time). At an aggregate level, we find a monotonous relation between the activity of individuals and the power-law exponent of the interevent-time distribution. At an individual level, we observe a heavy-tailed distribution for each user, as well as a negative correlation between the activity and the width of the distribution. We support these findings by a similar data set from mobile phone text-message communication. Our results demonstrate a significant role of the activity of individuals on the society-level patterns of human behavior. We believe this is a common character in the interest-driven human dynamics, corresponding to (but different from) the universality classes of task-driven dynamics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study \"Role of Activity in Human Dynamics\" investigate the relationship between individual activity and power-law distributed quantities in temporal statistics. What is the primary finding at the individual level, according to the study?\n\nA) A uniform distribution for each user, with no correlation between activity and rating frequency.\nB) A heavy-tailed distribution for each user, with a positive correlation between activity and rating frequency.\nC) A heavy-tailed distribution for each user, with a negative correlation between activity and rating frequency.\nD) A uniform distribution for each user, with a positive correlation between activity and rating frequency.\n\n**Correct Answer:** C) A heavy-tailed distribution for each user, with a negative correlation between activity and rating frequency.\n\n**Explanation:** The study finds that each user has a heavy-tailed distribution for their ratings, indicating that there are a few highly active users and many less active users. Additionally, the study observes a negative correlation between activity and the width of the distribution, meaning that more active users tend to have a narrower distribution of ratings. This suggests that the study's findings are consistent with the idea that individual activity plays a significant role in shaping the patterns of human behavior."}, "52": {"documentation": {"title": "A new method for estimation and model selection: $\\rho$-estimation", "source": "Yannick Baraud, Lucien Birg\\'e and Mathieu Sart", "docs_id": "1403.6057", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A new method for estimation and model selection: $\\rho$-estimation. The aim of this paper is to present a new estimation procedure that can be applied in many statistical frameworks including density and regression and which leads to both robust and optimal (or nearly optimal) estimators. In density estimation, they asymptotically coincide with the celebrated maximum likelihood estimators at least when the statistical model is regular enough and contains the true density to estimate. For very general models of densities, including non-compact ones, these estimators are robust with respect to the Hellinger distance and converge at optimal rate (up to a possible logarithmic factor) in all cases we know. In the regression setting, our approach improves upon the classical least squares from many aspects. In simple linear regression for example, it provides an estimation of the coefficients that are both robust to outliers and simultaneously rate-optimal (or nearly rate-optimal) for large class of error distributions including Gaussian, Laplace, Cauchy and uniform among others."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper on $\\rho$-estimation claim that their method is robust with respect to the Hellinger distance for estimating densities. However, they also mention that the estimators may not be optimal for non-compact models. What are the implications of this statement, and how does it relate to the concept of regularity in statistical models?\n\n**A)** The authors' method is only suitable for compact models, and its robustness is limited to these cases.\n**B)** The authors' method is robust with respect to the Hellinger distance, but its optimality is compromised for non-compact models.\n**C)** The authors' method is optimal for all statistical models, including non-compact ones, and its robustness is a secondary consideration.\n**D)** The authors' method is not robust with respect to the Hellinger distance, and its optimality is unaffected by the compactness of the model.\n\n**Correct Answer:** B) The authors' method is robust with respect to the Hellinger distance, but its optimality is compromised for non-compact models.\n\n**Explanation:** The statement in the documentation suggests that the authors' method is robust with respect to the Hellinger distance, but it also notes that the estimators may not be optimal for non-compact models. This implies that the method's robustness is limited to compact models, and its optimality is compromised for non-compact models. This requires the test-taker to carefully read the documentation and understand the nuances of the authors' claim."}, "53": {"documentation": {"title": "Lossy chaotic electromagnetic reverberation chambers: Universal\n  statistical behavior of the vectorial field", "source": "J.-B. Gros, U. Kuhl, O. Legrand, F. Mortessagne", "docs_id": "1509.06476", "section": ["cond-mat.mes-hall", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lossy chaotic electromagnetic reverberation chambers: Universal\n  statistical behavior of the vectorial field. The effective Hamiltonian formalism is extended to vectorial electromagnetic waves in order to describe statistical properties of the field in reverberation chambers. The latter are commonly used in electromagnetic compatibility tests. As a first step, the distribution of wave intensities in chaotic systems with varying opening in the weak coupling limit for scalar quantum waves is derived by means of random matrix theory. In this limit the only parameters are the modal overlap and the number of open channels. Using the extended effective Hamiltonian, we describe the intensity statistics of the vectorial electromagnetic eigenmodes of lossy reverberation chambers. Finally, the typical quantity of interest in such chambers, namely, the distribution of the electromagnetic response, is discussed. By determining the distribution of the phase rigidity, describing the coupling to the environment, using random matrix numerical data, we find good agreement between the theoretical prediction and numerical calculations of the response."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Lossy chaotic electromagnetic reverberation chambers: Universal statistical behavior of the vectorial field\" use the extended effective Hamiltonian formalism to describe the statistical properties of vectorial electromagnetic waves in lossy reverberation chambers. What is the primary assumption made in the derivation of the distribution of wave intensities in chaotic systems with varying opening in the weak coupling limit, and how does it relate to the parameters of the system?\n\nA) The primary assumption is that the system is in a stationary state, and the distribution of wave intensities is derived using the Boltzmann distribution. The parameters of the system are the modal overlap and the number of open channels.\n\nB) The primary assumption is that the system is in a non-stationary state, and the distribution of wave intensities is derived using the Fokker-Planck equation. The parameters of the system are the modal overlap and the number of open channels.\n\nC) The primary assumption is that the system is in a chaotic regime, and the distribution of wave intensities is derived using the random matrix theory. The parameters of the system are the modal overlap and the number of open channels.\n\nD) The primary assumption is that the system is in a quantum regime, and the distribution of wave intensities is derived using the Schr\u00f6dinger equation. The parameters of the system are the modal overlap and the number of open channels.\n\n**Correct Answer:** C) The primary assumption is that the system is in a chaotic regime, and the distribution of wave intensities is derived using the random matrix theory. The parameters of the system are the modal overlap and the number of open channels.\n\n**Explanation:** The correct answer is C) because the documentation states that the distribution of wave intensities in chaotic systems with varying opening in the weak coupling limit is derived using random matrix theory. The parameters of the system are indeed the modal overlap and the number of open channels, as mentioned in the documentation. The other options are incorrect because they do not accurately reflect the assumptions and parameters used in the derivation."}, "54": {"documentation": {"title": "Leading order CFT analysis of multi-scalar theories in d>2", "source": "Alessandro Codello, Mahmoud Safari, Gian Paolo Vacca, Omar Zanusso", "docs_id": "1809.05071", "section": ["hep-th", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Leading order CFT analysis of multi-scalar theories in d>2. We investigate multi-field multicritical scalar theories using CFT constraints on two- and three-point functions combined with the Schwinger-Dyson equation. This is done in general and without assuming any symmetry for the models, which we just define to admit a Landau-Ginzburg description that includes the most general critical interactions built from monomials of the form $\\phi_{i_1} \\cdots \\phi_{i_m}$. For all such models we analyze to the leading order of the $\\epsilon$-expansion the anomalous dimensions of the fields and those of the composite quadratic operators. For models with even $m$ we extend the analysis to an infinite tower of composite operators of arbitrary order. The results are supplemented by the computation of some families of structure constants. We also find the equations which constrain the nontrivial critical theories at leading order and show that they coincide with the ones obtained with functional perturbative RG methods. This is done for the case $m=3$ as well as for all the even models. We ultimately specialize to $S_q$ symmetric models, which are related to the $q$-state Potts universality class, and focus on three realizations appearing below the upper critical dimensions $6$, $4$ and $\\frac{10}{3}$, which can thus be nontrivial CFTs in three dimensions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of multi-scalar theories in d>2, what is the significance of the Landau-Ginzburg description, and how does it relate to the Schwinger-Dyson equation, particularly for models with even m?\n\nA) The Landau-Ginzburg description is a method for computing the anomalous dimensions of fields and composite operators, while the Schwinger-Dyson equation is a constraint on the nontrivial critical theories.\n\nB) The Landau-Ginzburg description is a way to extend the analysis to an infinite tower of composite operators of arbitrary order, while the Schwinger-Dyson equation is a method for computing the structure constants.\n\nC) The Landau-Ginzburg description is a general framework for multi-field multicritical scalar theories, while the Schwinger-Dyson equation is a specific constraint that arises from the CFT constraints on two- and three-point functions.\n\nD) The Landau-Ginzburg description is a method for computing the critical dimensions of models, while the Schwinger-Dyson equation is a constraint on the nontrivial critical theories that coincide with functional perturbative RG methods.\n\nCorrect Answer: C) The Landau-Ginzburg description is a general framework for multi-field multicritical scalar theories, while the Schwinger-Dyson equation is a specific constraint that arises from the CFT constraints on two- and three-point functions.\n\nExplanation: The correct answer is C) because the Landau-Ginzburg description is a general framework for multi-field multicritical scalar theories, which is stated in the documentation. The Schwinger-Dyson equation is a specific constraint that arises from the CFT constraints on two- and three-point functions, as mentioned in the documentation. The other options are incorrect because they either misrepresent the Landau-Ginzburg description or the Schwinger-Dyson equation."}, "55": {"documentation": {"title": "Estimated Correlation Matrices and Portfolio Optimization", "source": "Szilard Pafka, Imre Kondor", "docs_id": "cond-mat/0305475", "section": ["cond-mat.stat-mech", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimated Correlation Matrices and Portfolio Optimization. Financial correlations play a central role in financial theory and also in many practical applications. From theoretical point of view, the key interest is in a proper description of the structure and dynamics of correlations. From practical point of view, the emphasis is on the ability of the developed models to provide the adequate input for the numerous portfolio and risk management procedures used in the financial industry. This is crucial, since it has been long argued that correlation matrices determined from financial series contain a relatively large amount of noise and, in addition, most of the portfolio and risk management techniques used in practice can be quite sensitive to the inputs. In this paper we introduce a model (simulation)-based approach which can be used for a systematic investigation of the effect of the different sources of noise in financial correlations in the portfolio and risk management context. To illustrate the usefulness of this framework, we develop several toy models for the structure of correlations and, by considering the finiteness of the time series as the only source of noise, we compare the performance of several correlation matrix estimators introduced in the academic literature and which have since gained also a wide practical use. Based on this experience, we believe that our simulation-based approach can also be useful for the systematic investigation of several other problems of much interest in finance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of portfolio optimization, what is the primary concern when estimating correlation matrices from financial series, and how does the proposed simulation-based approach address this concern?\n\n**A)** The primary concern is that correlation matrices are too complex to be estimated accurately, and the simulation-based approach simplifies the structure of correlations to facilitate estimation.\n\n**B)** The primary concern is that correlation matrices contain a large amount of noise, and the simulation-based approach introduces additional noise sources to investigate the effect of noise on portfolio and risk management procedures.\n\n**C)** The primary concern is that correlation matrices are too sensitive to the inputs used in portfolio and risk management techniques, and the simulation-based approach provides a systematic investigation of the effect of different sources of noise on these inputs.\n\n**D)** The primary concern is that correlation matrices are too time-series-dependent, and the simulation-based approach assumes finiteness of the time series as the only source of noise to compare the performance of different correlation matrix estimators.\n\n**Correct Answer:** C) The primary concern is that correlation matrices are too sensitive to the inputs used in portfolio and risk management techniques, and the simulation-based approach provides a systematic investigation of the effect of different sources of noise on these inputs.\n\n**Explanation:** The correct answer is C) because the documentation states that \"most of the portfolio and risk management techniques used in practice can be quite sensitive to the inputs\" and that the proposed simulation-based approach aims to investigate the effect of different sources of noise on these inputs. This aligns with the concern that correlation matrices are sensitive to the inputs used in portfolio and risk management techniques."}, "56": {"documentation": {"title": "Off-Policy Multi-Agent Decomposed Policy Gradients", "source": "Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, Chongjie Zhang", "docs_id": "2007.12322", "section": ["cs.LG", "cs.MA", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Off-Policy Multi-Agent Decomposed Policy Gradients. Multi-agent policy gradient (MAPG) methods recently witness vigorous progress. However, there is a significant performance discrepancy between MAPG methods and state-of-the-art multi-agent value-based approaches. In this paper, we investigate causes that hinder the performance of MAPG algorithms and present a multi-agent decomposed policy gradient method (DOP). This method introduces the idea of value function decomposition into the multi-agent actor-critic framework. Based on this idea, DOP supports efficient off-policy learning and addresses the issue of centralized-decentralized mismatch and credit assignment in both discrete and continuous action spaces. We formally show that DOP critics have sufficient representational capability to guarantee convergence. In addition, empirical evaluations on the StarCraft II micromanagement benchmark and multi-agent particle environments demonstrate that DOP significantly outperforms both state-of-the-art value-based and policy-based multi-agent reinforcement learning algorithms. Demonstrative videos are available at https://sites.google.com/view/dop-mapg/."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary contribution of the proposed multi-agent decomposed policy gradient method (DOP) in addressing the performance discrepancy between multi-agent policy gradient (MAPG) methods and state-of-the-art multi-agent value-based approaches?\n\nA) DOP introduces a new centralized-decentralized architecture that enables efficient off-policy learning in both discrete and continuous action spaces.\nB) DOP decomposes the value function into separate components, allowing for more accurate credit assignment and convergence guarantees.\nC) DOP employs a novel actor-critic framework that leverages value function decomposition to overcome the issue of centralized-decentralized mismatch.\nD) DOP presents a new off-policy learning algorithm that enables MAPG methods to learn from experience without relying on a centralized critic.\n\nCorrect Answer: B) DOP decomposes the value function into separate components, allowing for more accurate credit assignment and convergence guarantees.\n\nExplanation: The correct answer is B) DOP decomposes the value function into separate components, allowing for more accurate credit assignment and convergence guarantees. This is because the documentation states that DOP introduces the idea of value function decomposition into the multi-agent actor-critic framework, which enables efficient off-policy learning and addresses the issue of centralized-decentralized mismatch and credit assignment in both discrete and continuous action spaces."}, "57": {"documentation": {"title": "The rise of science in low-carbon energy technologies", "source": "Kerstin H\\\"otte, Anton Pichler, Fran\\c{c}ois Lafond", "docs_id": "2004.09959", "section": ["cs.DL", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The rise of science in low-carbon energy technologies. Successfully combating climate change will require substantial technological improvements in Low-Carbon Energy Technologies (LCETs), but designing efficient allocation of R\\&D budgets requires a better understanding of how LCETs rely on scientific knowledge. Using data covering almost all US patents and scientific articles that are cited by them over the past two centuries, we describe the evolution of knowledge bases of ten key LCETs and show how technological interdependencies have changed over time. The composition of low-carbon energy innovations shifted over time, from Hydro and Wind energy in the 19th and early 20th century, to Nuclear fission after World War II, and more recently to Solar PV and back to Wind. In recent years, Solar PV, Nuclear fusion and Biofuels (including energy from waste) have 35-65\\% of their citations directed toward scientific papers, while this ratio is less than 10\\% for Wind, Solar thermal, Hydro, Geothermal, and Nuclear fission. Over time, the share of patents citing science and the share of citations that are to scientific papers has been increasing for all technology types. The analysis of the scientific knowledge base of each LCET reveals three fairly separate clusters, with nuclear energy technologies, Biofuels and Waste, and all the other LCETs. Our detailed description of knowledge requirements for each LCET helps to design of targeted innovation policies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the evolution of knowledge bases of Low-Carbon Energy Technologies (LCETs) over time, as described in the provided Arxiv documentation. How have technological interdependencies changed, and what insights can be gained from the increasing share of patents citing science and citations directed toward scientific papers?\n\n**A)** The evolution of knowledge bases of LCETs has been characterized by a shift from Hydro and Wind energy in the 19th and early 20th century to Nuclear fission after World War II, and more recently to Solar PV and back to Wind. This shift reflects changes in technological interdependencies, with Solar PV and Nuclear fusion emerging as key drivers of innovation in the field.\n\n**B)** The analysis of the scientific knowledge base of each LCET reveals three fairly separate clusters: nuclear energy technologies, Biofuels and Waste, and all the other LCETs. This suggests that LCETs have distinct knowledge requirements, which can inform the design of targeted innovation policies.\n\n**C)** The increasing share of patents citing science and citations directed toward scientific papers over time indicates a growing reliance on scientific knowledge in LCETs. This trend is particularly pronounced in Solar PV, Nuclear fusion, and Biofuels, which have 35-65% of their citations directed toward scientific papers.\n\n**D)** The composition of low-carbon energy innovations has shifted over time, reflecting changes in technological interdependencies and the increasing importance of scientific knowledge. The analysis of the scientific knowledge base of each LCET reveals a complex interplay between different technologies, with nuclear energy technologies, Biofuels and Waste, and all the other LCETs forming distinct clusters.\n\n**Correct Answer:** C) The increasing share of patents citing science and citations directed toward scientific papers over time indicates a growing reliance on scientific knowledge in LCETs. This trend is particularly pronounced in Solar PV, Nuclear fusion, and Biofuels, which have 35-65% of their citations directed toward scientific papers.\n\n**Explanation:** The correct answer highlights the key finding that the increasing share of patents citing science and citations directed toward scientific papers over time indicates a growing reliance on scientific knowledge in LCETs. This trend is particularly pronounced in Solar PV, Nuclear fusion, and Biofuels, which have a high proportion of citations directed toward scientific papers. This suggests that these technologies are increasingly dependent on scientific knowledge to drive innovation and progress."}, "58": {"documentation": {"title": "Adaptive Control of a Soft Continuum Manipulator", "source": "Amirhossein Kazemipour, Oliver Fischer, Yasunori Toshimitsu, Ki Wan\n  Wong, Robert K. Katzschmann", "docs_id": "2109.11388", "section": ["cs.RO", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adaptive Control of a Soft Continuum Manipulator. Soft robots are made of compliant and deformable materials and can perform tasks challenging for conventional rigid robots. The inherent compliance of soft robots makes them more suitable and adaptable for interactions with humans and the environment. However, this preeminence comes at a cost: their continuum nature makes it challenging to develop robust model-based control strategies. Specifically, an adaptive control approach addressing this challenge has not yet been applied to physical soft robotic arms. This work presents a reformulation of dynamics for a soft continuum manipulator using the Euler-Lagrange method. The proposed model eliminates the simplifying assumption made in previous works and provides a more accurate description of the robot's inertia. Based on our model, we introduce a task-space adaptive control scheme. This controller is robust against model parameter uncertainties and unknown input disturbances. The controller is implemented on a physical soft continuum arm. A series of experiments were carried out to validate the effectiveness of the controller in task-space trajectory tracking under different payloads. The controller outperforms the state-of-the-art method both in terms of accuracy and robustness. Moreover, the proposed model-based control design is flexible and can be generalized to any continuum robotic arm with an arbitrary number of continuum segments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using a soft continuum manipulator in tasks that require interaction with humans and the environment, and how does the proposed adaptive control scheme address the challenges associated with its continuum nature?\n\nA) Soft robots are more suitable for tasks that require high precision and accuracy, and the adaptive control scheme is designed to improve their performance in these areas.\nB) The inherent compliance of soft robots makes them more suitable for interactions with humans and the environment, and the proposed model-based control design eliminates the simplifying assumption made in previous works to provide a more accurate description of the robot's inertia.\nC) Soft robots are more suitable for tasks that require high speed and agility, and the adaptive control scheme is designed to improve their performance in these areas.\nD) The proposed adaptive control scheme is designed to improve the robustness of soft robots to model parameter uncertainties and unknown input disturbances, but it does not address the challenges associated with their continuum nature.\n\nCorrect Answer: B) The inherent compliance of soft robots makes them more suitable for interactions with humans and the environment, and the proposed model-based control design eliminates the simplifying assumption made in previous works to provide a more accurate description of the robot's inertia.\n\nExplanation: The question requires the test-taker to understand the advantages of soft continuum manipulators and how the proposed adaptive control scheme addresses the challenges associated with their continuum nature. The correct answer, B, highlights the primary advantage of soft robots (their compliance) and how the proposed model-based control design provides a more accurate description of the robot's inertia, which is a key challenge in controlling continuum robots. The other options are incorrect because they do not accurately capture the primary advantage of soft robots or the focus of the proposed adaptive control scheme."}, "59": {"documentation": {"title": "Pseudogap formation above the superconducting dome in iron-pnictides", "source": "T. Shimojima, T. Sonobe, W. Malaeb, K. Shinada, A. Chainani, S. Shin,\n  T. Yoshida, S. Ideta, A. Fujimori, H. Kumigashira, K Ono, Y. Nakashima, H.\n  Anzai, M. Arita, A. Ino, H. Namatame, M. Taniguchi, M. Nakajima, S. Uchida,\n  Y. Tomioka, T.Ito, K. Kihou, C. H. Lee, A. Iyo, H. Eisaki, K. Ohgushi, S.\n  Kasahara, T. Terashima, H. Ikeda, T. Shibauchi, Y. Matsuda and K. Ishizaka", "docs_id": "1305.3875", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pseudogap formation above the superconducting dome in iron-pnictides. The nature of the pseudogap in high transition temperature (high-Tc) superconducting cuprates has been a major issue in condensed matter physics. It is still unclear whether the high-Tc superconductivity can be universally associated with the pseudogap formation. Here we provide direct evidence of the existence of the pseudogap phase via angle-resolved photoemission spectroscopy in another family of high-Tc superconductor, iron-pnictides. Our results reveal a composition dependent pseudogap formation in the multi-band electronic structure of BaFe2(As1-xPx)2. The pseudogap develops well above the magnetostructural transition for low x, persists above the nonmagnetic superconducting dome for optimal x and is destroyed for x ~ 0.6, thus showing a notable similarity with cuprates. In addition, the pseudogap formation is accompanied by inequivalent energy shifts in xz/yz orbitals of iron atoms, indicative of a peculiar iron orbital ordering which breaks the four-fold rotational symmetry."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the angle-resolved photoemission spectroscopy results presented in the study on BaFe2(As1-xPx)2, and how does it relate to the nature of high-Tc superconductivity in iron-pnictides?\n\nA) The pseudogap formation in BaFe2(As1-xPx)2 is a direct result of the iron orbital ordering, which is a necessary condition for high-Tc superconductivity.\nB) The pseudogap formation in BaFe2(As1-xPx)2 is a universal feature of high-Tc superconductors, and its existence is a hallmark of the high-Tc superconducting dome.\nC) The pseudogap formation in BaFe2(As1-xPx)2 is accompanied by a peculiar iron orbital ordering that breaks the four-fold rotational symmetry, and this ordering is a key factor in the high-Tc superconductivity.\nD) The pseudogap formation in BaFe2(As1-xPx)2 is a composition-dependent phenomenon that is not directly related to the high-Tc superconductivity.\n\nCorrect Answer: C) The pseudogap formation in BaFe2(As1-xPx)2 is accompanied by a peculiar iron orbital ordering that breaks the four-fold rotational symmetry, and this ordering is a key factor in the high-Tc superconductivity.\n\nExplanation: The correct answer is C) because the study presents evidence of a composition-dependent pseudogap formation in BaFe2(As1-xPx)2, which is accompanied by inequivalent energy shifts in the xz/yz orbitals of iron atoms. This peculiar iron orbital ordering is a key factor in the high-Tc superconductivity, as it breaks the four-fold rotational symmetry. The other options are incorrect because they do not accurately reflect the primary conclusion drawn from the study. Option A is incorrect because the study does not suggest that the iron orbital ordering is a necessary condition for high-Tc superconductivity. Option B is incorrect because the study does not suggest that the pseudogap formation is a universal feature of high-Tc superconductors. Option D is incorrect because the study does suggest that the pseudogap formation is a composition-dependent phenomenon, but it does not suggest that it is not directly related to the high-Tc superconductivity."}}