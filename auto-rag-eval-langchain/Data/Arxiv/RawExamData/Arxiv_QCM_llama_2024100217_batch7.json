{"0": {"documentation": {"title": "Orthogonal Multiple Access with Correlated Sources: Feasible Region and\n  Pragmatic Schemes", "source": "A. Abrardo, G. Ferrari, M. Martalo', M. Franceschini, and R. Raheli", "docs_id": "1201.6548", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Orthogonal Multiple Access with Correlated Sources: Feasible Region and\n  Pragmatic Schemes. In this paper, we consider orthogonal multiple access coding schemes, where correlated sources are encoded in a distributed fashion and transmitted, through additive white Gaussian noise (AWGN) channels, to an access point (AP). At the AP, component decoders, associated with the source encoders, iteratively exchange soft information by taking into account the source correlation. The first goal of this paper is to investigate the ultimate achievable performance limits in terms of a multi-dimensional feasible region in the space of channel parameters, deriving insights on the impact of the number of sources. The second goal is the design of pragmatic schemes, where the sources use \"off-the-shelf\" channel codes. In order to analyze the performance of given coding schemes, we propose an extrinsic information transfer (EXIT)-based approach, which allows to determine the corresponding multi-dimensional feasible regions. On the basis of the proposed analytical framework, the performance of pragmatic coded schemes, based on serially concatenated convolutional codes (SCCCs), is discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of orthogonal multiple access coding schemes with correlated sources, what is the primary goal of the proposed extrinsic information transfer (EXIT)-based approach, and how does it relate to the design of pragmatic schemes?\n\n**A)** The primary goal is to determine the ultimate achievable performance limits in terms of a multi-dimensional feasible region in the space of channel parameters, and the approach is used to analyze the performance of given coding schemes.\n\n**B)** The primary goal is to design pragmatic schemes that use \"off-the-shelf\" channel codes, and the approach is used to determine the corresponding multi-dimensional feasible regions.\n\n**C)** The primary goal is to investigate the impact of the number of sources on the ultimate achievable performance limits, and the approach is used to analyze the performance of given coding schemes.\n\n**D)** The primary goal is to determine the ultimate achievable performance limits in terms of a multi-dimensional feasible region in the space of channel parameters, and the approach is used to design pragmatic schemes that use \"off-the-shelf\" channel codes.\n\n**Correct Answer:** D) The primary goal is to determine the ultimate achievable performance limits in terms of a multi-dimensional feasible region in the space of channel parameters, and the approach is used to design pragmatic schemes that use \"off-the-shelf\" channel codes.\n\n**Explanation:** The correct answer is D) because the primary goal of the proposed extrinsic information transfer (EXIT)-based approach is to determine the ultimate achievable performance limits in terms of a multi-dimensional feasible region in the space of channel parameters, and this approach is used to design pragmatic schemes that use \"off-the-shelf\" channel codes. The other options are incorrect because they either focus on a different aspect of the problem (A and C) or incorrectly state the primary goal of the approach (B)."}, "1": {"documentation": {"title": "Penalized and Decentralized Contextual Bandit Learning for WLAN Channel\n  Allocation with Contention-Driven Feature Extraction", "source": "Kota Yamashita, Shotaro Kamiya, Koji Yamamoto, Yusuke Koda, Takayuki\n  Nishio, Masahiro Morikura", "docs_id": "2003.10094", "section": ["cs.NI", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Penalized and Decentralized Contextual Bandit Learning for WLAN Channel\n  Allocation with Contention-Driven Feature Extraction. In this study, a contextual multi-armed bandit (CMAB)-based decentralized channel exploration framework disentangling a channel utility function (i.e., reward) with respect to contending neighboring access points (APs) is proposed. The proposed framework enables APs to evaluate observed rewards compositionally for contending APs, allowing both robustness against reward fluctuation due to neighboring APs' varying channels and assessment of even unexplored channels. To realize this framework, we propose contention-driven feature extraction (CDFE), which extracts the adjacency relation among APs under contention and forms the basis for expressing reward functions in the disentangled form, that is, a linear combination of parameters associated with neighboring APs under contention). This allows the CMAB to be leveraged with joint a linear upper confidence bound (JLinUCB) exploration and to delve into the effectiveness of the proposed framework. Moreover, we address the problem of non-convergence -- the channel exploration cycle -- by proposing a penalized JLinUCB (P-JLinUCB) based on the key idea of introducing a discount parameter to the reward for exploiting a different channel before and after the learning round. Numerical evaluations confirm that the proposed method allows APs to assess the channel quality robustly against reward fluctuations by CDFE and achieves better convergence properties by P-JLinUCB."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the proposed contention-driven feature extraction (CDFE) framework, what is the primary purpose of extracting the adjacency relation among access points (APs) under contention, and how does this relate to the disentangled reward function?\n\n**A)** CDFE extracts the channel quality of each AP, which is then used to update the reward function. This allows the CMAB to adapt to changing channel conditions.\n\n**B)** CDFE extracts the adjacency relation among APs under contention, which is used to express the reward function as a linear combination of parameters associated with neighboring APs under contention. This enables the CMAB to evaluate observed rewards compositionally.\n\n**C)** CDFE extracts the channel allocation strategy of each AP, which is then used to update the reward function. This allows the CMAB to optimize channel allocation for maximum throughput.\n\n**D)** CDFE extracts the power levels of each AP, which is then used to update the reward function. This allows the CMAB to optimize power allocation for maximum throughput.\n\n**Correct Answer:** B) CDFE extracts the adjacency relation among APs under contention, which is used to express the reward function as a linear combination of parameters associated with neighboring APs under contention. This enables the CMAB to evaluate observed rewards compositionally.\n\n**Explanation:** The correct answer is B) because the documentation states that the proposed framework enables APs to evaluate observed rewards compositionally for contending APs, allowing both robustness against reward fluctuation due to neighboring APs' varying channels and assessment of even unexplored channels. This is achieved through the extraction of the adjacency relation among APs under contention, which is used to express the reward function in a disentangled form."}, "2": {"documentation": {"title": "Neutron-antineutron oscillations from lattice QCD", "source": "Enrico Rinaldi and Sergey Syritsyn and Michael L. Wagman and Michael\n  I. Buchoff and Chris Schroeder and Joseph Wasem", "docs_id": "1809.00246", "section": ["hep-lat", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neutron-antineutron oscillations from lattice QCD. Fundamental symmetry tests of baryon number violation in low-energy experiments can probe beyond the Standard Model (BSM) explanations of the matter-antimatter asymmetry of the universe. Neutron-antineutron oscillations are predicted to be a signature of many baryogenesis mechanisms involving low-scale baryon number violation. This work presents first-principles calculations of neutron-antineutron matrix elements needed to accurately connect measurements of the neutron-antineutron oscillation rate to constraints on $|\\Delta B|=2$ baryon number violation in BSM theories. Several important systematic uncertainties are controlled by using a state-of-the-art lattice gauge field ensemble with physical quark masses and approximate chiral symmetry, performing nonperturbative renormalization with perturbative matching to the $\\overline{\\text{MS}}$ scheme, and studying excited state effects in two-state fits. Phenomenological implications are highlighted by comparing expected bounds from proposed neutron-antineutron oscillation experiments to predictions of a specific model of post-sphaleron baryogenesis. Quantum chromodynamics is found to predict at least an order of magnitude more events in neutron-antineutron oscillation experiments than previous estimates based on the \"MIT bag model\" for fixed BSM parameters. Lattice artifacts and other systematic uncertainties that are not controlled in this pioneering calculation are not expected to significantly change this conclusion."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of the lattice QCD calculations presented in the paper \"Neutron-antineutron oscillations from lattice QCD\" in terms of probing beyond the Standard Model (BSM) explanations of the matter-antimatter asymmetry of the universe?\n\nA) To constrain the parameter space of BSM theories\nB) To calculate the neutron-antineutron oscillation rate with high precision\nC) To study the phenomenological implications of post-sphaleron baryogenesis\nD) To control systematic uncertainties in lattice gauge field ensembles\n\nCorrect Answer: A) To constrain the parameter space of BSM theories\n\nExplanation: The paper aims to connect measurements of the neutron-antineutron oscillation rate to constraints on $|\\Delta B|=2$ baryon number violation in BSM theories, which is a fundamental symmetry test of baryon number violation. By doing so, the authors hope to constrain the parameter space of BSM theories and potentially shed light on the matter-antimatter asymmetry of the universe.\n\nCandidate B is incorrect because while the paper does present calculations of the neutron-antineutron oscillation rate, its primary goal is not to calculate it with high precision. Candidate C is incorrect because while the paper does study the phenomenological implications of post-sphaleron baryogenesis, this is not its primary goal. Candidate D is incorrect because controlling systematic uncertainties in lattice gauge field ensembles is a method used in the paper, but not its primary goal."}, "3": {"documentation": {"title": "Multi-wavelength carbon recombination line observations with the VLA\n  toward an UCHII region in W48: Physical properties and kinematics of neutral\n  material", "source": "D. Anish Roshi, W. M. Goss, K. R. Anantharamaiah, S. Jeyakumar", "docs_id": "astro-ph/0503063", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-wavelength carbon recombination line observations with the VLA\n  toward an UCHII region in W48: Physical properties and kinematics of neutral\n  material. Using the Very Large Array (VLA) the C76$\\alpha$ and C53$\\alpha$ recombination lines (RLs) have been detected toward the ultra-compact \\HII\\ region (UCHII region) G35.20$-$1.74. We also obtained upper limits to the carbon RLs at 6 cm (C110$\\alpha$ & C111$\\alpha$) and 3.6 cm (C92$\\alpha$) wavelengths with the VLA. In addition, continuum images of the W48A complex (which includes G35.20$-$1.74) are made with angular resolutions in the range 14\\arcsec to 2\\arcsec. Modeling the multi-wavelength line and continuum data has provided the physical properties of the UCHII region and the photodissociation region (PDR) responsible for the carbon RL emission. The gas pressure in the PDR, estimated using the derived physical properties, is at least four times larger than that in the UCHII region. The dominance of stimulated emission of carbon RLs near 2 cm, as implied by our models, is used to study the relative motion of the PDR with respect to the molecular cloud and ionized gas. Our results from the kinematical study are consistent with a pressure-confined UCHII region with the ionizing star moving with respect to the molecular cloud. However, based on the existing data, other models to explain the extended lifetime and morphology of UCHII regions cannot be ruled out."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the physical properties of the UCHII region and the photodissociation region (PDR) responsible for the carbon RL emission, based on the multi-wavelength line and continuum data?\n\nA) The gas pressure in the PDR is at least twice as large as that in the UCHII region.\nB) The gas pressure in the PDR is at least four times larger than that in the UCHII region.\nC) The gas pressure in the PDR is significantly larger than that in the UCHII region, but the exact ratio is unknown.\nD) The gas pressure in the PDR is comparable to that in the UCHII region.\n\nCorrect Answer: B) The gas pressure in the PDR is at least four times larger than that in the UCHII region.\n\nExplanation: The question requires the test-taker to analyze the information provided in the documentation and make an inference about the physical properties of the UCHII region and the PDR. The correct answer is supported by the text, which states that \"The gas pressure in the PDR, estimated using the derived physical properties, is at least four times larger than that in the UCHII region.\" The other options are incorrect because they either understate or misrepresent the information provided in the documentation."}, "4": {"documentation": {"title": "Description of $^{93}$Nb stellar electron-capture rates by the Projected\n  Shell Model", "source": "Long-Jun Wang, Liang Tan, Zhipan Li, Bingshui Gao, Yang Sun", "docs_id": "2111.14288", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Description of $^{93}$Nb stellar electron-capture rates by the Projected\n  Shell Model. Capture of electrons by nuclei is an important process in stellar environments where excited nuclear states are thermally populated. However, accurate treatment for excited configurations in electron capture (EC) rates has been an unsolved problem for medium-heavy and heavy nuclei. In this work, we take the $^{93}$Nb $\\rightarrow$ $^{93}$Zr EC rates as the example to introduce the Projected-Shell-Model (PSM) in which excited configurations are explicitly included as multi-quasiparticle states. Applying the prevalent assumption that the parent nucleus always stays in its ground state in stellar conditions, we critically compare the obtained PSM results with the recently-measured Gamow-Teller transition data, and with the previous calculations by the conventional shell model and the quasiparticle random-phase approximation. We discuss important ingredients that are required in theoretical models used for stellar EC calculations, and demonstrate effects of the explicit inclusion of excited nuclear states in EC rate calculations, especially when both electron density and environment temperature are high."}, "answer": "Here is the difficult multi-form question based on the provided documentation:\n\nWhat are the key differences between the Projected-Shell-Model (PSM) approach and the conventional shell model in calculating electron-capture (EC) rates for medium-heavy and heavy nuclei, as demonstrated in the study on $^{93}$Nb stellar electron-capture rates?\n\nA) The PSM approach assumes a lower electron density than the conventional shell model.\nB) The PSM approach explicitly includes excited nuclear states as multi-quasiparticle states, whereas the conventional shell model does not.\nC) The PSM approach uses a different temperature-dependent assumption than the conventional shell model.\nD) The PSM approach requires a higher environmental temperature than the conventional shell model.\n\nCorrect Answer: B) The PSM approach explicitly includes excited nuclear states as multi-quasiparticle states, whereas the conventional shell model does not.\n\nExplanation: The question requires the test-taker to understand the key differences between the PSM approach and the conventional shell model in calculating EC rates. The correct answer, B, highlights the explicit inclusion of excited nuclear states in the PSM approach, which is a crucial aspect of the study. The other options are incorrect because they do not accurately reflect the differences between the two approaches. Option A is incorrect because the PSM approach does not assume a lower electron density. Option C is incorrect because the temperature-dependent assumption is not explicitly mentioned in the study. Option D is incorrect because the environmental temperature is not a key factor in the comparison between the two approaches."}, "5": {"documentation": {"title": "Spectroscopic study of some diatomic molecules via the proper\n  quantization rule", "source": "Babatunde J. Falaye, Sameer M. Ikhdair, Majid Hamzavi", "docs_id": "1503.09091", "section": ["physics.chem-ph", "math-ph", "math.MP", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spectroscopic study of some diatomic molecules via the proper\n  quantization rule. Spectroscopic techniques are very essential tools in studying electronic structures, spectroscopic constants and energetic properties of diatomic molecules. These techniques are also required for parametrization of new method based on theoretical analysis and computational calculations. In this research, we apply the proper quantization rule in spectroscopic study of some diatomic molecules by solving the Schr\\\"odinger equation with two solvable quantum molecular systems-Tietz-Wei and shifted Deng-Fan potential models for their approximate nonrelativistic energy states via an appropriate approximation to the centrifugal term. We show that the energy levels can be determined from its ground state energy. The beauty and simplicity of the method applied in this study is that, it can be applied to any exactly as well as approximately solvable models. The validity and accuracy of the method is tested with previous techniques via numerical computation for H$_2$ and CO diatomic molecules. The result also include energy spectrum of 5 different electronic states of NO and 2 different electronic state of ICl."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proper quantization rule applied in the spectroscopic study of diatomic molecules, as described in the research paper?\n\nA) It allows for the calculation of energy levels only for exactly solvable models.\nB) It enables the determination of energy levels from the ground state energy of any exactly or approximately solvable model.\nC) It provides a more accurate method for calculating energy levels compared to previous techniques.\nD) It is limited to only non-relativistic energy states.\n\n**Correct Answer:** B) It enables the determination of energy levels from the ground state energy of any exactly or approximately solvable model.\n\n**Explanation:** The correct answer is B) because the research paper states that the beauty and simplicity of the method applied is that it can be applied to any exactly as well as approximately solvable models, and that energy levels can be determined from its ground state energy. This highlights the flexibility and universality of the proper quantization rule in spectroscopic studies of diatomic molecules."}, "6": {"documentation": {"title": "Tidal disruption events from a nuclear star cluster as possible origin\n  of transient relativistic spectral lines near SMBH", "source": "V. Karas, M. Dovciak, D. Kunneriath, W. Yu, W. Zhang", "docs_id": "1409.3746", "section": ["astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tidal disruption events from a nuclear star cluster as possible origin\n  of transient relativistic spectral lines near SMBH. We discuss a possibility that a tidal disruption event near a dormant supermassive black hole (SMBH) can give rise to spectral features of iron in 6-7 keV X-ray signal: a relativistic line profile emerges from debris illuminated and ionised by an intense flash produced from the destroyed star. This could provide a unique way to determine parameters of the system. We consider a model where the nuclear stellar population acquires an oblate shape (i.e., a flattened distribution) in the inner region near a supermassive black hole, and also the primary irradiation flare is expected to occur more likely near the equatorial plane, co-planar with the infalling material. This suggests that the reprocessing of primary X-rays results in a transient profile that should be relevant for tidal-disruption events (TDE) in otherwise under-luminous (inactive) galactic nuclei, i.e. with no prior accretion disc. Resonance mechanisms of the stellar motion can increase the orbital eccentricity for some stars in the nuclear cluster and help to bring them close to the tidal radius, where they can give rise to TDEs. The proposed scenario appears to be distinguishably different from the standard scheme of lamp-post model for the origin of the relativistic line in active galaxies, where the source is thought to be located predominantly near the symmetry axis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which the reprocessing of primary X-rays occurs in the context of a tidal disruption event near a dormant supermassive black hole, and how does this differ from the standard lamp-post model for the origin of relativistic lines in active galaxies?\n\nA) Resonance mechanisms of stellar motion increase the orbital eccentricity of stars in the nuclear cluster, leading to a more efficient reprocessing of primary X-rays near the equatorial plane.\nB) The primary irradiation flare occurs near the symmetry axis, resulting in a more efficient reprocessing of primary X-rays and a distinct relativistic line profile.\nC) The nuclear stellar population acquires an oblate shape in the inner region near the supermassive black hole, leading to a more efficient reprocessing of primary X-rays near the equatorial plane.\nD) The reprocessing of primary X-rays occurs through a process of gravitational lensing, which is distinct from the standard lamp-post model.\n\nCorrect Answer: C) The nuclear stellar population acquires an oblate shape in the inner region near the supermassive black hole, leading to a more efficient reprocessing of primary X-rays near the equatorial plane.\n\nExplanation: The correct answer is C) because the documentation states that the nuclear stellar population acquires an oblate shape in the inner region near the supermassive black hole, which leads to a more efficient reprocessing of primary X-rays near the equatorial plane. This is a key aspect of the proposed scenario, and it differs from the standard lamp-post model, which is mentioned in the context of the incorrect answer B."}, "7": {"documentation": {"title": "Planar Prior Assisted PatchMatch Multi-View Stereo", "source": "Qingshan Xu and Wenbing Tao", "docs_id": "1912.11744", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Planar Prior Assisted PatchMatch Multi-View Stereo. The completeness of 3D models is still a challenging problem in multi-view stereo (MVS) due to the unreliable photometric consistency in low-textured areas. Since low-textured areas usually exhibit strong planarity, planar models are advantageous to the depth estimation of low-textured areas. On the other hand, PatchMatch multi-view stereo is very efficient for its sampling and propagation scheme. By taking advantage of planar models and PatchMatch multi-view stereo, we propose a planar prior assisted PatchMatch multi-view stereo framework in this paper. In detail, we utilize a probabilistic graphical model to embed planar models into PatchMatch multi-view stereo and contribute a novel multi-view aggregated matching cost. This novel cost takes both photometric consistency and planar compatibility into consideration, making it suited for the depth estimation of both non-planar and planar regions. Experimental results demonstrate that our method can efficiently recover the depth information of extremely low-textured areas, thus obtaining high complete 3D models and achieving state-of-the-art performance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of utilizing planar models in the proposed planar prior assisted PatchMatch multi-view stereo framework, particularly in low-textured areas?\n\nA) Planar models can improve the accuracy of depth estimation in non-planar regions.\nB) Planar models can efficiently reduce the computational complexity of the PatchMatch algorithm.\nC) Planar models can effectively capture the photometric consistency and planar compatibility in low-textured areas.\nD) Planar models can only be used for depth estimation in areas with strong planarity.\n\nCorrect Answer: C) Planar models can effectively capture the photometric consistency and planar compatibility in low-textured areas.\n\nExplanation: The correct answer is C) Planar models can effectively capture the photometric consistency and planar compatibility in low-textured areas. This is because the proposed framework utilizes a probabilistic graphical model to embed planar models into PatchMatch multi-view stereo, which allows it to consider both photometric consistency and planar compatibility when estimating depth information. This is particularly important in low-textured areas, where photometric consistency is often unreliable."}, "8": {"documentation": {"title": "Penalized estimation in high-dimensional hidden Markov models with\n  state-specific graphical models", "source": "Nicolas St\\\"adler, Sach Mukherjee", "docs_id": "1208.4989", "section": ["stat.ME", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Penalized estimation in high-dimensional hidden Markov models with\n  state-specific graphical models. We consider penalized estimation in hidden Markov models (HMMs) with multivariate Normal observations. In the moderate-to-large dimensional setting, estimation for HMMs remains challenging in practice, due to several concerns arising from the hidden nature of the states. We address these concerns by $\\ell_1$-penalization of state-specific inverse covariance matrices. Penalized estimation leads to sparse inverse covariance matrices which can be interpreted as state-specific conditional independence graphs. Penalization is nontrivial in this latent variable setting; we propose a penalty that automatically adapts to the number of states $K$ and the state-specific sample sizes and can cope with scaling issues arising from the unknown states. The methodology is adaptive and very general, applying in particular to both low- and high-dimensional settings without requiring hand tuning. Furthermore, our approach facilitates exploration of the number of states $K$ by coupling estimation for successive candidate values $K$. Empirical results on simulated examples demonstrate the effectiveness of the proposed approach. In a challenging real data example from genome biology, we demonstrate the ability of our approach to yield gains in predictive power and to deliver richer estimates than existing methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of penalized estimation in high-dimensional hidden Markov models with state-specific graphical models, what is the primary concern that arises from the hidden nature of the states, and how does the proposed approach address this concern?\n\n**A)** The primary concern is the curse of dimensionality, and the proposed approach addresses this by using a penalty that automatically adapts to the number of states and the state-specific sample sizes.\n\n**B)** The primary concern is the non-identifiability of the model parameters, and the proposed approach addresses this by using a penalty that facilitates exploration of the number of states K.\n\n**C)** The primary concern is the computational complexity of estimation, and the proposed approach addresses this by using a sparse inverse covariance matrix that can be interpreted as state-specific conditional independence graphs.\n\n**D)** The primary concern is the lack of prior knowledge about the state-specific inverse covariance matrices, and the proposed approach addresses this by using a penalty that can cope with scaling issues arising from the unknown states.\n\n**Correct Answer:** C) The primary concern is the computational complexity of estimation, and the proposed approach addresses this by using a sparse inverse covariance matrix that can be interpreted as state-specific conditional independence graphs.\n\n**Explanation:** The correct answer is C) because the documentation states that \"estimation for HMMs remains challenging in practice, due to several concerns arising from the hidden nature of the states.\" One of these concerns is the computational complexity of estimation, which is addressed by the proposed approach using a sparse inverse covariance matrix. This matrix can be interpreted as a state-specific conditional independence graph, which provides a more interpretable and efficient way of modeling the relationships between variables."}, "9": {"documentation": {"title": "Increased accuracy of ligand sensing by receptor diffusion on cell\n  surface", "source": "Gerardo Aquino and Robert G. Endres", "docs_id": "1008.5171", "section": ["q-bio.SC", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Increased accuracy of ligand sensing by receptor diffusion on cell\n  surface. The physical limit with which a cell senses external ligand concentration corresponds to the perfect absorber, where all ligand particles are absorbed and overcounting of same ligand particles does not occur. Here we analyze how the lateral diffusion of receptors on the cell membrane affects the accuracy of sensing ligand concentration. Specifically, we connect our modeling to neurotransmission in neural synapses where the diffusion of glutamate receptors is already known to refresh synaptic connections. We find that receptor diffusion indeed increases the accuracy of sensing for both the glutamate AMPA and NDMA receptors, although the NMDA receptor is overall much noiser. We propose that the difference in accuracy of sensing of the two receptors can be linked to their different roles in neurotransmission. Specifically, the high accuracy in sensing glutamate is essential for the AMPA receptor to start membrane depolarization, while the NMDA receptor is believed to work in a second stage as a coincidence detector, involved in long-term potentiation and memory."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the effect of receptor diffusion on the accuracy of ligand sensing in cell surface receptors. How do the different roles of AMPA and NMDA receptors in neurotransmission influence their sensing accuracy?\n\n**A)** Receptor diffusion increases the accuracy of sensing for both AMPA and NMDA receptors, as it allows for more efficient ligand binding and reduces overcounting of ligand particles.\n\n**B)** Receptor diffusion has no effect on the accuracy of sensing for AMPA receptors, but increases the accuracy for NMDA receptors due to their role in coincidence detection.\n\n**C)** The accuracy of sensing for AMPA receptors is increased by receptor diffusion, while the accuracy for NMDA receptors is decreased due to the increased noise introduced by diffusion.\n\n**D)** Receptor diffusion only affects the accuracy of sensing for NMDA receptors, which require a higher level of accuracy for their role in long-term potentiation and memory.\n\n**Correct Answer:** C) The accuracy of sensing for AMPA receptors is increased by receptor diffusion, while the accuracy for NMDA receptors is decreased due to the increased noise introduced by diffusion.\n\n**Explanation:** The correct answer is C) because the text states that receptor diffusion increases the accuracy of sensing for both AMPA and NMDA receptors, but also notes that the NMDA receptor is overall much noiser. This suggests that while diffusion increases accuracy, it also introduces more noise, which is more pronounced in the case of the NMDA receptor. The text also implies that the high accuracy in sensing glutamate is essential for the AMPA receptor to start membrane depolarization, which supports the idea that diffusion increases accuracy for AMPA receptors."}, "10": {"documentation": {"title": "A common origin for 3/4- and 2/3-power rules in metabolic scaling", "source": "Jinkui Zhao", "docs_id": "1509.08912", "section": ["physics.bio-ph", "q-bio.OT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A common origin for 3/4- and 2/3-power rules in metabolic scaling. A central debate in biology has been the allometric scaling of metabolic rate. Kleiber's observation that animals' basal metabolic rate scales to the 3/4-power of body mass (Kleiber's rule) has been the prevailing hypothesis in the last eight decades. Increasingly, more evidences are supporting the alternative 2/3-power scaling rule, especially for smaller animals. The 2/3-rule dates back to before Kleiber's time and was thought to originate from the surface to volume relationship in Euclidean geometry. In this study, we show that both the 3/4- and 2/3-scaling rules have in fact one common origin. They are governed by animals' nutrient supply networks-their vascular systems that obey Murray's law. Murray's law describes the branching pattern of energy optimized vascular network under laminar flow. It is generally regarded as being closely followed by blood vessels. Our analysis agrees with experimental observations and recent numerical analyses that showed a curvature in metabolic scaling. When applied to metabolic data, our model accurately produces the observed 2/3-scaling rule for small animals of ~10 kg or less and the 3/4-rule for all animals excluding the smallest ones (~15 g). The model has broad implications to the ongoing debate. It proves that both the 3/4- and 2/3-exponents are phenomenological approximations of the same scaling rule within their applicable mass ranges, and that the 2/3-rule does not originate from the classical surface law."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the underlying mechanism that governs the scaling of metabolic rate in animals, and how does it relate to the 3/4- and 2/3-power rules?\n\nA) The 3/4- and 2/3-power rules are governed by the surface area of an animal's body, which is proportional to the cube of its linear dimensions.\n\nB) The 3/4- and 2/3-power rules are governed by the branching pattern of an animal's vascular system, which is optimized for laminar flow and follows Murray's law.\n\nC) The 3/4- and 2/3-power rules are governed by the animal's muscle mass, which is proportional to the cube of its body mass.\n\nD) The 3/4- and 2/3-power rules are governed by the animal's surface-to-volume ratio, which is a classical concept in physics.\n\n**Correct Answer:** B) The 3/4- and 2/3-power rules are governed by the branching pattern of an animal's vascular system, which is optimized for laminar flow and follows Murray's law.\n\n**Explanation:** The study suggests that both the 3/4- and 2/3-power rules are governed by the same underlying mechanism, which is the branching pattern of an animal's vascular system. This system is optimized for laminar flow and follows Murray's law, which describes the branching pattern of energy-optimized vascular networks. This explanation is supported by the fact that the model accurately produces the observed 2/3-scaling rule for small animals and the 3/4-rule for larger animals, and that it agrees with experimental observations and recent numerical analyses."}, "11": {"documentation": {"title": "Electrical Control of Broadband Terahertz Wave Transmission with\n  Two-Terminal Graphene Oxide Devices", "source": "Seungwoo Lee, Kyung Eun Lee, Won Jun Lee, Byung Cheol Park, Byungsoo\n  Kang, Euyheon Hwang, Sang Ouk Kim", "docs_id": "1508.02813", "section": ["physics.optics", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electrical Control of Broadband Terahertz Wave Transmission with\n  Two-Terminal Graphene Oxide Devices. Carbon nanomaterials such as carbon nanotubes and graphene have proved to be efficient building blocks for active optoelectronic devices. Especially, the exotic properties of crystalline graphene, such as a linear/gapless energy dispersion, offer a generic route to the development of active photonic modulator at the infrared (IR) and terahertz (THz) regime with large modulation depth. Here, we show that graphene oxide (GO), an oxygenated derivative of graphene with randomly distributed molecular defects (e.g., adsorbed water molecules and punched holes), can provide a different way to effectively control broadband THz transmission amplitude, when incorporated into two-terminal electrode devices. Electrically trapped charge carriers within localized impurity states (LIS) of GO, which originate from fully randomized defective structure of GO, results in a large modulation of transmission amplitude (~30%) for broadband THz waves (0.3 ~ 2.0 THz) even at room temperature. Interesting hysteretic behavior observed in the control of broadband THz transmission further confirms the key role of trapped charge carriers in switching of broadband THz waves. The device architecture constructed by simple solution printing of GO onto the two-terminal electrode enables easy-to-implement active photonic devices."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which graphene oxide (GO) devices control broadband terahertz wave transmission amplitude, and what is the significance of the observed hysteretic behavior?\n\nA) Electrically trapped charge carriers within localized impurity states (LIS) of GO modulate transmission amplitude due to the random distribution of molecular defects, leading to a large modulation depth of ~30% for broadband THz waves.\n\nB) The hysteretic behavior observed in the control of broadband THz transmission is a result of the thermal expansion of the GO device, causing a change in the transmission amplitude.\n\nC) The device architecture constructed by simple solution printing of GO onto the two-terminal electrode enables easy-to-implement active photonic devices, but does not explain the mechanism of transmission amplitude modulation.\n\nD) The large modulation depth of ~30% for broadband THz waves is a result of the piezoelectric properties of GO, which are activated by the electric field.\n\nCorrect Answer: A) Electrically trapped charge carriers within localized impurity states (LIS) of GO modulate transmission amplitude due to the random distribution of molecular defects, leading to a large modulation depth of ~30% for broadband THz waves.\n\nExplanation: The correct answer is A) because the documentation states that electrically trapped charge carriers within localized impurity states (LIS) of GO, which originate from the fully randomized defective structure of GO, result in a large modulation of transmission amplitude (~30%) for broadband THz waves. The hysteretic behavior observed in the control of broadband THz transmission further confirms the key role of trapped charge carriers in switching of broadband THz waves."}, "12": {"documentation": {"title": "Angular momentum evolution of young low-mass stars and brown dwarfs:\n  observations and theory", "source": "J. Bouvier, S.P. Matt, S. Mohanty, A. Scholz, K.G. Stassun, C. Zanni", "docs_id": "1309.7851", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Angular momentum evolution of young low-mass stars and brown dwarfs:\n  observations and theory. This chapter aims at providing the most complete review of both the emerging concepts and the latest observational results regarding the angular momentum evolution of young low-mass stars and brown dwarfs. In the time since Protostars & Planets V, there have been major developments in the availability of rotation period measurements at multiple ages and in different star-forming environments that are essential for testing theory. In parallel, substantial theoretical developments have been carried out in the last few years, including the physics of the star-disk interaction, numerical simulations of stellar winds, and the investigation of angular momentum transport processes in stellar interiors. This chapter reviews both the recent observational and theoretical advances that prompted the development of renewed angular momentum evolution models for cool stars and brown dwarfs. While the main observational trends of the rotational history of low mass objects seem to be accounted for by these new models, a number of critical open issues remain that are outlined in this review."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the main developments in the availability of rotation period measurements that have been essential for testing theory regarding the angular momentum evolution of young low-mass stars and brown dwarfs?\n\n**A)** The development of new observational techniques that allow for more accurate measurements of rotation periods at multiple ages.\n**B)** The availability of rotation period measurements at multiple ages and in different star-forming environments, which has enabled the testing of theoretical models.\n**C)** The use of numerical simulations to model the effects of stellar winds on rotation periods.\n**D)** The development of new theoretical models that predict the effects of stellar winds on rotation periods.\n\n**Correct Answer:** B) The availability of rotation period measurements at multiple ages and in different star-forming environments, which has enabled the testing of theoretical models.\n\n**Explanation:** The correct answer is B) because the documentation states that \"In the time since Protostars & Planets V, there have been major developments in the availability of rotation period measurements at multiple ages and in different star-forming environments that are essential for testing theory.\" This indicates that the availability of rotation period measurements at multiple ages and in different environments has been a crucial development in testing theoretical models of angular momentum evolution."}, "13": {"documentation": {"title": "Capital adequacy tests and limited liability of financial institutions", "source": "Pablo Koch-Medina, Santiago Moreno-Bromberg, Cosimo Munari", "docs_id": "1401.3133", "section": ["q-fin.RM", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Capital adequacy tests and limited liability of financial institutions. The theory of acceptance sets and their associated risk measures plays a key role in the design of capital adequacy tests. The objective of this paper is to investigate, in the context of bounded financial positions, the class of surplus-invariant acceptance sets. These are characterized by the fact that acceptability does not depend on the positive part, or surplus, of a capital position. We argue that surplus invariance is a reasonable requirement from a regulatory perspective, because it focuses on the interests of liability holders of a financial institution. We provide a dual characterization of surplus-invariant, convex acceptance sets, and show that the combination of surplus invariance and coherence leads to a narrow range of capital adequacy tests, essentially limited to scenario-based tests. Finally, we emphasize the advantages of dealing with surplus-invariant acceptance sets as the primary object rather than directly with risk measures, such as loss-based and excess-invariant risk measures, which have been recently studied by Cont, Deguest, and He (2013) and by Staum (2013), respectively."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of the paper \"Capital adequacy tests and limited liability of financial institutions\" in the context of bounded financial positions, and what is the main advantage of dealing with surplus-invariant acceptance sets?\n\n**A)** The paper aims to investigate the class of surplus-invariant acceptance sets to design capital adequacy tests that focus on the interests of liability holders, and the main advantage is that it provides a dual characterization of these sets.\n\n**B)** The paper aims to investigate the class of surplus-invariant acceptance sets to design capital adequacy tests that focus on the interests of liability holders, and the main advantage is that it provides a narrow range of capital adequacy tests, essentially limited to scenario-based tests.\n\n**C)** The paper aims to investigate the class of surplus-invariant acceptance sets to design capital adequacy tests that focus on the interests of liability holders, and the main advantage is that it provides a coherent and risk-measure-based approach to capital adequacy tests.\n\n**D)** The paper aims to investigate the class of surplus-invariant acceptance sets to design capital adequacy tests that focus on the interests of liability holders, and the main advantage is that it provides a limited range of capital adequacy tests, essentially limited to scenario-based tests.\n\n**Correct Answer:** B) The paper aims to investigate the class of surplus-invariant acceptance sets to design capital adequacy tests that focus on the interests of liability holders, and the main advantage is that it provides a narrow range of capital adequacy tests, essentially limited to scenario-based tests.\n\n**Explanation:** The correct answer is B) because the paper aims to investigate the class of surplus-invariant acceptance sets to design capital adequacy tests that focus on the interests of liability holders, and the main advantage is that it provides a narrow range of capital adequacy tests, essentially limited to scenario-based tests. This is stated in the introduction of the paper: \"We argue that surplus invariance is a reasonable requirement from a regulatory perspective, because it focuses on the interests of liability holders of a financial institution. We provide a dual characterization of surplus-invariant, convex acceptance sets, and show that the combination of surplus invariance and coherence leads to a narrow range of capital adequacy tests, essentially limited to scenario-based tests.\""}, "14": {"documentation": {"title": "Dynamics of soliton self-injection locking in a photonic chip-based\n  microresonator", "source": "Andrey S. Voloshin (1,2), Nikita M. Kondratiev (1), Grigory V.\n  Lihachev (2), Junqiu Liu (2), Valery E. Lobanov (1,3), Nikita Yu. Dmitriev\n  (1,4), Wenle Weng (2), Tobias J. Kippenberg (2), and Igor A. Bilenko (1,5)\n  ((1) Russian Quantum Center, (2) Institute of Physics, Swiss Federal\n  Institute of Technology Lausanne (EPFL), (3) National University of Science\n  and Technology (MISiS), (4) Moscow Institute of Physics and Technology\n  (MIPT), (5) Faculty of Physics, M.V. Lomonosov Moscow State University)", "docs_id": "1912.11303", "section": ["physics.optics", "nlin.PS", "physics.app-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamics of soliton self-injection locking in a photonic chip-based\n  microresonator. Soliton microcombs constitute chip-scale optical frequency combs, and have the potential to impact a myriad of applications from frequency synthesis and telecommunications to astronomy. The requirement on external driving lasers has been significantly relaxed with the demonstration of soliton formation via self-injection locking of the pump laser to the microresonator. Yet to date, the dynamics of this process has not been fully understood. Prior models of self-injection locking were not able to explain sufficiently large detunings, crucial for soliton formation. Here we develop a theoretical model of self-injection locking to a nonlinear microresonator (nonlinear self-injection locking) for the first time and show that self- and cross-phase modulation of the clockwise and counter-clockwise light enables soliton formation. Using an integrated soliton microcomb of directly detectable 30 GHz repetition rate, consisting of a DFB laser self-injection-locked to a Si3N4 microresonator chip, we study the soliton formation dynamics via self-injection locking, as well as the repetition rate evolution, experimentally. We reveal that Kerr nonlinearity in microresonator significantly modifies locking dynamics, making laser emission frequency red detuned. We propose and implement a novel technique for measurements of the nonlinear frequency tuning curve and concurrent observation of microcomb states switching in real time."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of prior models of self-injection locking in nonlinear microresonators, and how does the proposed model address this limitation?\n\nA) Prior models were unable to account for the effects of Kerr nonlinearity, which is a crucial factor in soliton formation. The proposed model incorporates Kerr nonlinearity to explain the red detuning of laser emission frequency.\n\nB) Prior models were unable to explain the dynamics of self-injection locking at high repetition rates. The proposed model uses an integrated soliton microcomb with a 30 GHz repetition rate to study the dynamics of self-injection locking.\n\nC) Prior models were unable to account for the effects of cross-phase modulation, which is a key mechanism for soliton formation. The proposed model includes cross-phase modulation to explain the dynamics of self-injection locking.\n\nD) Prior models were unable to explain the effects of microresonator material on the locking dynamics. The proposed model uses a Si3N4 microresonator chip to study the effects of microresonator material on the locking dynamics.\n\n**Correct Answer:** A) Prior models were unable to account for the effects of Kerr nonlinearity, which is a crucial factor in soliton formation. The proposed model incorporates Kerr nonlinearity to explain the red detuning of laser emission frequency.\n\n**Explanation:** The question requires the test-taker to understand the limitations of prior models of self-injection locking and how the proposed model addresses these limitations. The correct answer, A, highlights the importance of Kerr nonlinearity in explaining the red detuning of laser emission frequency, which is a key factor in soliton formation. The other options, while related to the topic, do not accurately capture the primary limitation of prior models and the key feature of the proposed model."}, "15": {"documentation": {"title": "Artificial Intelligence Enabled Wireless Networking for 5G and Beyond:\n  Recent Advances and Future Challenges", "source": "Cheng-Xiang Wang, Marco Di Renzo, Slawomir Sta\\'nczak, Sen Wang and\n  Erik G. Larsson", "docs_id": "2001.08159", "section": ["cs.NI", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Artificial Intelligence Enabled Wireless Networking for 5G and Beyond:\n  Recent Advances and Future Challenges. The fifth generation (5G) wireless communication networks are currently being deployed, and beyond 5G (B5G) networks are expected to be developed over the next decade. Artificial intelligence (AI) technologies and, in particular, machine learning (ML) have the potential to efficiently solve the unstructured and seemingly intractable problems by involving large amounts of data that need to be dealt with in B5G. This article studies how AI and ML can be leveraged for the design and operation of B5G networks. We first provide a comprehensive survey of recent advances and future challenges that result from bringing AI/ML technologies into B5G wireless networks. Our survey touches different aspects of wireless network design and optimization, including channel measurements, modeling, and estimation, physical-layer research, and network management and optimization. Then, ML algorithms and applications to B5G networks are reviewed, followed by an overview of standard developments of applying AI/ML algorithms to B5G networks. We conclude this study by the future challenges on applying AI/ML to B5G networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the primary benefits of leveraging Artificial Intelligence (AI) and Machine Learning (ML) technologies in the design and operation of Beyond 5G (B5G) wireless networks, as discussed in the article \"Artificial Intelligence Enabled Wireless Networking for 5G and Beyond: Recent Advances and Future Challenges\"?\n\n**A)** AI and ML can improve network security by detecting and mitigating cyber threats in real-time.\n**B)** AI and ML can efficiently solve unstructured and seemingly intractable problems by involving large amounts of data, enabling the development of more efficient and effective wireless networks.\n**C)** AI and ML can enhance network management and optimization by predicting and preventing network congestion and downtime.\n**D)** AI and ML can improve network performance by optimizing channel measurements, modeling, and estimation, as well as physical-layer research.\n\n**Correct Answer:** B) AI and ML can efficiently solve unstructured and seemingly intractable problems by involving large amounts of data, enabling the development of more efficient and effective wireless networks.\n\n**Explanation:** The article highlights the potential of AI and ML to efficiently solve complex problems in B5G wireless networks by leveraging large amounts of data. This is evident in the introduction, where it is stated that AI and ML have the potential to \"efficiently solve the unstructured and seemingly intractable problems\" in B5G networks. Therefore, option B is the correct answer. Options A, C, and D are incorrect because while AI and ML may have some benefits in these areas, they are not the primary benefits discussed in the article."}, "16": {"documentation": {"title": "Conditional out-of-sample generation for unpaired data using trVAE", "source": "Mohammad Lotfollahi, Mohsen Naghipourfar, Fabian J. Theis, F.\n  Alexander Wolf", "docs_id": "1910.01791", "section": ["cs.LG", "eess.IV", "q-bio.CB", "q-bio.GN", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Conditional out-of-sample generation for unpaired data using trVAE. While generative models have shown great success in generating high-dimensional samples conditional on low-dimensional descriptors (learning e.g. stroke thickness in MNIST, hair color in CelebA, or speaker identity in Wavenet), their generation out-of-sample poses fundamental problems. The conditional variational autoencoder (CVAE) as a simple conditional generative model does not explicitly relate conditions during training and, hence, has no incentive of learning a compact joint distribution across conditions. We overcome this limitation by matching their distributions using maximum mean discrepancy (MMD) in the decoder layer that follows the bottleneck. This introduces a strong regularization both for reconstructing samples within the same condition and for transforming samples across conditions, resulting in much improved generalization. We refer to the architecture as \\emph{transformer} VAE (trVAE). Benchmarking trVAE on high-dimensional image and tabular data, we demonstrate higher robustness and higher accuracy than existing approaches. In particular, we show qualitatively improved predictions for cellular perturbation response to treatment and disease based on high-dimensional single-cell gene expression data, by tackling previously problematic minority classes and multiple conditions. For generic tasks, we improve Pearson correlations of high-dimensional estimated means and variances with their ground truths from 0.89 to 0.97 and 0.75 to 0.87, respectively."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of the Conditional Variational Autoencoder (CVAE) in generating samples out-of-sample, and how does the transformer VAE (trVAE) architecture address this limitation?\n\n**A)** CVAE has no incentive to learn a compact joint distribution across conditions because it does not explicitly relate conditions during training. trVAE addresses this limitation by introducing a strong regularization through maximum mean discrepancy (MMD) in the decoder layer.\n\n**B)** CVAE has no incentive to learn a compact joint distribution across conditions because it does not have enough training data. trVAE addresses this limitation by using a larger dataset and a more complex architecture.\n\n**C)** CVAE has no incentive to learn a compact joint distribution across conditions because it is not designed to handle high-dimensional data. trVAE addresses this limitation by using a bottleneck layer to reduce the dimensionality of the data.\n\n**D)** CVAE has no incentive to learn a compact joint distribution across conditions because it is not a generative model. trVAE addresses this limitation by using a generative model architecture that can learn a compact joint distribution across conditions.\n\n**Correct Answer:** A) CVAE has no incentive to learn a compact joint distribution across conditions because it does not explicitly relate conditions during training. trVAE addresses this limitation by introducing a strong regularization through maximum mean discrepancy (MMD) in the decoder layer.\n\n**Explanation:** The question requires the test-taker to understand the limitations of the CVAE and how the trVAE architecture addresses this limitation. The correct answer, A, is supported by the text, which states that the CVAE does not explicitly relate conditions during training, resulting in no incentive to learn a compact joint distribution across conditions. The trVAE architecture addresses this limitation by introducing a strong regularization through MMD in the decoder layer. The other options are incorrect because they do not accurately reflect the limitations of the CVAE or the solution provided by the trVAE architecture."}, "17": {"documentation": {"title": "Surface Brightness Profiles of Galactic Globular Clusters from Hubble\n  Space Telescope Images", "source": "E. Noyola and K. Gebhardt (University of Texas at Austin)", "docs_id": "astro-ph/0604251", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Surface Brightness Profiles of Galactic Globular Clusters from Hubble\n  Space Telescope Images. Hubble Space Telescope allows us to study the central surface brightness profiles for globular clusters at unprecedented detail. We have mined the HST archives to obtain 38 WFPC2 images of galactic globular clusters with adequate exposure times and filters, which we use to measure their central structure. We outline a reliable method to obtain surface brightness profiles from integrated light that we test on an extensive set of simulated images. Most clusters have central surface brightness about 0.5 mag brighter than previous measurements made from ground-based data, with the largest differences around 2 magnitudes. Including the uncertainties in the slope estimates, the surface brightness slope distribution is consistent with half of the sample having flat cores and the remaining half showing a gradual decline from 0 to -0.8 (dlog(Sigma)/dlogr). We deproject the surface brightness profiles in a non-parametric way to obtain luminosity density profiles. The distribution of luminosity density logarithmic slopes show similar features with half of the sample between -0.4 and -1.8. These results are in contrast to our theoretical bias that the central regions of globular clusters are either isothermal (i.e. flat central profiles) or very steep (i.e. luminosity density slope ~-1.6) for core-collapse clusters. With only 50% of our sample having central profiles consistent with isothermal cores, King models appear to poorly represent most globular clusters in their cores."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the central surface brightness profiles of galactic globular clusters from Hubble Space Telescope images, and how do these findings challenge theoretical expectations?\n\nA) Most clusters have central surface brightness profiles consistent with isothermal cores, and the remaining half show a gradual decline from 0 to -0.8 (dlog(Sigma)/dlogr).\n\nB) The largest differences in central surface brightness measurements are due to instrumental errors, and the surface brightness slope distribution is consistent with half of the sample having flat cores and the remaining half showing a gradual decline from 0 to -0.8 (dlog(Sigma)/dlogr).\n\nC) The distribution of luminosity density logarithmic slopes is consistent with half of the sample having central profiles consistent with isothermal cores, and the remaining half show a gradual decline from 0 to -0.8 (dlog(Sigma)/dlogr).\n\nD) The surface brightness profiles of globular clusters are consistent with King models, and the central regions of most clusters are either isothermal (i.e. flat central profiles) or very steep (i.e. luminosity density slope ~-1.6) for core-collapse clusters.\n\nCorrect Answer: B) The largest differences in central surface brightness measurements are due to instrumental errors, and the surface brightness slope distribution is consistent with half of the sample having flat cores and the remaining half showing a gradual decline from 0 to -0.8 (dlog(Sigma)/dlogr).\n\nExplanation: The correct answer is B) because the text states that \"Most clusters have central surface brightness about 0.5 mag brighter than previous measurements made from ground-based data, with the largest differences around 2 magnitudes.\" This suggests that the differences in central surface brightness measurements are due to instrumental errors, rather than any underlying physical processes. Additionally, the text states that the surface brightness slope distribution is consistent with half of the sample having flat cores and the remaining half showing a gradual decline from 0 to -0.8 (dlog(Sigma)/dlogr), which is consistent with option B."}, "18": {"documentation": {"title": "Vote Delegation and Misbehavior", "source": "Hans Gersbach, Akaki Mamageishvili, Manvir Schneider", "docs_id": "2102.08823", "section": ["cs.GT", "econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Vote Delegation and Misbehavior. We study vote delegation with \"well-behaving\" and \"misbehaving\" agents and compare it with conventional voting. Typical examples for vote delegation are validation or governance tasks on blockchains. There is a majority of well-behaving agents, but they may abstain or delegate their vote to other agents since voting is costly. Misbehaving agents always vote. We compare conventional voting allowing for abstention with vote delegation. Preferences of voters are private information and a positive outcome is achieved if well-behaving agents win. We illustrate that vote delegation leads to quite different outcomes than conventional voting with abstention. In particular, we obtain three insights: First, if the number of misbehaving voters, denoted by f , is high, both voting methods fail to deliver a positive outcome. Second, if f takes an intermediate value, conventional voting delivers a positive outcome, while vote delegation fails with probability one. Third, if f is low, delegation delivers a positive outcome with higher probability than conventional voting. Finally, our results characterize worst-case outcomes that can happen in a liquid democracy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of vote delegation and misbehavior, compare the outcomes of conventional voting with abstention and vote delegation for a system with a majority of well-behaving agents, where misbehaving agents always vote and preferences of voters are private information.\n\n**A)** Conventional voting with abstention is more likely to deliver a positive outcome than vote delegation, especially when the number of misbehaving voters is low.\n\n**B)** Vote delegation is more likely to deliver a positive outcome than conventional voting with abstention, especially when the number of misbehaving voters is high.\n\n**C)** The outcomes of conventional voting with abstention and vote delegation are equivalent, regardless of the number of misbehaving voters.\n\n**D)** Conventional voting with abstention is more likely to deliver a positive outcome than vote delegation when the number of misbehaving voters is intermediate, but vote delegation is more likely to deliver a positive outcome when the number of misbehaving voters is low.\n\n**Correct Answer:** D) Conventional voting with abstention is more likely to deliver a positive outcome than vote delegation when the number of misbehaving voters is intermediate, but vote delegation is more likely to deliver a positive outcome when the number of misbehaving voters is low.\n\n**Explanation:** According to the documentation, when the number of misbehaving voters (f) is high, both voting methods fail to deliver a positive outcome. However, when f takes an intermediate value, conventional voting delivers a positive outcome, while vote delegation fails with probability one. When f is low, delegation delivers a positive outcome with higher probability than conventional voting. Therefore, the correct answer is D, which accurately summarizes these findings."}, "19": {"documentation": {"title": "Iteration of composition operators on small Bergman spaces of Dirichlet\n  series", "source": "Jing Zhao", "docs_id": "1705.05743", "section": ["math.CV", "math.NT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Iteration of composition operators on small Bergman spaces of Dirichlet\n  series. The Hilbert spaces $\\mathscr{H}_{w}$ consisiting of Dirichlet series $F(s)=\\sum_{ n = 1}^\\infty a_n n^{ -s }$ that satisfty $\\sum_{ n=1 }^\\infty | a_n |^2/ w_n < \\infty$, with $\\{w_n\\}_n$ of average order $\\log_j n$ (the $j$-fold logarithm of $n$), can be embedded into certain small Bergman spaces. Using this embedding, we study the Gordon--Hedenmalm theorem on such $\\mathscr{H}_w$ from an iterative point of view. By that theorem, the composition operators are generated by functions of the form $\\Phi(s) = c_0s + \\phi(s)$, where $c_0$ is a nonnegative integer and $\\phi$ is a Dirichlet series with certain convergence and mapping properties. The iterative phenomenon takes place when $c_0=0$. It is verified for every integer $j\\geqslant 1$, real $\\alpha>0$ and $\\{w_n\\}_{n}$ having average order $(\\log_j^+ n)^\\alpha$ , that the composition operators map $\\mathscr{H}_w$ into a scale of $\\mathscr{H}_{w'}$ with $w_n'$ having average order $( \\log_{j+1}^+n)^\\alpha$. The case $j=1$ can be deduced from the proof of the main theorem of a recent paper of Bailleul and Brevig, and we adopt the same method to study the general iterative step."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Let $\\mathscr{H}_w$ be a Hilbert space consisting of Dirichlet series $F(s) = \\sum_{n=1}^\\infty a_n n^{-s}$ that satisfy $\\sum_{n=1}^\\infty |a_n|^2 / w_n < \\infty$, where $\\{w_n\\}_n$ has average order $\\log_j n$. Suppose that the composition operators on $\\mathscr{H}_w$ are generated by functions of the form $\\Phi(s) = c_0s + \\phi(s)$, where $c_0$ is a nonnegative integer and $\\phi$ is a Dirichlet series with certain convergence and mapping properties.\n\n**A)** If $c_0 = 0$, then the composition operators map $\\mathscr{H}_w$ into a scale of $\\mathscr{H}_{w'}$ with $w_n'$ having average order $(\\log_{j+1}^+n)^\\alpha$, where $\\alpha > 0$.\n\n**B)** The Gordon-Hedenmalm theorem implies that the composition operators on $\\mathscr{H}_w$ are bounded operators.\n\n**C)** If $\\{w_n\\}_n$ has average order $(\\log_j^+ n)^\\alpha$, then the composition operators map $\\mathscr{H}_w$ into a scale of $\\mathscr{H}_{w'}$ with $w_n'$ having average order $(\\log_{j+1}^+n)^\\alpha$.\n\n**D)** The iterative phenomenon for the composition operators on $\\mathscr{H}_w$ can be deduced from the proof of the main theorem of a recent paper of Bailleul and Brevig.\n\n**Correct Answer:** C) If $\\{w_n\\}_n$ has average order $(\\log_j^+ n)^\\alpha$, then the composition operators map $\\mathscr{H}_w$ into a scale of $\\mathscr{H}_{w'}$ with $w_n'$ having average order $(\\log_{j+1}^+n)^\\alpha$.\n\n**Explanation:** The correct answer is C) because the iterative phenomenon for the composition operators on $\\mathscr{H}_w$ is verified for every integer $j\\geqslant 1$, real $\\alpha>0$ and $\\{w_n\\}_{n}$ having average order $(\\log_j^+ n)^\\alpha$. This is stated in the problem statement, and the correct answer is a direct consequence of this statement. The other options are incorrect because they either make an incorrect assumption about the composition operators (A), make a general statement that is not supported by the problem statement (B), or make a statement that is not supported by the problem statement (D)."}, "20": {"documentation": {"title": "Interface engineering of graphene nanosheet reinforced ZrB$_2$\n  composites by tuning surface contacts", "source": "Yanhui Zhang, Stefano Sanvito", "docs_id": "1904.09008", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interface engineering of graphene nanosheet reinforced ZrB$_2$\n  composites by tuning surface contacts. The mechanical properties of heterophase interfaces are critically important for the behaviour of graphene-reinforced composites. In this work, the structure, adhesion, cleavage and sliding of heterophase interfaces, formed between a ZrB$_2$ matrix and graphene nanosheets, are systematically investigated by density functional theory, and compared to available experimental data. We demonstrate that the surface chemistry of the ZrB$_2$ matrix material largely shapes the interface structures (of either Zr-C-Zr or B-C-B type) and the nature of the interfacial interaction. The Zr-C-Zr interfaces present strong chemical bonding and their response to mechanical stress is significantly influenced by graphene corrugation. In contrast B-C-B interfaces, interacting through the relatively weak $\\pi$-$\\pi$ stacking, show attributes similar to 2D materials heterostructures. Our theoretical results provide insights into the interface bonding mechanisms in graphene/ceramic composites, and emphasize the prospect for their design via interface engineering enabled by surface contacts."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary factor that influences the structure and adhesion of heterophase interfaces in graphene-reinforced ZrB$_2$ composites?\n\nA){The type of graphene nanosheet used}\nB){The surface chemistry of the ZrB$_2$ matrix material}\nC){The mechanical stress applied to the composite}\nD){The temperature at which the composite is formed}\n\nCorrect Answer: B){The surface chemistry of the ZrB$_2$ matrix material}\n\nExplanation: According to the documentation, the surface chemistry of the ZrB$_2$ matrix material is the primary factor that shapes the interface structures (of either Zr-C-Zr or B-C-B type) and the nature of the interfacial interaction. This is evident from the statement \"We demonstrate that the surface chemistry of the ZrB$_2$ matrix material largely shapes the interface structures (of either Zr-C-Zr or B-C-B type) and the nature of the interfacial interaction.\"\n\nCandidate A is incorrect because the type of graphene nanosheet used may affect the properties of the composite, but it is not the primary factor that influences the structure and adhesion of heterophase interfaces.\n\nCandidate C is incorrect because mechanical stress may affect the response of the interface to stress, but it is not the primary factor that shapes the interface structures and adhesion.\n\nCandidate D is incorrect because the temperature at which the composite is formed may affect its properties, but it is not directly related to the structure and adhesion of heterophase interfaces."}, "21": {"documentation": {"title": "How initial distribution affects symmetry breaking induced by panic in\n  ants: experiment and flee-pheromone model", "source": "Geng Li, Weijia Wang, Jiahui Lin, Zhiyang Huang, Jianqiang Liang,\n  Huabo Wu, Jianping Wen, Zengru Di, Bertrand Roehner, and Zhangang Han", "docs_id": "2006.02396", "section": ["q-bio.QM", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "How initial distribution affects symmetry breaking induced by panic in\n  ants: experiment and flee-pheromone model. Collective escaping is a ubiquitous phenomenon in animal groups. Symmetry breaking caused by panic escape exhibits a shared feature across species that one exit is used more than the other when agents escaping from a closed space with two symmetrically located exists. Intuitively, one exit will be used more by more individuals close to it, namely there is an asymmetric distribution initially. We used ant groups to investigate how initial distribution of colonies would influence symmetry breaking in collective escaping. Surprisingly, there was no positive correlation between symmetry breaking and the asymmetrically initial distribution, which was quite counter-intuitive. In the experiments, a flee stage was observed and accordingly a flee-pheromone model was introduced to depict this special behavior in the early stage of escaping. Simulation results fitted well with the experiment. Furthermore, the flee stage duration was calibrated quantitatively and the model reproduced the observation demonstrated by our previous work. This paper explicitly distinguished two stages in ant panic escaping for the first time, thus enhancing the understanding in escaping behavior of ant colonies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** How does the initial distribution of ants in a colony affect the symmetry breaking induced by panic in collective escaping, and what implications does this have for our understanding of ant panic escaping behavior?\n\n**A)** The initial distribution of ants has no effect on symmetry breaking, and the flee stage is a universal phenomenon that occurs regardless of the initial distribution.\n\n**B)** The initial distribution of ants has a positive correlation with symmetry breaking, and the more asymmetric the initial distribution, the more likely it is that one exit will be used more than the other.\n\n**C)** The initial distribution of ants has a negative correlation with symmetry breaking, and the more symmetric the initial distribution, the more likely it is that one exit will be used more than the other.\n\n**D)** The initial distribution of ants does not affect the duration of the flee stage, but rather influences the probability of an individual ant using a particular exit.\n\n**Correct Answer:** C) The initial distribution of ants has a negative correlation with symmetry breaking, and the more symmetric the initial distribution, the more likely it is that one exit will be used more than the other.\n\n**Explanation:** The correct answer is C) because the documentation states that \"there was no positive correlation between symmetry breaking and the asymmetrically initial distribution, which was quite counter-intuitive.\" This suggests that the initial distribution of ants actually has a negative correlation with symmetry breaking, meaning that a more symmetric initial distribution leads to more symmetry breaking. This is because the intuitive expectation is that an asymmetric initial distribution would lead to more symmetry breaking, but the actual results show the opposite."}, "22": {"documentation": {"title": "Is the Chiral Magnetic Effect fast enough?", "source": "Jewel K. Ghosh, Sebastian Grieninger, Karl Landsteiner, Sergio\n  Morales-Tejera", "docs_id": "2105.05855", "section": ["hep-ph", "cond-mat.str-el", "hep-th", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Is the Chiral Magnetic Effect fast enough?. It depends: While we find within holography that the lifetime of the magnetic field for collider energies like the ones achieved at RHIC is long enough to build up the chiral magnetic current, the lifetime of the magnetic field at LHC seems to be too short. We study the real time evolution of the chiral magnetic effect out-of-equilibrium in strongly coupled holographic gauge theories. We consider the backreaction of the magnetic field onto the geometry and monitor pressure and chiral magnetic current. Our findings show that generically at small magnetic field the pressure builds up faster than the chiral magnetic current whereas at strong magnetic field the opposite is true. At large charge we also find that equilibration is delayed significantly due to long lived oscillations. We also match the parameters of our model to QCD parameters and draw lessons of possible relevance to the realization of the chiral magnetic effect in heavy ion collisions. In particular, we find an equilibration time of about $\\sim0.35$ fm/c in presence of the chiral anomaly for plasma temperatures of order $T\\sim300-400$ MeV."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Chiral Magnetic Effect, what is the expected behavior of the pressure and chiral magnetic current in strongly coupled holographic gauge theories at small and large magnetic fields, and how does this relate to the equilibration time of the system?\n\n**A)** At small magnetic fields, the pressure builds up faster than the chiral magnetic current, while at large magnetic fields, the opposite is true. Equilibration occurs rapidly due to the short lifetime of the magnetic field. \n**B)** At small magnetic fields, the chiral magnetic current builds up faster than the pressure, while at large magnetic fields, the opposite is true. Equilibration occurs rapidly due to the short lifetime of the magnetic field. \n**C)** At small magnetic fields, the pressure builds up faster than the chiral magnetic current, while at large magnetic fields, the pressure builds up slower than the chiral magnetic current. Equilibration occurs rapidly due to the short lifetime of the magnetic field. \n**D)** At small magnetic fields, the chiral magnetic current builds up faster than the pressure, while at large magnetic fields, the pressure builds up faster than the chiral magnetic current. Equilibration occurs rapidly due to the short lifetime of the magnetic field.\n\n**Correct Answer:** C) At small magnetic fields, the pressure builds up faster than the chiral magnetic current, while at large magnetic fields, the opposite is true. Equilibration occurs rapidly due to the short lifetime of the magnetic field.\n\n**Explanation:** The correct answer is based on the findings of the study, which states that \"generically at small magnetic field the pressure builds up faster than the chiral magnetic current whereas at strong magnetic field the opposite is true\". Additionally, the study mentions that \"at large charge we also find that equilibration is delayed significantly due to long lived oscillations\", which implies that equilibration occurs rapidly due to the short lifetime of the magnetic field."}, "23": {"documentation": {"title": "Signatures of $\\alpha$ clustering in ultra-relativistic collisions with\n  light nuclei", "source": "Maciej Rybczy\\'nski, Milena Piotrowska, Wojciech Broniowski", "docs_id": "1711.00438", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Signatures of $\\alpha$ clustering in ultra-relativistic collisions with\n  light nuclei. We explore possible observable signatures of $\\alpha$ clustering of light nuclei in ultra-relativistic nuclear collisions involving ${}^{7,9}$Be, ${}^{12}$C, and ${}^{16}$O. The clustering leads to specific spatial correlations of the nucleon distributions in the ground state, which are manifest in the earliest stage of the ultra-high energy reaction. The formed initial state of the fireball is sensitive to these correlations, and the effect influences, after the collective evolution of the system, the hadron production in the final stage. Specifically, we study effects on the harmonic flow in collisions of light clustered nuclei with a heavy target (${}^{208}$Pb), showing that measures of the elliptic flow are sensitive to clusterization in ${}^{7,9}$Be, whereas triangular flow is sensitive to clusterization in ${}^{12}$C and ${}^{16}$O. Specific predictions are made for model collisions at the CERN SPS energies. In another exploratory development we also examine the proton-beryllium collisions, where the $3/2^-$ ground state of ${}^{7,9}$Be nucleus is polarized by an external magnetic field. Clusterization leads to multiplicity distributions of participant nucleons which depend on the orientation of the polarization with respect to the collision axis, as well as on the magnetic number of the state. The obtained effects on multiplicities reach a factor of a few for collisions with a large number of participant nucleons."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In ultra-relativistic nuclear collisions, what is the predicted effect of $\\alpha$ clustering on the harmonic flow in collisions of light clustered nuclei with a heavy target (${}^{208}$Pb), and how does this effect manifest in the earliest stage of the reaction?\n\nA) $\\alpha$ clustering leads to an increase in triangular flow, which is sensitive to clusterization in ${}^{12}$C and ${}^{16}$O, and a decrease in elliptic flow, which is sensitive to clusterization in ${}^{7,9}$Be.\n\nB) $\\alpha$ clustering leads to a decrease in triangular flow, which is sensitive to clusterization in ${}^{12}$C and ${}^{16}$O, and an increase in elliptic flow, which is sensitive to clusterization in ${}^{7,9}$Be.\n\nC) $\\alpha$ clustering leads to a decrease in elliptic flow, which is sensitive to clusterization in ${}^{7,9}$Be, and an increase in triangular flow, which is sensitive to clusterization in ${}^{12}$C and ${}^{16}$O.\n\nD) $\\alpha$ clustering leads to an increase in elliptic flow, which is sensitive to clusterization in ${}^{7,9}$Be, and a decrease in triangular flow, which is sensitive to clusterization in ${}^{12}$C and ${}^{16}$O.\n\nCorrect Answer: B) $\\alpha$ clustering leads to a decrease in triangular flow, which is sensitive to clusterization in ${}^{12}$C and ${}^{16}$O, and an increase in elliptic flow, which is sensitive to clusterization in ${}^{7,9}$Be.\n\nExplanation: According to the documentation, triangular flow is sensitive to clusterization in ${}^{12}$C and ${}^{16}$O, while elliptic flow is sensitive to clusterization in ${}^{7,9}$Be. Therefore, $\\alpha$ clustering leads to a decrease in triangular flow and an increase in elliptic flow."}, "24": {"documentation": {"title": "Heterotic/type II Duality and Non-Geometric Compactifications", "source": "Yoan Gautier, Chris M. Hull, Dan Isra\\\"el", "docs_id": "1906.02165", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Heterotic/type II Duality and Non-Geometric Compactifications. We present a new class of dualities relating non-geometric Calabi-Yau compactifications of type II string theory to T-fold compactifications of the heterotic string, both preserving four-dimensional $\\mathcal{N}=2$ supersymmetry. The non-geometric Calabi-Yau space is a $K3$ fibration over $T^2$ with non-geometric monodromies in the duality group $O(\\Gamma_{4,20})$; this is dual to a heterotic reduction on a $T^4$ fibration over $T^2$ with the $O(\\Gamma_{4,20})$ monodromies now viewed as heterotic T-dualities. At a point in moduli space which is a minimum of the scalar potential, the type II compactification becomes an asymmetric Gepner model and the monodromies become automorphisms involving mirror symmetries, while the heterotic dual is an asymmetric toroidal orbifold. We generalise previous constructions to ones in which the automorphisms are not of prime order. The type II construction is perturbatively consistent, but the naive heterotic dual is not modular invariant. Modular invariance on the heterotic side is achieved by including twists in the circles dual to the winding numbers round the $T^2$, and this in turn introduces non-perturbative phases depending on NS5-brane charge in the type II construction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a non-geometric Calabi-Yau compactification of type II string theory, which is a $K3$ fibration over $T^2$ with non-geometric monodromies in the duality group $O(\\Gamma_{4,20})$. This compactification is dual to a heterotic reduction on a $T^4$ fibration over $T^2$, where the $O(\\Gamma_{4,20})$ monodromies are viewed as heterotic T-dualities.\n\nA) The heterotic dual is an asymmetric toroidal orbifold, and the type II compactification is an asymmetric Gepner model, with the monodromies becoming automorphisms involving mirror symmetries.\n\nB) The heterotic dual is an asymmetric toroidal orbifold, and the type II compactification is an asymmetric Gepner model, with the monodromies becoming automorphisms involving mirror symmetries, but the heterotic dual is not modular invariant.\n\nC) The heterotic dual is an asymmetric toroidal orbifold, and the type II compactification is an asymmetric Gepner model, with the monodromies becoming automorphisms involving mirror symmetries, but the heterotic dual is modular invariant.\n\nD) The heterotic dual is an asymmetric toroidal orbifold, and the type II compactification is an asymmetric Gepner model, with the monodromies becoming automorphisms involving mirror symmetries, but the heterotic dual is not modular invariant, and the type II compactification is not perturbatively consistent.\n\nCorrect Answer: C) The heterotic dual is an asymmetric toroidal orbifold, and the type II compactification is an asymmetric Gepner model, with the monodromies becoming automorphisms involving mirror symmetries, but the heterotic dual is modular invariant.\n\nExplanation: The correct answer is C) because the documentation states that at a point in moduli space which is a minimum of the scalar potential, the type II compactification becomes an asymmetric Gepner model and the monodromies become automorphisms involving mirror symmetries, while the heterotic dual is an asymmetric toroidal orbifold. Additionally, the documentation mentions that modular invariance on the heterotic side is achieved by including twists in the circles dual to the winding numbers round the $T^2$, which implies that the heterotic dual is modular invariant. Therefore, option C) is the correct answer."}, "25": {"documentation": {"title": "Automatic Backward Filtering Forward Guiding for Markov processes and\n  graphical models", "source": "Frank van der Meulen and Moritz Schauer", "docs_id": "2010.03509", "section": ["stat.CO", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Automatic Backward Filtering Forward Guiding for Markov processes and\n  graphical models. We incorporate discrete and continuous time Markov processes as building blocks into probabilistic graphical models with latent and observed variables. We introduce the automatic Backward Filtering Forward Guiding (BFFG) paradigm (Mider et al., 2020) for programmable inference on latent states and model parameters. Our starting point is a generative model, a forward description of the probabilistic process dynamics. We backpropagate the information provided by observations through the model to transform the generative (forward) model into a pre-conditional model guided by the data. It approximates the actual conditional model with known likelihood-ratio between the two. The backward filter and the forward change of measure are suitable to be incorporated into a probabilistic programming context because they can be formulated as a set of transformation rules. The guided generative model can be incorporated in different approaches to efficiently sample latent states and parameters conditional on observations. We show applicability in a variety of settings, including Markov chains with discrete state space, interacting particle systems, state space models, branching diffusions and Gamma processes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the Backward Filtering Forward Guiding (BFFG) paradigm, and how does it relate to the transformation of a generative model into a pre-conditional model guided by the data?\n\n**A)** The primary goal of BFFG is to approximate the conditional model with known likelihood-ratio between the two, and it achieves this by transforming the generative model into a pre-conditional model guided by the data.\n\n**B)** The primary goal of BFFG is to efficiently sample latent states and parameters conditional on observations, and it achieves this by incorporating the backward filter and the forward change of measure into a probabilistic programming context.\n\n**C)** The primary goal of BFFG is to incorporate discrete and continuous time Markov processes as building blocks into probabilistic graphical models, and it achieves this by introducing a new paradigm for programmable inference on latent states and model parameters.\n\n**D)** The primary goal of BFFG is to transform the generative model into a pre-conditional model guided by the data, and it achieves this by backpropagating the information provided by observations through the model.\n\n**Correct Answer:** D) The primary goal of BFFG is to transform the generative model into a pre-conditional model guided by the data, and it achieves this by backpropagating the information provided by observations through the model.\n\n**Explanation:** The correct answer is D) because the BFFG paradigm is described in the documentation as transforming a generative model into a pre-conditional model guided by the data through backpropagation of the information provided by observations. This is the primary goal of the BFFG paradigm, as stated in the introduction of the paper. The other options are incorrect because they either describe secondary aspects of the BFFG paradigm (B) or are unrelated to the primary goal of the paradigm (A and C)."}, "26": {"documentation": {"title": "Virtual Delamination Testing through Non-Linear Multi-Scale\n  Computational Methods: Some Recent Progress", "source": "Olivier Allix (LMT), Pierre Gosselet (LMT), Pierre Kerfriden, Karin\n  Saavedra (LMT)", "docs_id": "1304.6996", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Virtual Delamination Testing through Non-Linear Multi-Scale\n  Computational Methods: Some Recent Progress. This paper deals with the parallel simulation of delamination problems at the meso-scale by means of multi-scale methods, the aim being the Virtual Delamination Testing of Composite parts. In the non-linear context, Domain Decomposition Methods are mainly used as a solver for the tangent problem to be solved at each iteration of a Newton-Raphson algorithm. In case of strongly nonlinear and heterogeneous problems, this procedure may lead to severe difficulties. The paper focuses on methods to circumvent these problems, which can now be expressed using a relatively general framework, even though the different ingredients of the strategy have emerged separately. We rely here on the micro-macro framework proposed in (Ladev\\`eze, Loiseau, and Dureisseix, 2001). The method proposed in this paper introduces three additional features: (i) the adaptation of the macro-basis to situations where classical homogenization does not provide a good preconditioner, (ii) the use of non-linear relocalization to decrease the number of global problems to be solved in the case of unevenly distributed non-linearities, (iii) the adaptation of the approximation of the local Schur complement which governs the convergence of the proposed iterative technique. Computations of delamination and delamination-buckling interaction with contact on potentially large delaminated areas are used to illustrate those aspects."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Virtual Delamination Testing through Non-Linear Multi-Scale Computational Methods, what is the primary challenge faced by Domain Decomposition Methods when used to solve the tangent problem in a Newton-Raphson algorithm for strongly nonlinear and heterogeneous problems?\n\n**A)** The method is computationally expensive due to the large number of global problems to be solved.\n**B)** The method is sensitive to the choice of macro-basis, which can lead to poor preconditioning.\n**C)** The method is unable to handle unevenly distributed non-linearities, resulting in a significant increase in the number of global problems to be solved.\n**D)** The method is limited by the approximation of the local Schur complement, which governs the convergence of the iterative technique.\n\n**Correct Answer:** C) The method is unable to handle unevenly distributed non-linearities, resulting in a significant increase in the number of global problems to be solved.\n\n**Explanation:** The correct answer is C) because the paper highlights the challenge of unevenly distributed non-linearities in strongly nonlinear and heterogeneous problems, which can lead to a significant increase in the number of global problems to be solved. The proposed method addresses this challenge by using non-linear relocalization to decrease the number of global problems. The other options are incorrect because while they may be related to the challenges faced by Domain Decomposition Methods, they are not the primary challenge mentioned in the paper."}, "27": {"documentation": {"title": "Entropic patchiness drives multi-phase coexistence in discotic\n  colloid-depletant mixtures", "source": "\\'A. Gonz\\'alez Garc\\'ia, H. H. Wensink, H. N. W. Lekkerkerker, and R.\n  Tuinier", "docs_id": "1711.04143", "section": ["cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Entropic patchiness drives multi-phase coexistence in discotic\n  colloid-depletant mixtures. Entropy-driven equilibrium phase behaviour of hard particle dispersions can be understood from excluded volume arguments only. While monodisperse hard spheres only exhibit a fluid-solid phase transition, anisotropic hard particles such as rods, discs, cuboids or boards exhibit various multi-phase equilibria. Ordering of such anisotropic particles increases the free volume entropy by reducing the excluded volume between them. The addition of depletants gives rise to an entropic patchiness represented by orientation-dependent attraction resulting in non-trivial phase behaviour. We show that free volume theory is a simple, generic and tractable framework that enables to incorporate these effects and rationalise various experimental findings. Plate-shaped particles constitute the main building blocks of clays, asphaltenes and chromonic liquid crystals that find widespread use in the food, cosmetics and oil industry. We demonstrate that mixtures of platelets and ideal depletants exhibit a strikingly rich phase behaviour containing several types of three-phase coexistence areas and even a quadruple region with four coexisting phases."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of adding depletants to anisotropic hard particle dispersions, such as rods, discs, cuboids, or boards, on the phase behaviour of the system?\n\nA) It increases the excluded volume between particles, leading to a fluid-solid phase transition.\nB) It reduces the free volume entropy, resulting in a more ordered phase.\nC) It introduces orientation-dependent attraction, leading to entropic patchiness and non-trivial phase behaviour.\nD) It eliminates the possibility of three-phase coexistence areas.\n\nCorrect Answer: C) It introduces orientation-dependent attraction, leading to entropic patchiness and non-trivial phase behaviour.\n\nExplanation: The addition of depletants to anisotropic hard particle dispersions introduces orientation-dependent attraction, which increases the free volume entropy by reducing the excluded volume between particles. This leads to entropic patchiness and non-trivial phase behaviour, including three-phase coexistence areas and even quadruple regions with four coexisting phases. The correct answer, C, highlights the key effect of depletants on the phase behaviour of the system."}, "28": {"documentation": {"title": "A competitive search game with a moving target", "source": "Benoit Duvocelle, J\\'anos Flesch, Mathias Staudigl, Dries Vermeulen", "docs_id": "2008.12032", "section": ["cs.GT", "econ.TH", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A competitive search game with a moving target. We introduce a discrete-time search game, in which two players compete to find an object first. The object moves according to a time-varying Markov chain on finitely many states. The players know the Markov chain and the initial probability distribution of the object, but do not observe the current state of the object. The players are active in turns. The active player chooses a state, and this choice is observed by the other player. If the object is in the chosen state, this player wins and the game ends. Otherwise, the object moves according to the Markov chain and the game continues at the next period. We show that this game admits a value, and for any error-term $\\veps>0$, each player has a pure (subgame-perfect) $\\veps$-optimal strategy. Interestingly, a 0-optimal strategy does not always exist. The $\\veps$-optimal strategies are robust in the sense that they are $2\\veps$-optimal on all finite but sufficiently long horizons, and also $2\\veps$-optimal in the discounted version of the game provided that the discount factor is close to 1. We derive results on the analytic and structural properties of the value and the $\\veps$-optimal strategies. Moreover, we examine the performance of the finite truncation strategies, which are easy to calculate and to implement. We devote special attention to the important time-homogeneous case, where additional results hold."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the discrete-time search game with a moving target, what is the relationship between the $\\veps$-optimal strategies and the value function, and how do they relate to the discount factor in the discounted version of the game?\n\n**A)** The $\\veps$-optimal strategies are $2\\veps$-optimal on all finite but sufficiently long horizons, and the value function is independent of the discount factor.\n\n**B)** The $\\veps$-optimal strategies are $2\\veps$-optimal on all finite but sufficiently long horizons, and the value function is directly proportional to the discount factor.\n\n**C)** The $\\veps$-optimal strategies are $2\\veps$-optimal on all finite but sufficiently long horizons, and the value function is inversely proportional to the discount factor.\n\n**D)** The $\\veps$-optimal strategies are $2\\veps$-optimal on all finite but sufficiently long horizons, and the value function is a function of the discount factor that depends on the initial probability distribution of the object.\n\n**Correct Answer:** D) The $\\veps$-optimal strategies are $2\\veps$-optimal on all finite but sufficiently long horizons, and the value function is a function of the discount factor that depends on the initial probability distribution of the object.\n\n**Explanation:** The correct answer is D) because the $\\veps$-optimal strategies are robust in the sense that they are $2\\veps$-optimal on all finite but sufficiently long horizons, and the value function is a function of the discount factor that depends on the initial probability distribution of the object. This is stated in the problem description as \"the $\\veps$-optimal strategies are robust in the sense that they are $2\\veps$-optimal on all finite but sufficiently long horizons, and also $2\\veps$-optimal in the discounted version of the game provided that the discount factor is close to 1.\""}, "29": {"documentation": {"title": "Time--consistent investment under model uncertainty: the robust forward\n  criteria", "source": "Sigrid Kallblad, Jan Obloj and Thaleia Zariphopoulou", "docs_id": "1311.3529", "section": ["q-fin.PM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Time--consistent investment under model uncertainty: the robust forward\n  criteria. We combine forward investment performance processes and ambiguity averse portfolio selection. We introduce the notion of robust forward criteria which addresses the issues of ambiguity in model specification and in preferences and investment horizon specification. It describes the evolution of time-consistent ambiguity averse preferences. We first focus on establishing dual characterizations of the robust forward criteria. This offers various advantages as the dual problem amounts to a search for an infimum whereas the primal problem features a saddle-point. Our approach is based on ideas developed in Schied (2007) and Zitkovic (2009). We then study in detail non-volatile criteria. In particular, we solve explicitly the example of an investor who starts with a logarithmic utility and applies a quadratic penalty function. The investor builds a dynamical estimate of the market price of risk $\\hat \\lambda$ and updates her stochastic utility in accordance with the so-perceived elapsed market opportunities. We show that this leads to a time-consistent optimal investment policy given by a fractional Kelly strategy associated with $\\hat \\lambda$. The leverage is proportional to the investor's confidence in her estimate $\\hat \\lambda$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider an investor who starts with a logarithmic utility function and applies a quadratic penalty function to her investment portfolio. The investor builds a dynamical estimate of the market price of risk $\\hat \\lambda$ and updates her stochastic utility in accordance with the so-perceived elapsed market opportunities. What is the time-consistent optimal investment policy for this investor, and what is the relationship between the leverage and the investor's confidence in her estimate $\\hat \\lambda$?\n\n**A)** The time-consistent optimal investment policy is a fractional Kelly strategy associated with $\\hat \\lambda$, where the leverage is proportional to the investor's confidence in her estimate $\\hat \\lambda$. The investor's confidence in $\\hat \\lambda$ is measured by the variance of the estimate, and the leverage is adjusted accordingly.\n\n**B)** The time-consistent optimal investment policy is a mean-reverting strategy, where the investor invests in assets that have historically been mean-reverting. The leverage is proportional to the investor's confidence in her estimate $\\hat \\lambda$, but the relationship is not explicitly defined.\n\n**C)** The time-consistent optimal investment policy is a risk-averse strategy, where the investor invests in assets that minimize the expected utility of the portfolio. The leverage is proportional to the investor's confidence in her estimate $\\hat \\lambda$, but the relationship is not explicitly defined.\n\n**D)** The time-consistent optimal investment policy is a risk-neutral strategy, where the investor invests in assets that have a constant expected return. The leverage is proportional to the investor's confidence in her estimate $\\hat \\lambda$, but the relationship is not explicitly defined.\n\n**Correct Answer:** A) The time-consistent optimal investment policy is a fractional Kelly strategy associated with $\\hat \\lambda$, where the leverage is proportional to the investor's confidence in her estimate $\\hat \\lambda$. The investor's confidence in $\\hat \\lambda$ is measured by the variance of the estimate, and the leverage is adjusted accordingly.\n\n**Explanation:** The correct answer is based on the idea that the investor builds a dynamical estimate of the market price of risk $\\hat \\lambda$ and updates her stochastic utility in accordance with the so-perceived elapsed market opportunities. This leads to a time-consistent optimal investment policy, which is a fractional Kelly strategy associated with $\\hat \\lambda$. The leverage is proportional to the investor's confidence in her estimate $\\hat \\lambda$, and the confidence is measured by the variance of the estimate. This is a key insight from the provided documentation, which highlights the importance of considering model uncertainty and ambiguity aversion in investment decision-making."}, "30": {"documentation": {"title": "Random Aharonov-Bohm vortices and some exact families of integrals: Part\n  II", "source": "Stefan Mashkevich (New York / Kiev), St\\'ephane Ouvry (Orsay)", "docs_id": "0801.4818", "section": ["cond-mat.mes-hall", "cond-mat.stat-mech", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Random Aharonov-Bohm vortices and some exact families of integrals: Part\n  II. At 6th order in perturbation theory, the random magnetic impurity problem at second order in impurity density narrows down to the evaluation of a single Feynman diagram with maximal impurity line crossing. This diagram can be rewritten as a sum of ordinary integrals and nested double integrals of products of the modified Bessel functions $K_{\\nu}$ and $I_{\\nu}$, with $\\nu=0,1$. That sum, in turn, is shown to be a linear combination with rational coefficients of $(2^5-1)\\zeta(5)$, $\\int_0^{\\infty} u K_0(u)^6 du$ and $\\int_0^{\\infty} u^3 K_0(u)^6 du$. Unlike what happens at lower orders, these two integrals are not linear combinations with rational coefficients of Euler sums, even though they appear in combination with $\\zeta(5)$. On the other hand, any integral $\\int_0^{\\infty} u^{n+1} K_0(u)^p (uK_1(u))^q du$ with weight $p+q=6$ and an even $n$ is shown to be a linear combination with rational coefficients of the above two integrals and 1, a result that can be easily generalized to any weight $p+q=k$. A matrix recurrence relation in $n$ is built for such integrals. The initial conditions are such that the asymptotic behavior is determined by the smallest eigenvalue of the transition matrix."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Evaluate the integral $\\int_0^{\\infty} u^3 K_0(u)^6 (uK_1(u))^2 du$ using the results from the given Arxiv documentation.\n\nA) $\\frac{1}{2} \\left( \\frac{2^5-1}{\\zeta(5)} + \\int_0^{\\infty} u K_0(u)^6 du \\right)$\nB) $\\frac{1}{2} \\left( \\frac{2^5-1}{\\zeta(5)} + \\int_0^{\\infty} u^3 K_0(u)^6 du \\right)$\nC) $\\frac{1}{2} \\left( \\frac{2^5-1}{\\zeta(5)} + \\int_0^{\\infty} u^3 K_0(u)^6 du + 1 \\right)$\nD) $\\frac{1}{2} \\left( \\frac{2^5-1}{\\zeta(5)} + \\int_0^{\\infty} u^3 K_0(u)^6 du + \\int_0^{\\infty} u K_0(u)^6 du \\right)$\n\nCorrect Answer: C) $\\frac{1}{2} \\left( \\frac{2^5-1}{\\zeta(5)} + \\int_0^{\\infty} u^3 K_0(u)^6 du + 1 \\right)$\n\nExplanation: According to the documentation, any integral $\\int_0^{\\infty} u^{n+1} K_0(u)^p (uK_1(u))^q du$ with weight $p+q=6$ and an even $n$ is a linear combination with rational coefficients of the integrals $\\int_0^{\\infty} u K_0(u)^6 du$ and $\\int_0^{\\infty} u^3 K_0(u)^6 du$, and 1. In this case, $n=3$, $p=6$, and $q=2$, so the integral can be expressed as a linear combination of the two integrals and 1. The correct answer is option C, which includes the additional constant term 1."}, "31": {"documentation": {"title": "Sinogram interpolation for sparse-view micro-CT with deep learning\n  neural network", "source": "Xu Dong, Swapnil Vekhande, Guohua Cao", "docs_id": "1902.03362", "section": ["physics.med-ph", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sinogram interpolation for sparse-view micro-CT with deep learning\n  neural network. In sparse-view Computed Tomography (CT), only a small number of projection images are taken around the object, and sinogram interpolation method has a significant impact on final image quality. When the amount of sparsity (the amount of missing views in sinogram data) is not high, conventional interpolation methods have yielded good results. When the amount of sparsity is high, more advanced sinogram interpolation methods are needed. Recently, several deep learning (DL) based sinogram interpolation methods have been proposed. However, those DL-based methods have mostly tested so far on computer simulated sinogram data rather experimentally acquired sinogram data. In this study, we developed a sinogram interpolation method for sparse-view micro-CT based on the combination of U-Net and residual learning. We applied the method to sinogram data obtained from sparse-view micro-CT experiments, where the sparsity reached 90%. The interpolated sinogram by the DL neural network was fed to FBP algorithm for reconstruction. The result shows that both RMSE and SSIM of CT image are greatly improved. The experimental results demonstrate that this sinogram interpolation method produce significantly better results over standard linear interpolation methods when the sinogram data are extremely sparse."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary limitation of conventional interpolation methods for sinogram interpolation in sparse-view micro-CT, and how do deep learning-based methods address this limitation?\n\nA) Conventional interpolation methods are limited by their inability to handle high-dimensional sinogram data, whereas deep learning-based methods can learn complex patterns in the data.\nB) Conventional interpolation methods are limited by their inability to handle extremely sparse sinogram data, whereas deep learning-based methods can produce significantly better results in such cases.\nC) Conventional interpolation methods are limited by their inability to learn from large datasets, whereas deep learning-based methods can leverage large datasets to improve performance.\nD) Conventional interpolation methods are limited by their inability to handle non-linear relationships in the data, whereas deep learning-based methods can learn non-linear relationships.\n\nCorrect Answer: B) Conventional interpolation methods are limited by their inability to handle extremely sparse sinogram data, whereas deep learning-based methods can produce significantly better results in such cases.\n\nExplanation: The question requires the test-taker to understand the limitations of conventional interpolation methods and how deep learning-based methods address these limitations. The correct answer, B, highlights the specific limitation of conventional interpolation methods in handling extremely sparse sinogram data, which is a key challenge in sparse-view micro-CT. The incorrect answers, A, C, and D, are plausible but incorrect, requiring the test-taker to carefully analyze the pros and cons of each option."}, "32": {"documentation": {"title": "Time-diffracting beams: On their nature, diffraction-free propagation as\n  needles of light, and nonlinear generation", "source": "Miguel A. Porras", "docs_id": "1802.10475", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Time-diffracting beams: On their nature, diffraction-free propagation as\n  needles of light, and nonlinear generation. We investigate on the properties of the recently introduced time-diffracting (TD) beams in free space. They are shown to be paraxial and quasi-monochromatic realizations of localized waves, spatiotemporal localized waves travelling undistorted at arbitrary speeds. The paraxial and quasi-monochromatic regime is shown to be necessary to observe what can properly be named diffraction in time. TD beams of finite energy travelling at quasi-luminal velocities are seen to form substantially longer foci or needles of light than the so-called abruptly focusing and defocusing needle of light, or limiting TD beam of infinite speed. Exploring the properties of TD beams under Lorentz transformations and transformation by paraxial optical systems, we realize that the relativistically moving nonlinear polarization of material media induced by a strongly localized fundamental pump wave generates a TD beam at its second harmonic, whose diffraction-free behavior as a needle of light in free space can be optimized with a standard $4f$-imager system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using time-diffracting (TD) beams of finite energy traveling at quasi-luminal velocities, as opposed to the so-called abruptly focusing and defocusing needle of light or limiting TD beam of infinite speed?\n\n**A)** They form shorter foci or needles of light.\n**B)** They exhibit diffraction-free behavior as needles of light in free space.\n**C)** They can be easily generated using standard optical systems.\n**D)** They can be used to induce nonlinear polarization of material media at their second harmonic.\n\n**Correct Answer:** B) They exhibit diffraction-free behavior as needles of light in free space.\n\n**Explanation:** According to the documentation, TD beams of finite energy traveling at quasi-luminal velocities form substantially longer foci or needles of light than the so-called abruptly focusing and defocusing needle of light or limiting TD beam of infinite speed. This is because they exhibit diffraction-free behavior as needles of light in free space, which is a key property of TD beams. The other options are incorrect because they do not accurately describe the primary advantage of using TD beams of finite energy traveling at quasi-luminal velocities."}, "33": {"documentation": {"title": "3alpha clustering in the excited states of 16C", "source": "T. Baba, Y. Chiba and M. Kimura", "docs_id": "1410.0789", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "3alpha clustering in the excited states of 16C. The alpha cluster states of 16C are investigated by using the antisymmetrized molecular dynamics. It is shown that two different types of alpha cluster states exist: triangular and linear-chain states. The former has an approximate isosceles triangular configuration of alpha particles surrounded by four valence neutrons occupying sd-shell, while the latter has the linearly aligned alpha particles with two sd-shell neutrons and two pf-shell neutrons. It is found that the structure of the linear-chain state is qualitatively understood in terms of the 3/2 pi- and 1/2 sigma- molecular orbit as predicted by molecular-orbital model, but there exists non-negligible Be+alpha+2n correlation. The band-head energies of the triangular and linear-chain rotational bands are 8.0 and 15.5 MeV, and the latter is close to the He+Be threshold energy. It is also shown that the linear-chain state becomes the yrast sstate at J=10 with excitation energy 27.8 MeV owing to its very large moment-of-inertia comparable with hyperdeformation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism underlying the emergence of the linear-chain state in 16C, and how does it relate to the molecular-orbital model?\n\nA) The linear-chain state arises from the Be+alpha+2n correlation, which is a result of the strong nuclear force.\nB) The linear-chain state is a consequence of the 3/2 \u03c0- and 1/2 \u03c3- molecular orbitals predicted by the molecular-orbital model, but it is not directly related to the Be+alpha+2n correlation.\nC) The linear-chain state is a result of the hyperdeformation of the nucleus, which leads to a large moment-of-inertia comparable to hyperdeformation.\nD) The linear-chain state is a result of the sd-shell neutrons occupying the sd-shell, which leads to a qualitative understanding of the state's structure.\n\nCorrect Answer: B) The linear-chain state is a consequence of the 3/2 \u03c0- and 1/2 \u03c3- molecular orbitals predicted by the molecular-orbital model, but it is not directly related to the Be+alpha+2n correlation.\n\nExplanation: The correct answer is B) because the documentation states that the structure of the linear-chain state is qualitatively understood in terms of the 3/2 \u03c0- and 1/2 \u03c3- molecular orbitals predicted by the molecular-orbital model. However, it also notes that there exists a non-negligible Be+alpha+2n correlation, which is not directly related to the emergence of the linear-chain state. The other options are incorrect because they either oversimplify the relationship between the linear-chain state and the molecular-orbital model (A), or they introduce unrelated concepts (C and D)."}, "34": {"documentation": {"title": "Non-Abelian Alice strings in two-flavor dense QCD", "source": "Yuki Fujimoto, Muneto Nitta", "docs_id": "2011.09947", "section": ["hep-ph", "astro-ph.HE", "cond-mat.supr-con", "hep-th", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-Abelian Alice strings in two-flavor dense QCD. Quark-hadron continuity with two-flavor quarks that was proposed recently connects hadronic matter with neutron $^3P_2$ superfluidity and two-flavor dense quark matter. This two-flavor dense quark phase consists of the coexistence of the 2SC condensates and the $P$-wave diquark condensates of $d$-quarks, which gives rise to color superconductivity as well as superfluidity. We classify vortices in this phase. The most stable vortices are what we call the non-Abelian Alice strings, which are superfluid vortices with non-Abelian color magnetic fluxes therein, exhibiting so-called topological obstruction, or a non-Abelian generalization of the Alice property. We show that a single Abelian superfluid vortex is unstable against decay into three non-Abelian Alice strings. We discover that a non-Abelian Alice string carries orientational moduli of the real projective space $\\mathbb{R}P^2$ corresponding to the color flux therein in the presence of the $P$-wave condensates alone. We calculate Aharanov-Bohm (AB) phases around the non-Abelian Alice string, and find that the 2SC condensates and string's orientational moduli must be aligned with each other because of single-valuedness of the AB phases of the 2SC condensates."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of non-Abelian Alice strings in two-flavor dense QCD, what is the topological obstruction that arises from the non-Abelian color magnetic fluxes within these superfluid vortices?\n\nA) The existence of a non-Abelian generalization of the Meissner effect\nB) The requirement for single-valuedness of the Aharanov-Bohm phases of the 2SC condensates\nC) The instability of a single Abelian superfluid vortex against decay into three non-Abelian Alice strings\nD) The necessity for the 2SC condensates and string's orientational moduli to be aligned with each other due to the topological obstruction\n\nCorrect Answer: D) The necessity for the 2SC condensates and string's orientational moduli to be aligned with each other due to the topological obstruction\n\nExplanation: The correct answer is D) because the topological obstruction in non-Abelian Alice strings arises from the non-Abelian color magnetic fluxes within these superfluid vortices. This obstruction requires that the 2SC condensates and the string's orientational moduli be aligned with each other, as stated in the original documentation. The other options are incorrect because they describe related but distinct concepts: A) refers to a general property of non-Abelian superconductors, B) refers to a requirement for single-valuedness of the Aharanov-Bohm phases of the 2SC condensates, and C) refers to the instability of a single Abelian superfluid vortex against decay into three non-Abelian Alice strings."}, "35": {"documentation": {"title": "Semiparametric Functional Factor Models with Bayesian Rank Selection", "source": "Daniel R. Kowal and Antonio Canale", "docs_id": "2108.02151", "section": ["stat.ME", "econ.EM", "stat.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semiparametric Functional Factor Models with Bayesian Rank Selection. Functional data are frequently accompanied by parametric templates that describe the typical shapes of the functions. Although the templates incorporate critical domain knowledge, parametric functional data models can incur significant bias, which undermines the usefulness and interpretability of these models. To correct for model misspecification, we augment the parametric templates with an infinite-dimensional nonparametric functional basis. Crucially, the nonparametric factors are regularized with an ordered spike-and-slab prior, which implicitly provides consistent rank selection and satisfies several appealing theoretical properties. This prior is accompanied by a parameter-expansion scheme customized to boost MCMC efficiency, and is broadly applicable for Bayesian factor models. The nonparametric basis functions are learned from the data, yet constrained to be orthogonal to the parametric template in order to preserve distinctness between the parametric and nonparametric terms. The versatility of the proposed approach is illustrated through applications to synthetic data, human motor control data, and dynamic yield curve data. Relative to parametric alternatives, the proposed semiparametric functional factor model eliminates bias, reduces excessive posterior and predictive uncertainty, and provides reliable inference on the effective number of nonparametric terms--all with minimal additional computational costs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Semiparametric Functional Factor Models with Bayesian Rank Selection, what is the primary advantage of using an ordered spike-and-slab prior to regularize the nonparametric functional basis?\n\n**A)** It allows for the incorporation of prior knowledge about the functional forms of the data.\n**B)** It provides a way to select the number of nonparametric terms based on model evidence.\n**C)** It enables the estimation of the effective number of nonparametric terms with minimal additional computational costs.\n**D)** It facilitates the estimation of the parametric template parameters using a maximum likelihood approach.\n\n**Correct Answer:** C) It enables the estimation of the effective number of nonparametric terms with minimal additional computational costs.\n\n**Explanation:** The ordered spike-and-slab prior is a key component of the proposed approach, and its primary advantage is that it allows for the estimation of the effective number of nonparametric terms with minimal additional computational costs. This is because the prior provides a way to select the number of nonparametric terms based on model evidence, which reduces the computational burden of estimating the model parameters. The other options are incorrect because they do not accurately capture the primary advantage of the ordered spike-and-slab prior. Option A is incorrect because the prior does not incorporate prior knowledge about the functional forms of the data. Option B is incorrect because the prior does not provide a way to select the number of nonparametric terms based on model evidence. Option D is incorrect because the prior does not facilitate the estimation of the parametric template parameters using a maximum likelihood approach."}, "36": {"documentation": {"title": "Discrete modes of social information processing predict individual\n  behavior of fish in a group", "source": "Roy Harpaz, Ga\\v{s}per Tka\\v{c}ik, Elad Schneidman", "docs_id": "1703.03065", "section": ["q-bio.NC", "physics.bio-ph", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Discrete modes of social information processing predict individual\n  behavior of fish in a group. Individual computations and social interactions underlying collective behavior in groups of animals are of great ethological, behavioral, and theoretical interest. While complex individual behaviors have successfully been parsed into small dictionaries of stereotyped behavioral modes, studies of collective behavior largely ignored these findings; instead, their focus was on inferring single, mode-independent social interaction rules that reproduced macroscopic and often qualitative features of group behavior. Here we bring these two approaches together to predict individual swimming patterns of adult zebrafish in a group. We show that fish alternate between an active mode in which they are sensitive to the swimming patterns of conspecifics, and a passive mode where they ignore them. Using a model that accounts for these two modes explicitly, we predict behaviors of individual fish with high accuracy, outperforming previous approaches that assumed a single continuous computation by individuals and simple metric or topological weighing of neighbors behavior. At the group level, switching between active and passive modes is uncorrelated among fish, yet correlated directional swimming behavior still emerges. Our quantitative approach for studying complex, multi-modal individual behavior jointly with emergent group behavior is readily extensible to additional behavioral modes and their neural correlates, as well as to other species."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study on zebrafish behavior propose a model that accounts for two modes of social information processing: an active mode and a passive mode. What is the primary advantage of this model over previous approaches that assumed a single continuous computation by individuals?\n\nA) It allows for the incorporation of additional behavioral modes and their neural correlates.\nB) It provides a more accurate prediction of individual swimming patterns in groups of zebrafish.\nC) It enables the study of complex, multi-modal individual behavior jointly with emergent group behavior.\nD) It assumes a single, mode-independent social interaction rule that reproduces macroscopic features of group behavior.\n\n**Correct Answer:** C) It enables the study of complex, multi-modal individual behavior jointly with emergent group behavior.\n\n**Explanation:** The correct answer is C) because the study highlights the importance of accounting for both individual computations and social interactions in understanding collective behavior. The authors' model explicitly accounts for two modes of social information processing, which allows for a more comprehensive understanding of the complex, multi-modal individual behavior of zebrafish. This is in contrast to previous approaches that assumed a single, mode-independent social interaction rule."}, "37": {"documentation": {"title": "Masses of optical components and black holes in x-ray novae: the effects\n  of components proximity", "source": "V. S. Petrov and E. A. Antokhina and A. M. Cherepashchuk", "docs_id": "1702.04580", "section": ["astro-ph.SR", "astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Masses of optical components and black holes in x-ray novae: the effects\n  of components proximity. An exact calculation of CaI $\\lambda 6439.075$A absorption profiles in the spectra of optical stars in low-mass X-ray binary systems with black holes (BH LMXBs) is carried out. We show that the approximation of a real Roche lobe filling star as disk with uniform local line profile and linear limb darkening law leads to overestimation of projected equatorial rotational velocity $V_{rot} \\sin i$ and accordingly, underestimation of mass ratio $q=M_x/M_v$. Refined value of $q$ does not affect the mass of a black hole, but the mass of an optical star has shrunk $\\sim 1.5$ times. We present refined components masses in low-mass X-ray binaries with black holes. Companion masses in BH LMXBs are found in the mass range 0.1 - 1.6 $M_{\\odot}$ with the peak at $M_v \\simeq 0.35 M_{\\odot}$. This finding poses additional problem for the standard evolutionary scenarios of BH LMXBs formation with a common envelope phase (CE). We also discuss the implications of these masses into the evolutionary history of the binary."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of low-mass X-ray binary systems with black holes, what is the primary effect of the proximity of optical components to the black hole on the calculated masses of these components?\n\nA) Overestimation of the mass ratio q and underestimation of the mass of the optical star\nB) Underestimation of the mass ratio q and overestimation of the mass of the optical star\nC) No effect on the mass ratio q or the mass of the optical star\nD) Overestimation of the mass of the optical star and underestimation of the mass ratio q\n\n**Correct Answer:** A) Overestimation of the mass ratio q and underestimation of the mass of the optical star\n\n**Explanation:** According to the documentation, the approximation of a real Roche lobe filling star as a disk with uniform local line profile and linear limb darkening law leads to an overestimation of the projected equatorial rotational velocity V_rot sin i, which in turn results in an underestimation of the mass ratio q = M_x / M_v. This means that the calculated mass of the optical star is underestimated, while the mass ratio q is overestimated."}, "38": {"documentation": {"title": "Critical Switching in Globally Attractive Chimeras", "source": "Yuanzhao Zhang, Zachary G. Nicolaou, Joseph D. Hart, Rajarshi Roy,\n  Adilson E. Motter", "docs_id": "1911.07871", "section": ["cond-mat.dis-nn", "math.DS", "nlin.AO", "nlin.CD", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Critical Switching in Globally Attractive Chimeras. We report on a new type of chimera state that attracts almost all initial conditions and exhibits power-law switching behavior in networks of coupled oscillators. Such switching chimeras consist of two symmetric configurations, which we refer to as subchimeras, in which one cluster is synchronized and the other is incoherent. Despite each subchimera being linearly stable, switching chimeras are extremely sensitive to noise: arbitrarily small noise triggers and sustains persistent switching between the two symmetric subchimeras. The average switching frequency scales as a power law with the noise intensity, which is in contrast with the exponential scaling observed in typical stochastic transitions. Rigorous numerical analysis reveals that the power-law switching behavior originates from intermingled basins of attraction associated with the two subchimeras, which in turn are induced by chaos and symmetry in the system. The theoretical results are supported by experiments on coupled optoelectronic oscillators, which demonstrate the generality and robustness of switching chimeras."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the underlying mechanism that gives rise to the power-law scaling of the average switching frequency in switching chimeras, and how does it differ from the exponential scaling observed in typical stochastic transitions?\n\n**A)** The power-law scaling is due to the intermingled basins of attraction associated with the two subchimeras, which are induced by chaos and symmetry in the system.\n\n**B)** The power-law scaling is due to the linear stability of the subchimeras, which allows for a more efficient switching between the two symmetric configurations.\n\n**C)** The power-law scaling is due to the noise intensity, which triggers and sustains persistent switching between the two symmetric subchimeras.\n\n**D)** The power-law scaling is due to the global attractivity of the chimera state, which allows for a more efficient switching between the two symmetric configurations.\n\n**Correct Answer:** A) The power-law scaling is due to the intermingled basins of attraction associated with the two subchimeras, which are induced by chaos and symmetry in the system.\n\n**Explanation:** The correct answer is A) because the documentation states that the power-law switching behavior originates from intermingled basins of attraction associated with the two subchimeras, which are induced by chaos and symmetry in the system. This is supported by the numerical analysis and experiments on coupled optoelectronic oscillators. The other options are incorrect because they do not accurately reflect the underlying mechanism described in the documentation. Option B is incorrect because the subchimeras are linearly stable, not because of their linear stability. Option C is incorrect because the noise intensity triggers and sustains switching, but it is not the underlying mechanism that gives rise to the power-law scaling. Option D is incorrect because the global attractivity of the chimera state is not the underlying mechanism that gives rise to the power-law scaling."}, "39": {"documentation": {"title": "Leverage effect in energy futures", "source": "Ladislav Kristoufek", "docs_id": "1403.0064", "section": ["q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Leverage effect in energy futures. We propose a comprehensive treatment of the leverage effect, i.e. the relationship between returns and volatility of a specific asset, focusing on energy commodities futures, namely Brent and WTI crude oils, natural gas and heating oil. After estimating the volatility process without assuming any specific form of its behavior, we find the volatility to be long-term dependent with the Hurst exponent on a verge of stationarity and non-stationarity. Bypassing this using by using the detrended cross-correlation and the detrending moving-average cross-correlation coefficients, we find the standard leverage effect for both crude oil. For heating oil, the effect is not statistically significant, and for natural gas, we find the inverse leverage effect. Finally, we also show that none of the effects between returns and volatility is detected as the long-term cross-correlated one. These findings can be further utilized to enhance forecasting models and mainly in the risk management and portfolio diversification."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study on the leverage effect in energy futures propose a comprehensive treatment of the relationship between returns and volatility of specific assets. What is the primary finding regarding the volatility process of the energy commodities futures, namely Brent and WTI crude oils, natural gas, and heating oil?\n\nA) The volatility process is short-term independent and exhibits a constant Hurst exponent.\nB) The volatility process is long-term dependent, with the Hurst exponent on the verge of stationarity and non-stationarity.\nC) The volatility process is stationary and exhibits a constant Hurst exponent.\nD) The volatility process is non-stationary and exhibits a constant Hurst exponent.\n\n**Correct Answer:** B) The volatility process is long-term dependent, with the Hurst exponent on the verge of stationarity and non-stationarity.\n\n**Explanation:** The correct answer is based on the first sentence of the provided documentation, which states that the authors find the volatility process to be long-term dependent with the Hurst exponent on a verge of stationarity and non-stationarity. This finding is crucial in understanding the behavior of the volatility process and its implications for forecasting models and risk management.\n\n**Candidate A Explanation:** This option is incorrect because it states that the volatility process is short-term independent, which contradicts the finding that the volatility process is long-term dependent.\n\n**Candidate B Explanation:** This option is correct, but it is not the complete answer. The correct answer is B) The volatility process is long-term dependent, with the Hurst exponent on the verge of stationarity and non-stationarity. The option B is partially correct, but it does not provide the complete answer.\n\n**Candidate C Explanation:** This option is incorrect because it states that the volatility process is stationary, which contradicts the finding that the Hurst exponent is on the verge of stationarity and non-stationarity.\n\n**Candidate D Explanation:** This option is incorrect because it states that the volatility process is non-stationary and exhibits a constant Hurst exponent, which contradicts the finding that the Hurst exponent is on the verge of stationarity and non-stationarity."}, "40": {"documentation": {"title": "Bayesian Nonparametric Dynamic State Space Modeling with Circular Latent\n  States", "source": "Satyaki Mazumder and Sourabh Bhattacharya", "docs_id": "1408.3041", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian Nonparametric Dynamic State Space Modeling with Circular Latent\n  States. State space models are well-known for their versatility in modeling dynamic systems that arise in various scientific disciplines. Although parametric state space models are well studied, nonparametric approaches are much less explored in comparison. In this article we propose a novel Bayesian nonparametric approach to state space modeling assuming that both the observational and evolutionary functions are unknown and are varying with time; crucially, we assume that the unknown evolutionary equation describes dynamic evolution of some latent circular random variable. Based on appropriate kernel convolution of the standard Wiener process we model the time-varying observational and evolutionary functions as suitable Gaussian processes that take both linear and circular variables as arguments. Additionally, for the time-varying evolutionary function, we wrap the Gaussian process thus constructed around the unit circle to form an appropriate circular Gaussian process. We show that our process thus created satisfies desirable properties. For the purpose of inference we develop an MCMC based methodology combining Gibbs sampling and Metropolis-Hastings algorithms. Applications to a simulated data set, a real wind speed data set and a real ozone data set demonstrated quite encouraging performances of our model and methodologies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Bayesian nonparametric dynamic state space modeling approach proposed in the article \"Bayesian Nonparametric Dynamic State Space Modeling with Circular Latent States\", what is the key assumption made about the evolutionary function, and how is it addressed?\n\n**A)** The evolutionary function is assumed to be a linear Gaussian process, and it is modeled as a standard Wiener process wrapped around the unit circle to form a circular Gaussian process.\n\n**B)** The evolutionary function is assumed to be a non-linear non-Gaussian process, and it is modeled as a standard Wiener process wrapped around the unit circle to form a circular Gaussian process.\n\n**C)** The evolutionary function is assumed to be a linear Gaussian process, and it is modeled as a standard Wiener process without wrapping around the unit circle.\n\n**D)** The evolutionary function is assumed to be a non-linear non-Gaussian process, and it is modeled as a standard Wiener process without wrapping around the unit circle.\n\n**Correct Answer:** A) The evolutionary function is assumed to be a linear Gaussian process, and it is modeled as a standard Wiener process wrapped around the unit circle to form a circular Gaussian process.\n\n**Explanation:** The article states that the unknown evolutionary equation describes the dynamic evolution of some latent circular random variable, and that the time-varying evolutionary function is modeled as a Gaussian process wrapped around the unit circle to form a circular Gaussian process. This is a key assumption made about the evolutionary function, and it is addressed by using a kernel convolution of the standard Wiener process to model the time-varying observational and evolutionary functions."}, "41": {"documentation": {"title": "Octave-spanning microcomb generation in 4H-silicon-carbide-on-insulator\n  photonics platform", "source": "Lutong Cai, Jingwei Li, Ruixuan Wang and Qing Li", "docs_id": "2111.10856", "section": ["physics.optics", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Octave-spanning microcomb generation in 4H-silicon-carbide-on-insulator\n  photonics platform. Silicon carbide has recently emerged as a promising photonics material due to its unique properties, including possessing strong second- and third-order nonlinear coefficients and hosting various color centers that can be utilized for a wealth of quantum applications. Here, we report the design and demonstration of octave-spanning microcombs in a 4H-silicon-carbide-on-insulator platform for the first time. Such broadband operation is enabled by optimized nanofabrication achieving >1 million intrinsic quality factors in a 36-$\\mu$m-radius microring resonator, and careful dispersion engineering by investigating the dispersion properties of different mode families. For example, for the fundamental transverse-electric mode whose dispersion can be tailored by simply varying the microring waveguide width, we realized a microcomb spectrum covering the wavelength range from 1100 nm to 2400 nm with an on-chip power near 120 mW. While the observed comb state is verified to be chaotic and not soliton, attaining such a large bandwidth is a crucial step towards realizing $f$-2$f$ self-referencing. In addition, we have also observed coherent soliton-crystal state for the fundamental transverse-magnetic mode, which exhibits stronger dispersion than the fundamental transverse-electric mode and hence a narrower bandwidth."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary reason for the octave-spanning microcomb generation in the 4H-silicon-carbide-on-insulator platform, and how does it relate to the material's unique properties?\n\n**A)** The material's strong second-order nonlinear coefficient enables the generation of microcombs, allowing for a broad bandwidth. However, this is not the primary reason.\n\n**B)** The material's unique properties, including strong second- and third-order nonlinear coefficients and various color centers, enable the generation of microcombs with a large bandwidth, making it a promising photonics material.\n\n**C)** The material's dispersion properties are optimized through careful engineering, allowing for the generation of microcombs with a large bandwidth.\n\n**D)** The material's color centers are used to generate microcombs, but this is not the primary reason for the octave-spanning operation.\n\n**Correct Answer:** B) The material's unique properties, including strong second- and third-order nonlinear coefficients and various color centers, enable the generation of microcombs with a large bandwidth, making it a promising photonics material.\n\n**Explanation:** The correct answer is B) because the documentation states that \"Silicon carbide has recently emerged as a promising photonics material due to its unique properties, including possessing strong second- and third-order nonlinear coefficients and hosting various color centers that can be utilized for a wealth of quantum applications.\" This indicates that the material's unique properties are the primary reason for the octave-spanning microcomb generation."}, "42": {"documentation": {"title": "Asset volatility forecasting:The optimal decay parameter in the EWMA\n  model", "source": "Axel A. Araneda", "docs_id": "2105.14382", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asset volatility forecasting:The optimal decay parameter in the EWMA\n  model. The exponentially weighted moving average (EMWA) could be labeled as a competitive volatility estimator, where its main strength relies on computation simplicity, especially in a multi-asset scenario, due to dependency only on the decay parameter, $\\lambda$. But, what is the best election for $\\lambda$ in the EMWA volatility model? Through a large time-series data set of historical returns of the top US large-cap companies; we test empirically the forecasting performance of the EWMA approach, under different time horizons and varying the decay parameter. Using a rolling window scheme, the out-of-sample performance of the variance-covariance matrix is computed following two approaches. First, if we look for a fixed decay parameter for the full sample, the results are in agreement with the RiskMetrics suggestion for 1-month forecasting. In addition, we provide the full-sample optimal decay parameter for the weekly and bi-weekly forecasting horizon cases, confirming two facts: i) the optimal value is as a function of the forecasting horizon, and ii) for lower forecasting horizons the short-term memory gains importance. In a second way, we also evaluate the forecasting performance of EWMA, but this time using the optimal time-varying decay parameter which minimizes the in-sample variance-covariance estimator, arriving at better accuracy than the use of a fixed-full-sample optimal parameter."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The exponentially weighted moving average (EWMA) model is a popular volatility estimator, but its performance can be sensitive to the choice of the decay parameter, \u03bb. According to the study on asset volatility forecasting, what are the two main findings regarding the optimal value of \u03bb for the EWMA model, and how do they relate to the forecasting horizon?\n\n**A)** The optimal value of \u03bb is a fixed value that works well for all forecasting horizons, and it is not affected by the time-varying nature of volatility.\n\n**B)** The optimal value of \u03bb is a function of the forecasting horizon, and it is more sensitive to short-term memory for lower forecasting horizons.\n\n**C)** The optimal value of \u03bb is a fixed value that is optimal for the full sample, but it may not be optimal for shorter or longer forecasting horizons.\n\n**D)** The optimal value of \u03bb is a time-varying parameter that is optimal for the in-sample variance-covariance estimator, but it may not be optimal for out-of-sample performance.\n\n**Correct Answer:** B) The optimal value of \u03bb is a function of the forecasting horizon, and it is more sensitive to short-term memory for lower forecasting horizons.\n\n**Explanation:** The study found that the optimal value of \u03bb is a function of the forecasting horizon, and it is more sensitive to short-term memory for lower forecasting horizons. This means that for shorter forecasting horizons, the EWMA model is more effective at capturing short-term volatility, while for longer forecasting horizons, the model is more effective at capturing long-term volatility. This is in line with the idea that short-term memory gains importance for lower forecasting horizons."}, "43": {"documentation": {"title": "Downstream Effects of Affirmative Action", "source": "Sampath Kannan and Aaron Roth and Juba Ziani", "docs_id": "1808.09004", "section": ["cs.GT", "cs.LG", "econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Downstream Effects of Affirmative Action. We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions. We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. We show that both goals can be achieved when the college does not report grades. On the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** A college has two populations of students, A and B, with different type distributions. The college uses a two-stage model to admit students and then hire them based on their college grades. The employer at the end of the pipeline is rational and computes a posterior distribution on student type conditional on all available information. The college has two fairness goals: (1) guaranteeing equal opportunity across populations, and (2) incentivizing the employer to have a group-blind hiring rule. However, the college has a limited amount of information about student type, and it must decide whether to report grades to the employer.\n\n**A)** The college should report grades to the employer in order to achieve the first fairness goal, as this will allow the employer to make a more informed decision about student type.\n\n**B)** The college should not report grades to the employer, as this will prevent the employer from making biased decisions based on group membership.\n\n**C)** The college should report grades to the employer, but only if the employer is willing to adopt a group-blind hiring rule in response to the reported grades.\n\n**D)** The college should not report grades to the employer, as this will allow the employer to make decisions based on the college's admissions rule, which may be biased towards one population over the other.\n\n**Correct Answer:** D) The college should not report grades to the employer, as this will allow the employer to make decisions based on the college's admissions rule, which may be biased towards one population over the other.\n\n**Explanation:** The correct answer is D) because, as the paper shows, the college's admissions rule and grading policy can be designed to achieve the fairness goals even when grades are not reported to the employer. This is because the employer's posterior distribution on student type can be computed based on the college's admissions rule and the group membership of the students. By not reporting grades, the college can prevent the employer from making biased decisions based on group membership, while still achieving the fairness goals."}, "44": {"documentation": {"title": "Nonparametric inverse probability weighted estimators based on the\n  highly adaptive lasso", "source": "Ashkan Ertefaie, Nima S. Hejazi, Mark J. van der Laan", "docs_id": "2005.11303", "section": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonparametric inverse probability weighted estimators based on the\n  highly adaptive lasso. Inverse probability weighted estimators are the oldest and potentially most commonly used class of procedures for the estimation of causal effects. By adjusting for selection biases via a weighting mechanism, these procedures estimate an effect of interest by constructing a pseudo-population in which selection biases are eliminated. Despite their ease of use, these estimators require the correct specification of a model for the weighting mechanism, are known to be inefficient, and suffer from the curse of dimensionality. We propose a class of nonparametric inverse probability weighted estimators in which the weighting mechanism is estimated via undersmoothing of the highly adaptive lasso, a nonparametric regression function proven to converge at $n^{-1/3}$-rate to the true weighting mechanism. We demonstrate that our estimators are asymptotically linear with variance converging to the nonparametric efficiency bound. Unlike doubly robust estimators, our procedures require neither derivation of the efficient influence function nor specification of the conditional outcome model. Our theoretical developments have broad implications for the construction of efficient inverse probability weighted estimators in large statistical models and a variety of problem settings. We assess the practical performance of our estimators in simulation studies and demonstrate use of our proposed methodology with data from a large-scale epidemiologic study."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a causal effect estimation problem where the outcome variable is binary (0/1) and the exposure variable is a binary variable (0/1). The data is generated from a population with a non-linear relationship between the exposure and outcome variables. The authors of the paper propose a nonparametric inverse probability weighted estimator based on the highly adaptive lasso to estimate the causal effect. However, the estimator requires the correct specification of the weighting mechanism.\n\n**A)** The authors propose using a kernel density estimation (KDE) to estimate the weighting mechanism, which is a nonparametric approach that can handle complex relationships between the exposure and outcome variables.\n\n**B)** The authors propose using a linear regression model to estimate the weighting mechanism, which is a parametric approach that can handle non-linear relationships between the exposure and outcome variables.\n\n**C)** The authors propose using a generalized additive model (GAM) to estimate the weighting mechanism, which is a semi-parametric approach that can handle complex relationships between the exposure and outcome variables.\n\n**D)** The authors propose using a Bayesian approach to estimate the weighting mechanism, which is a non-parametric approach that can handle complex relationships between the exposure and outcome variables.\n\n**Correct Answer:** C) The authors propose using a generalized additive model (GAM) to estimate the weighting mechanism, which is a semi-parametric approach that can handle complex relationships between the exposure and outcome variables.\n\n**Explanation:** The authors propose using a generalized additive model (GAM) to estimate the weighting mechanism, which is a semi-parametric approach that can handle complex relationships between the exposure and outcome variables. This is because the GAM can capture non-linear relationships between the exposure and outcome variables, which is suitable for the problem at hand. The other options are incorrect because the KDE is a non-parametric approach that may not capture the underlying structure of the data, the linear regression model is a parametric approach that may not handle non-linear relationships, and the Bayesian approach is a non-parametric approach that may not be suitable for this specific problem."}, "45": {"documentation": {"title": "Strengthening science, technology, and innovation-based incubators to\n  help achieve Sustainable Development Goals: Lessons from India", "source": "Kavita Surana, Anuraag Singh and Ambuj D Sagar", "docs_id": "2005.13138", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Strengthening science, technology, and innovation-based incubators to\n  help achieve Sustainable Development Goals: Lessons from India. Policymakers in developing countries increasingly see science, technology, and innovation (STI) as an avenue for meeting sustainable development goals (SDGs), with STI-based startups as a key part of these efforts. Market failures call for government interventions in supporting STI for SDGs and publicly-funded incubators can potentially fulfil this role. Using the specific case of India, we examine how publicly-funded incubators could contribute to strengthening STI-based entrepreneurship. India's STI policy and its links to societal goals span multiple decades -- but since 2015 these goals became formally organized around the SDGs. We examine why STI-based incubators were created under different policy priorities before 2015, the role of public agencies in implementing these policies, and how some incubators were particularly effective in addressing the societal challenges that can now be mapped to SDGs. We find that effective incubation for supporting STI-based entrepreneurship to meet societal goals extended beyond traditional incubation activities. For STI-based incubators to be effective, policymakers must strengthen the 'incubation system'. This involves incorporating targeted SDGs in specific incubator goals, promoting coordination between existing incubator programs, developing a performance monitoring system, and finally, extending extensive capacity building at multiple levels including for incubator managers and for broader STI in the country."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the key factors that policymakers in developing countries, such as India, need to strengthen in order to make publicly-funded incubators effective in supporting science, technology, and innovation (STI)-based entrepreneurship to achieve Sustainable Development Goals (SDGs)?\n\n**A)** Targeted SDGs in incubator goals, coordination between incubator programs, and performance monitoring systems.\n**B)** Public funding for incubators, government support for STI, and the role of public agencies in implementing policies.\n**C)** Extensive capacity building for incubator managers and broader STI in the country, coordination between incubator programs, and performance monitoring systems.\n**D)** Targeted SDGs in incubator goals, coordination between incubator programs, and the role of public agencies in implementing policies.\n\n**Correct Answer:** C) Extensive capacity building for incubator managers and broader STI in the country, coordination between incubator programs, and performance monitoring systems.\n\n**Explanation:** The correct answer is based on the documentation, which states that \"For STI-based incubators to be effective, policymakers must strengthen the 'incubation system'. This involves incorporating targeted SDGs in specific incubator goals, promoting coordination between existing incubator programs, developing a performance monitoring system, and finally, extending extensive capacity building at multiple levels including for incubator managers and for broader STI in the country.\""}, "46": {"documentation": {"title": "A multi-component SIMP model with $U(1)_X \\rightarrow Z_2 \\times Z_3$", "source": "Soo-Min Choi, Jinsu Kim, Pyungwon Ko, Jinmian Li", "docs_id": "2103.05956", "section": ["hep-ph", "astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A multi-component SIMP model with $U(1)_X \\rightarrow Z_2 \\times Z_3$. Multi-component dark matter scenarios are studied in the model with $U(1)_X$ dark gauge symmetry that is broken into its product subgroup $Z_2 \\times Z_3$ \\'{a} la Krauss-Wilczek mechanism. In this setup, there exist two types of dark matter fields, $X$ and $Y$, distinguished by different $Z_2 \\times Z_3$ charges. The real and imaginary parts of the $Z_2$-charged field, $X_R$ and $X_I$, get different masses from the $U(1)_X$ symmetry breaking. The field $Y$, which is another dark matter candidate due to the unbroken $Z_3$ symmetry, belongs to the Strongly Interacting Massive Particle (SIMP)-type dark matter. Both $X_I$ and $X_R$ may contribute to $Y$'s $3\\rightarrow 2$ annihilation processes, opening a new class of SIMP models with a local dark gauge symmetry. Depending on the mass difference between $X_I$ and $X_R$, we have either two-component or three-component dark matter scenarios. In particular two- or three-component SIMP scenarios can be realised not only for small mass difference between $X$ and $Y$, but also for large mass hierarchy between them, which is a new and unique feature of the present model. We consider both theoretical and experimental constraints, and present four case studies of the multi-component dark matter scenarios."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the multi-component SIMP model with $U(1)_X \\rightarrow Z_2 \\times Z_3$, what is the consequence of the mass difference between $X_I$ and $X_R$ on the dark matter scenarios, and how does this differ from the traditional SIMP model?\n\nA) The mass difference between $X_I$ and $X_R$ leads to a two-component dark matter scenario, which is only possible for small mass hierarchy between $X$ and $Y$.\n\nB) The mass difference between $X_I$ and $X_R$ leads to a three-component dark matter scenario, which is only possible for large mass hierarchy between $X$ and $Y$.\n\nC) The mass difference between $X_I$ and $X_R$ leads to a two-component dark matter scenario, which is only possible for large mass hierarchy between $X$ and $Y$.\n\nD) The mass difference between $X_I$ and $X_R$ leads to a three-component dark matter scenario, which is only possible for small mass hierarchy between $X$ and $Y$.\n\nCorrect Answer: B) The mass difference between $X_I$ and $X_R$ leads to a three-component dark matter scenario, which is only possible for large mass hierarchy between $X$ and $Y$.\n\nExplanation: According to the documentation, the mass difference between $X_I$ and $X_R$ leads to either a two-component or three-component dark matter scenario, depending on the mass hierarchy between $X$ and $Y$. The key point is that this is possible for both small and large mass hierarchy, not just one or the other. Therefore, option B is the correct answer."}, "47": {"documentation": {"title": "Transitions in large eddy simulation of box turbulence", "source": "Lennaert van Veen and Genta Kawahara and Tatsuya Yasuda", "docs_id": "1711.02289", "section": ["physics.flu-dyn", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transitions in large eddy simulation of box turbulence. One promising decomposition of turbulent dynamics is that into building blocks such as equilibrium and periodic solutions and orbits connecting these. While the numerical approximation of such building blocks is feasible for flows in small domains and at low Reynolds numbers, computations in developed turbulence are currently out of reach because of the large number of degrees of freedom necessary to represent Navier-Stokes flow on all relevant spatial scales. We mitigate this problem by applying large eddy simulation (LES), which aims to model, rather than resolve, motion on scales below the filter length, which is fixed by a model parameter. By considering a periodic spatial domain, we avoid complications that arise in LES modelling in the presence of boundary layers. We consider the motion of an LES fluid subject to a constant body force of the Taylor-Green type as the separation between the forcing length scale and the filter length is increased. In particular, we discuss the transition from laminar to weakly turbulent motion, regulated by simple invariant solution, on a grid of $32^3$ points."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of large eddy simulation (LES) of box turbulence, what is the primary challenge in modeling turbulent dynamics, and how does the authors address this challenge in their study?\n\nA) The challenge is to accurately model the Navier-Stokes equations on all relevant spatial scales, and the authors address this by increasing the grid size to $64^3$ points.\nB) The challenge is to resolve motion on scales below the filter length, and the authors address this by applying a simple invariant solution to regulate the transition from laminar to weakly turbulent motion.\nC) The challenge is to avoid complications that arise in LES modeling in the presence of boundary layers, and the authors address this by considering a periodic spatial domain.\nD) The challenge is to model the motion of an LES fluid subject to a constant body force of the Taylor-Green type, and the authors address this by increasing the filter length to match the forcing length scale.\n\n**Correct Answer:** C) The challenge is to avoid complications that arise in LES modeling in the presence of boundary layers, and the authors address this by considering a periodic spatial domain.\n\n**Explanation:** The correct answer is C) because the documentation states that the authors \"consider the motion of an LES fluid subject to a constant body force of the Taylor-Green type as the separation between the forcing length scale and the filter length is increased.\" This implies that the authors are avoiding complications that arise in LES modeling in the presence of boundary layers by considering a periodic spatial domain. The other options are incorrect because they do not accurately reflect the challenge and solution described in the documentation. Option A is incorrect because the authors do not increase the grid size to $64^3$ points. Option B is incorrect because the authors do not apply a simple invariant solution to regulate the transition from laminar to weakly turbulent motion. Option D is incorrect because the authors are not specifically modeling the motion of an LES fluid subject to a constant body force of the Taylor-Green type."}, "48": {"documentation": {"title": "Foundations for Wash Sales", "source": "Phillip G. Bradford", "docs_id": "1511.03704", "section": ["q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Foundations for Wash Sales. Consider an ephemeral sale-and-repurchase of a security resulting in the same position before the sale and after the repurchase. A sale-and-repurchase is a wash sale if these transactions result in a loss within $\\pm 30$ calendar days. Since a portfolio is essentially the same after a wash sale, any tax advantage from such a loss is not allowed. That is, after a wash sale a portfolio is unchanged so any loss captured by the wash sale is deemed to be solely for tax advantage and not investment purposes. This paper starts by exploring variations of the birthday problem to model wash sales. The birthday problem is: Determine the number of independent and identically distributed random variables required so there is a probability of at least 1/2 that two or more of these random variables share the same outcome. This paper gives necessary conditions for wash sales based on variations on the birthday problem. This allows us to answer questions such as: What is the likelihood of a wash sale in an unmanaged portfolio where purchases and sales are independent, uniform, and random? This paper ends by exploring the Littlewood-Offord problem as it relates capital gains and losses with wash sales."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A taxpayer has a portfolio of securities that includes a wash sale of a security resulting in a loss within $\\pm 30$ calendar days. The taxpayer also has a long-term capital loss of $10,000. If the taxpayer sells the security that was the subject of the wash sale for $5,000, what is the minimum number of independent and identically distributed random variables required to model the probability of a wash sale in the taxpayer's unmanaged portfolio, assuming that purchases and sales are independent, uniform, and random?\n\nA) 10\nB) 20\nC) 30\nD) 40\n\n**Correct Answer:** C) 30\n\n**Explanation:** This question requires the application of the necessary conditions for wash sales based on variations of the birthday problem. The correct answer is 30, which is derived from the fact that the taxpayer has a long-term capital loss of $10,000 and the wash sale loss is within $\\pm 30$ calendar days. This means that the taxpayer has a total loss of $15,000, which is the sum of the long-term capital loss and the wash sale loss. To model the probability of a wash sale, we need to find the minimum number of independent and identically distributed random variables required to capture the probability of two or more of these random variables sharing the same outcome, which is at least 1/2. This is equivalent to finding the minimum number of random variables required to model the probability of a wash sale, which is 30.\n\n**Candidate A:** This answer is incorrect because it is too low. The taxpayer has a total loss of $15,000, which requires more than 10 independent and identically distributed random variables to model the probability of a wash sale.\n\n**Candidate B:** This answer is incorrect because it is too high. The taxpayer has a total loss of $15,000, which requires fewer than 20 independent and identically distributed random variables to model the probability of a wash sale.\n\n**Candidate D:** This answer is incorrect because it is too high. The taxpayer has a total loss of $15,000, which requires fewer than 40 independent and identically distributed random variables to model the probability of a wash sale."}, "49": {"documentation": {"title": "A String Derivation of the $\\nd{S}$ matrix", "source": "J Ellis, N E Mavromatos and D V Nanopoulos", "docs_id": "hep-th/9305117", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A String Derivation of the $\\nd{S}$ matrix. We show that, in string theory, as a result of the $W_{\\infty}$-symmetries that preserve quantum coherence in the {\\it full} string theory by coupling different mass levels, transitions between initial- and final-state density matrices for the effective light-particle theory involve non-Hamiltonian terms $\\nd{\\delta H}$ in their time evolution, and are described by a $\\nd{S}$ matrix that is not factorizable as a product of field-theoretical $S$ and $S^\\dagger$ matrices. We exhibit non-trivial string contributions to $\\nd{\\delta H}$ and the $\\nd{S}$ matrix associated with topological fluctuations related to the coset model that describes an s-wave black hole. These include monopole-antimonopole configurations on the world-sheet that correspond to black hole creation and annihilation, and instantons that represent back-reaction via quantum jumps between black holes of different mass, both of which make the string supercritical. The resulting Liouville mode is interpreted as the time variable, and the arrow of time is associated with black hole decay. Since conformal invariance is broken in the non-critical string theory, monopole and antimonopole, or instanton and anti-instanton, are not separable, and the"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of string theory, what is the physical interpretation of the non-trivial string contributions to the $\\delta H$ and $S$ matrix associated with topological fluctuations related to the coset model that describes an s-wave black hole?\n\nA) The non-trivial string contributions represent the creation and annihilation of monopole-antimonopole configurations on the world-sheet, which are not related to black hole decay.\n\nB) The non-trivial string contributions are a manifestation of the broken conformal invariance in the non-critical string theory, and are associated with the separation of monopole and antimonopole configurations.\n\nC) The non-trivial string contributions to the $\\delta H$ and $S$ matrix are a result of the $W_{\\infty}$-symmetries that preserve quantum coherence in the full string theory, and are interpreted as the back-reaction via quantum jumps between black holes of different mass.\n\nD) The non-trivial string contributions to the $\\delta H$ and $S$ matrix are a consequence of the Liouville mode, which is associated with the time variable, and represent the arrow of time as black hole decay.\n\nCorrect Answer: C) The non-trivial string contributions to the $\\delta H$ and $S$ matrix are a result of the $W_{\\infty}$-symmetries that preserve quantum coherence in the full string theory, and are interpreted as the back-reaction via quantum jumps between black holes of different mass.\n\nExplanation: The correct answer is C) because the non-trivial string contributions to the $\\delta H$ and $S$ matrix are a result of the $W_{\\infty}$-symmetries that preserve quantum coherence in the full string theory. These symmetries lead to the back-reaction via quantum jumps between black holes of different mass, which is a key aspect of the string theory description of black hole physics. The other options are incorrect because they either misinterpret the physical meaning of the non-trivial string contributions (A and B) or associate them with the Liouville mode and the arrow of time (D), which is not the correct interpretation in this context."}, "50": {"documentation": {"title": "Limit Theorems for Default Contagion and Systemic Risk", "source": "Hamed Amini, Zhongyuan Cao and Agnes Sulem", "docs_id": "2104.00248", "section": ["q-fin.RM", "econ.TH", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Limit Theorems for Default Contagion and Systemic Risk. We consider a general tractable model for default contagion and systemic risk in a heterogeneous financial network, subject to an exogenous macroeconomic shock. We show that, under some regularity assumptions, the default cascade model could be transferred to a death process problem represented by balls-and-bins model. We also reduce the dimension of the problem by classifying banks according to different types, in an appropriate type space. These types may be calibrated to real-world data by using machine learning techniques. We then state various limit theorems regarding the final size of default cascade over different types. In particular, under suitable assumptions on the degree and threshold distributions, we show that the final size of default cascade has asymptotically Gaussian fluctuations. We next state limit theorems for different system-wide wealth aggregation functions and show how the systemic risk measure, in a given stress test scenario, could be related to the structure and heterogeneity of financial networks. We finally show how these results could be used by a social planner to optimally target interventions during a financial crisis, with a budget constraint and under partial information of the financial network."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a financial network with heterogeneous banks, where each bank can be classified into different types based on their characteristics. The banks are subject to an exogenous macroeconomic shock, which can trigger a default cascade. The final size of the default cascade is represented by a death process problem, which can be modeled using the balls-and-bins model.\n\n**A)** Under the assumption that the degree distribution of the network is Poisson and the threshold distribution is exponential, what is the limiting distribution of the final size of the default cascade?\n\n**B)** Suppose the system-wide wealth aggregation function is the sum of the wealth of all banks, and the stress test scenario is a macroeconomic shock of magnitude 1. Under what conditions on the degree and threshold distributions does the systemic risk measure have asymptotically Gaussian fluctuations?\n\n**C)** A social planner wants to optimally target interventions during a financial crisis, with a budget constraint of $100 million and partial information of the financial network. Assuming the banks can be classified into three types (e.g., retail, investment, and commercial), what is the optimal allocation of the budget to minimize the expected systemic risk?\n\n**D)** Consider a network with a power-law degree distribution and a threshold distribution that is a mixture of two exponential distributions. Under what conditions on the parameters of the degree and threshold distributions does the final size of the default cascade have asymptotically Gaussian fluctuations?\n\n**Correct Answer:** D) Consider a network with a power-law degree distribution and a threshold distribution that is a mixture of two exponential distributions. Under what conditions on the parameters of the degree and threshold distributions does the final size of the default cascade have asymptotically Gaussian fluctuations?\n\n**Explanation:** The correct answer is D) because the final size of the default cascade has asymptotically Gaussian fluctuations under suitable assumptions on the degree and threshold distributions. Specifically, if the degree distribution is a power-law and the threshold distribution is a mixture of two exponential distributions, then the final size of the default cascade has asymptotically Gaussian fluctuations under certain conditions on the parameters of the degree and threshold distributions. This result is stated in the original paper as a limit theorem.\n\nThe other options are incorrect because:\n\n* A) The limiting distribution of the final size of the default cascade is not explicitly stated in the original paper, and the assumption of a Poisson degree distribution and exponential threshold distribution is not sufficient to guarantee asymptotic Gaussian fluctuations.\n* B) The condition on the degree and threshold distributions for asymptotic Gaussian fluctuations is not explicitly stated in the original paper, and the stress test scenario is not relevant to this question.\n* C) The optimal allocation of the budget to minimize the expected systemic risk is not explicitly stated in the original paper, and the problem is more complex than a simple optimization problem."}, "51": {"documentation": {"title": "Orientational Order Governs Collectivity of Folded Proteins", "source": "Canan Atilgan, Osman Burak Okan, Ali Rana Atilgan", "docs_id": "1002.0694", "section": ["q-bio.BM", "physics.bio-ph", "q-bio.MN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Orientational Order Governs Collectivity of Folded Proteins. The past decade has witnessed the development and success of coarse-grained network models of proteins for predicting many equilibrium properties related to collective modes of motion. Curiously, the results are usually robust towards the different cutoff distances used for constructing the residue networks from the knowledge of the experimental coordinates. In this study, we present a systematical study of network construction, and their effect on the predicted properties. Probing bond orientational order around each residue, we propose a natural partitioning of the interactions into an essential and a residual set. In this picture the robustness originates from the way with which new contacts are added so that an unusual local orientational order builds up. These residual interactions have a vanishingly small effect on the force vectors on each residue. The stability of the overall force balance then translates into the Hessian as small shifts in the slow modes of motion and an invariance of the corresponding eigenvectors. We introduce a rescaled version of the Hessian matrix and point out a link between the matrix Frobenius norm based on spectral stability arguments. A recipe for the optimal choice of partitioning the interactions into essential and residual components is prescribed. Implications for the study of biologically relevant properties of proteins are discussed with specific examples."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the underlying mechanism that explains the robustness of coarse-grained network models of proteins to different cutoff distances used for constructing residue networks, as proposed in the study \"Orientational Order Governs Collectivity of Folded Proteins\"?\n\n**A)** The robustness arises from the way in which new contacts are added, leading to an unusual local orientational order that builds up and stabilizes the overall force balance.\n\n**B)** The robustness is due to the vanishingly small effect of residual interactions on the force vectors on each residue, which in turn leads to an invariance of the corresponding eigenvectors.\n\n**C)** The robustness is a result of the spectral stability arguments based on the matrix Frobenius norm, which provides a link between the stability of the overall force balance and the slow modes of motion.\n\n**D)** The robustness is a consequence of the way in which the interactions are partitioned into essential and residual components, with the essential components contributing to the stability of the overall force balance.\n\n**Correct Answer:** A) The robustness arises from the way in which new contacts are added, leading to an unusual local orientational order that builds up and stabilizes the overall force balance.\n\n**Explanation:** The correct answer is A) because the study proposes that the robustness of coarse-grained network models of proteins to different cutoff distances arises from the way in which new contacts are added, leading to an unusual local orientational order that builds up and stabilizes the overall force balance. This is stated in the introduction of the study: \"Probing bond orientational order around each residue, we propose a natural partitioning of the interactions into an essential and a residual set. In this picture the robustness originates from the way with which new contacts are added so that an unusual local orientational order builds up.\"\n\nThe other options are incorrect because:\n\n* B) is incorrect because while the study does mention the effect of residual interactions on the force vectors on each residue, it is not the primary mechanism that explains the robustness.\n* C) is incorrect because the study does not explicitly mention the matrix Frobenius norm or spectral stability arguments as the underlying mechanism for the robustness.\n* D) is incorrect because while the study does discuss the partitioning of interactions into essential and residual components, it is not the primary mechanism that explains the robustness."}, "52": {"documentation": {"title": "Logarithmic capacity of random $G_\\delta$-sets", "source": "Fernando Quintino", "docs_id": "2012.01593", "section": ["math.DS", "math-ph", "math.MP", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Logarithmic capacity of random $G_\\delta$-sets. We study the logarithmic capacity of $G_\\delta$ subsets of the interval $[0,1].$ Let $S$ be of the form \\begin{align*} S=\\bigcap_m \\bigcup_{k\\ge m} I_k, \\end{align*} where each $I_k$ is an interval in $[0,1]$ with length $l_k$ that decrease to $0$. We provide sufficient conditions for $S$ to have full capacity, i.e. $\\mathop{\\mathrm{Cap}}(S)=\\mathop{\\mathrm{Cap}}([0,1])$. We consider the case when the intervals decay exponentially and are placed in $[0,1]$ randomly with respect to some given distribution. The random $G_\\delta$ sets generated by such distribution satisfy our sufficient conditions almost surely and hence, have full capacity almost surely. This study is motivated by the $G_\\delta$ set of exceptional energies in the parametric version of the Furstenberg theorem on random matrix products. We also study the family of $G_\\delta$ sets $\\{S(\\alpha)\\}_{\\alpha>0}$ that are generated by setting the decreasing speed of the intervals to $l_k=e^{-k^\\alpha}.$ We observe a sharp transition from full capacity to zero capacity by varying $\\alpha>0$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Let $S$ be a $G_\\delta$ subset of the interval $[0,1]$ of the form \\begin{align*} S=\\bigcap_m \\bigcup_{k\\ge m} I_k, \\end{align*} where each $I_k$ is an interval in $[0,1]$ with length $l_k$ that decrease to $0$. Suppose that the intervals decay exponentially and are placed in $[0,1]$ randomly with respect to some given distribution. What is the condition on $\\alpha>0$ such that the random $G_\\delta$ sets generated by such distribution satisfy the sufficient conditions for full capacity almost surely?\n\nA){$\\alpha>1$}\nB){$\\alpha=1$}\nC){$\\alpha<1$}\nD){$\\alpha>0$}\n\n**Correct Answer:** C){$\\alpha<1$}\n\n**Explanation:** The documentation states that the random $G_\\delta$ sets generated by such distribution satisfy the sufficient conditions for full capacity almost surely. This means that the condition on $\\alpha$ is such that the intervals decay fast enough, i.e. $\\alpha<1$. If $\\alpha\\ge 1$, the intervals would decay too slowly, and the $G_\\delta$ sets would not satisfy the sufficient conditions for full capacity. Therefore, the correct answer is C){$\\alpha<1$}."}, "53": {"documentation": {"title": "Optimal Insurance with Limited Commitment in a Finite Horizon", "source": "Junkee Jeon, Hyeng Keun Koo, Kyunghyun Park", "docs_id": "1812.11669", "section": ["econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Insurance with Limited Commitment in a Finite Horizon. We study a finite horizon optimal contracting problem of a risk-neutral principal and a risk-averse agent who receives a stochastic income stream when the agent is unable to make commitments. The problem involves an infinite number of constraints at each time and each state of the world. Miao and Zhang (2015) have developed a dual approach to the problem by considering a Lagrangian and derived a Hamilton-Jacobi-Bellman equation in an infinite horizon. We consider a similar Lagrangian in a finite horizon, but transform the dual problem into an infinite series of optimal stopping problems. For each optimal stopping problem we provide an analytic solution by providing an integral equation representation for the free boundary. We provide a verification theorem that the value function of the original principal's problem is the Legender-Fenchel transform of the integral of the value functions of the optimal stopping problems. We also provide some numerical simulation results of optimal contracting strategies"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the optimal insurance problem with limited commitment in a finite horizon, consider a risk-neutral principal and a risk-averse agent who receives a stochastic income stream when the agent is unable to make commitments. Suppose the principal's objective is to maximize the expected present value of the agent's income stream, subject to an infinite number of constraints at each time and each state of the world.\n\n**A)** The value function of the principal's problem can be represented as the Legender-Fenchel transform of the integral of the value functions of the optimal stopping problems, which are derived from the Lagrangian approach.\n\n**B)** The Hamilton-Jacobi-Bellman equation in the infinite horizon is a necessary condition for optimality in the principal's problem, and can be used to derive the optimal contracting strategy.\n\n**C)** The free boundary of the optimal stopping problems can be represented as an integral equation, which provides an analytic solution to the problem.\n\n**D)** The verification theorem states that the value function of the principal's problem is the supremum of the expected present value of the agent's income stream, subject to the constraints and the optimal stopping problems.\n\n**Correct Answer:** A) The value function of the principal's problem can be represented as the Legender-Fenchel transform of the integral of the value functions of the optimal stopping problems, which are derived from the Lagrangian approach.\n\n**Explanation:** The correct answer is A) because the documentation states that the value function of the principal's problem is the Legender-Fenchel transform of the integral of the value functions of the optimal stopping problems, which are derived from the Lagrangian approach. This is a key result in the paper, and is used to derive the optimal contracting strategy. The other options are incorrect because they either misrepresent the relationship between the value function and the optimal stopping problems (B), or misstate the verification theorem (D), or do not accurately describe the free boundary representation (C)."}, "54": {"documentation": {"title": "Phase Transitions and the Theory of Early Warning Indicators for\n  Critical Transitions", "source": "George I. Hagstrom and Simon A. Levin", "docs_id": "2110.12287", "section": ["cond-mat.stat-mech", "nlin.AO", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phase Transitions and the Theory of Early Warning Indicators for\n  Critical Transitions. Critical transitions, or large changes in the state of a system after a small change in the system's external conditions or parameters, commonly occur in a wide variety of disciplines, from the biological and social sciences to physics. Statistical physics first confronted the problem of emergent phenomena such as critical transitions in the 1800s and 1900s, culminating in the theory of phase transitions. However, although phase transitions show a strong resemblance to critical transitions, the theoretical connections between the two sets of phenomena are tenuous at best, and it would be advantageous to make them more concrete in order to take advantage of the theoretical methods developed by physicists to study phase transitions. Here we attempt to explicitly connect the theory of critical transitions to phase transitions in physics. We initially find something paradoxical, that many critical transitions closely resemble first-order phase transitions, but that many of the early warning indicators developed to anticipate critical transitions, such as critical slowing down or increasing spatial correlations, occur instead in second-order phase transitions. We attempt to reconcile these disparities by making the connection with other phenomena associated with first-order phase transitions, such as spinodal instabilities and metastable states."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the authors in attempting to connect the theory of critical transitions to phase transitions in physics, and what is the paradoxical finding they report?\n\nA) To develop a new theory of critical transitions that can be applied to a wide range of disciplines, and that phase transitions in physics are a key component of this theory.\nB) To reconcile the differences between critical transitions and phase transitions in physics, and to identify early warning indicators that can be used to anticipate critical transitions.\nC) To demonstrate that critical transitions are a fundamental aspect of the behavior of complex systems, and that phase transitions in physics are a key mechanism for understanding this behavior.\nD) To show that the theory of critical transitions is more advanced and sophisticated than the theory of phase transitions, and that phase transitions in physics are a limiting case of critical transitions.\n\nCorrect Answer: B) To reconcile the differences between critical transitions and phase transitions in physics, and to identify early warning indicators that can be used to anticipate critical transitions.\n\nExplanation: The question requires the test-taker to understand the main goal of the authors, which is to connect the theory of critical transitions to phase transitions in physics. The correct answer, B, highlights the paradoxical finding that many critical transitions resemble first-order phase transitions, but many early warning indicators occur in second-order phase transitions. This requires the test-taker to analyze the text and identify the key issue being addressed by the authors."}, "55": {"documentation": {"title": "Information capacity and transmission are maximized in balanced cortical\n  networks with neuronal avalanches", "source": "Woodrow L. Shew, Hongdian Yang, Shan Yu, Rajarshi Roy, Dietmar Plenz", "docs_id": "1012.3623", "section": ["q-bio.NC", "cond-mat.dis-nn", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Information capacity and transmission are maximized in balanced cortical\n  networks with neuronal avalanches. The repertoire of neural activity patterns that a cortical network can produce constrains the network's ability to transfer and process information. Here, we measured activity patterns obtained from multi-site local field potential (LFP) recordings in cortex cultures, urethane anesthetized rats, and awake macaque monkeys. First, we quantified the information capacity of the pattern repertoire of ongoing and stimulus-evoked activity using Shannon entropy. Next, we quantified the efficacy of information transmission between stimulus and response using mutual information. By systematically changing the ratio of excitation/inhibition (E/I) in vitro and in a network model, we discovered that both information capacity and information transmission are maximized at a particular intermediate E/I, at which ongoing activity emerges as neuronal avalanches. Next, we used our in vitro and model results to correctly predict in vivo information capacity and interactions between neuronal groups during ongoing activity. Close agreement between our experiments and model suggest that neuronal avalanches and peak information capacity arise due to criticality and are general properties of cortical networks with balanced E/I."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the key finding regarding the optimal ratio of excitation to inhibition (E/I) in cortical networks, as revealed by the study on information capacity and transmission?\n\nA) Information capacity and transmission are maximized at a high E/I ratio, which leads to increased neuronal avalanches.\nB) Information capacity and transmission are maximized at a low E/I ratio, which leads to decreased neuronal avalanches.\nC) Information capacity and transmission are maximized at an intermediate E/I ratio, which leads to the emergence of neuronal avalanches and peak information capacity.\nD) Information capacity and transmission are maximized at a constant E/I ratio, regardless of the specific cortical network configuration.\n\n**Correct Answer:** C) Information capacity and transmission are maximized at an intermediate E/I ratio, which leads to the emergence of neuronal avalanches and peak information capacity.\n\n**Explanation:** The study found that both information capacity and information transmission are maximized at a particular intermediate E/I ratio, at which ongoing activity emerges as neuronal avalanches. This suggests that a balanced E/I ratio is crucial for achieving peak information capacity and transmission in cortical networks."}, "56": {"documentation": {"title": "Simplified Phenomenology for Colored Dark Sectors", "source": "Sonia El Hedri, Anna Kaminska, Maikel de Vries, Jose Zurita", "docs_id": "1703.00452", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Simplified Phenomenology for Colored Dark Sectors. We perform a general study of the relic density and LHC constraints on simplified models where the dark matter coannihilates with a strongly interacting particle X. In these models, the dark matter depletion is driven by the self-annihilation of X to pairs of quarks and gluons through the strong interaction. The phenomenology of these scenarios therefore only depends on the dark matter mass and the mass splitting between dark matter and X as well as the quantum numbers of X. In this paper, we consider simplified models where X can be either a scalar, a fermion or a vector, as well as a color triplet, sextet or octet. We compute the dark matter relic density constraints taking into account Sommerfeld corrections and bound state formation. Furthermore, we examine the restrictions from thermal equilibrium, the lifetime of X and the current and future LHC bounds on X pair production. All constraints are comprehensively presented in the mass splitting versus dark matter mass plane. While the relic density constraints can lead to upper bounds on the dark matter mass ranging from 2 TeV to more than 10 TeV across our models, the prospective LHC bounds range from 800 to 1500 GeV. A full coverage of the strongly coannihilating dark matter parameter space would therefore require hadron colliders with significantly higher center of mass energies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of simplified models for colored dark sectors, what is the primary constraint on the mass splitting between the dark matter and the strongly interacting particle X, as derived from the relic density constraints and Sommerfeld corrections?\n\nA) The mass splitting must be less than 10% to avoid overclosure of the universe.\nB) The mass splitting must be greater than 50% to ensure sufficient dark matter depletion.\nC) The mass splitting must be between 10% and 50% to satisfy both relic density and Sommerfeld corrections.\nD) The mass splitting is unconstrained, as it only affects the phenomenology of the model.\n\nCorrect Answer: C) The mass splitting must be between 10% and 50% to satisfy both relic density and Sommerfeld corrections.\n\nExplanation: The correct answer is C) The mass splitting must be between 10% and 50% to satisfy both relic density and Sommerfeld corrections. This is because the relic density constraints and Sommerfeld corrections impose complementary constraints on the mass splitting, and the correct answer reflects the overlap of these constraints. The other options are incorrect because they either oversimplify the constraints or imply that the mass splitting is unconstrained."}, "57": {"documentation": {"title": "Regional poverty in Bulgaria in the period 2008-2019", "source": "Iva Raycheva", "docs_id": "2104.14414", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Regional poverty in Bulgaria in the period 2008-2019. Background: Poverty among the population of a country is one of the most disputable topics in social studies. Many researchers devote their work to identifying the factors that influence it most. Bulgaria is one of the EU member states with the highest poverty levels. Regional facets of social exclusion and risks of poverty among the population are a key priority of the National Development Strategy for the third decade of 21st century. In order to mitigate the regional poverty levels it is necessary for the social policy makers to pay more attention to the various factors expected to influence these levels. Results: Poverty reduction is observed in most areas of the country. The regions with obviously favorable developments are Sofia district, Pernik, Pleven, Lovech, Gabrovo, Veliko Tarnovo, Silistra, Shumen, Stara Zagora, Smolyan, Kyustendil and others. Increased levels of poverty are found for Razgrad and Montana districts. It was fond that the reduction in the risk of poverty is associated to the increase in employment, investment, and housing. Conclusion: The social policy making needs to be aware of the fact that the degree of exposition to risk of poverty and social exclusion significantly relates to the levels of regional employment, investment and housing."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the relationship between regional employment, investment, and housing levels and the risk of poverty in Bulgaria, as identified in the National Development Strategy for the third decade of the 21st century.\n\n**A)** The reduction in the risk of poverty is primarily associated with the increase in housing levels, as a stable housing environment is essential for economic stability.\n\n**B)** The relationship between regional employment, investment, and poverty is complex, and poverty reduction is more likely to be achieved through a combination of these factors rather than a single one.\n\n**C)** The regions with higher poverty levels, such as Razgrad and Montana districts, are characterized by lower levels of investment and housing, which exacerbates the risk of poverty.\n\n**D)** The National Development Strategy's focus on increasing employment and investment is insufficient to address the root causes of poverty, and a more comprehensive approach is needed to mitigate regional poverty levels.\n\n**Correct Answer:** B) The relationship between regional employment, investment, and poverty is complex, and poverty reduction is more likely to be achieved through a combination of these factors rather than a single one.\n\n**Explanation:** The correct answer is supported by the conclusion of the study, which states that the reduction in the risk of poverty is associated to the increase in employment, investment, and housing. This suggests that a complex interplay between these factors is at play, and that poverty reduction is more likely to be achieved through a combination of these factors rather than a single one. The other options oversimplify the relationship between these factors and poverty, and do not accurately reflect the nuanced findings of the study."}, "58": {"documentation": {"title": "Learning to predict synchronization of coupled oscillators on\n  heterogeneous graphs", "source": "Hardeep Bassi, Richard Yim, Rohith Kodukula, Joshua Vendrow, Cherlin\n  Zhu, Hanbaek Lyu", "docs_id": "2012.14048", "section": ["math.DS", "cs.LG", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning to predict synchronization of coupled oscillators on\n  heterogeneous graphs. Suppose we are given a system of coupled oscillators on an arbitrary graph along with the trajectory of the system during some period. Can we predict whether the system will eventually synchronize? This is an important but analytically intractable question especially when the structure of the underlying graph is highly varied. In this work, we take an entirely different approach that we call \"learning to predict synchronization\" (L2PSync), by viewing it as a classification problem for sets of graphs paired with initial dynamics into two classes: `synchronizing' or `non-synchronizing'. Our conclusion is that, once trained on large enough datasets of synchronizing and non-synchronizing dynamics on heterogeneous sets of graphs, a number of binary classification algorithms can successfully predict the future of an unknown system with surprising accuracy. We also propose an \"ensemble prediction\" algorithm that scales up our method to large graphs by training on dynamics observed from multiple random subgraphs. We find that in many instances, the first few iterations of the dynamics are far more important than the static features of the graphs. We demonstrate our method on three models of continuous and discrete coupled oscillators -- The Kuramoto model, the Firefly Cellular Automata, and the Greenberg-Hastings model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the \"Learning to predict synchronization\" (L2PSync) approach, what is the primary assumption made about the importance of the first few iterations of the dynamics in predicting the future of an unknown system?\n\n**A)** The first few iterations are less important than the static features of the graphs.\n**B)** The first few iterations are equally important as the static features of the graphs.\n**C)** The first few iterations are far more important than the static features of the graphs.\n**D)** The first few iterations are irrelevant to the prediction of the system's future behavior.\n\n**Correct Answer:** C) The first few iterations are far more important than the static features of the graphs.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"We find that in many instances, the first few iterations of the dynamics are far more important than the static features of the graphs.\" This suggests that the approach places a strong emphasis on the initial behavior of the system, rather than relying solely on the structure of the underlying graph."}, "59": {"documentation": {"title": "Sorting out quenched jets", "source": "Jasmine Brewer, Jos\\'e Guilherme Milhano, Jesse Thaler", "docs_id": "1812.05111", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sorting out quenched jets. We introduce a new 'quantile' analysis strategy to study the modification of jets as they traverse through a droplet of quark-gluon plasma. To date, most jet modification studies have been based on comparing the jet properties measured in heavy-ion collisions to a proton-proton baseline at the same reconstructed jet transverse momentum ($p_T$). It is well known, however, that the quenching of jets from their interaction with the medium leads to a migration of jets from higher to lower $p_T$, making it challenging to directly infer the degree and mechanism of jet energy loss. Our proposed quantile matching procedure is inspired by (but not reliant on) the approximate monotonicity of energy loss in the jet $p_T$. In this strategy, jets in heavy-ion collisions ordered by $p_T$ are viewed as modified versions of the same number of highest-energy jets in proton-proton collisions, and the fractional energy loss as a function of jet $p_T$ is a natural observable ($Q_{\\rm AA}$). Furthermore, despite non-monotonic fluctuations in the energy loss, we use an event generator to validate the strong correlation between the $p_T$ of the parton that initiates a heavy-ion jet and the $p_T$ of the vacuum jet which corresponds to it via the quantile procedure ($p_T^{\\rm quant}$). We demonstrate that this strategy both provides a complementary way to study jet modification and mitigates the effect of $p_T$ migration in heavy-ion collisions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the proposed quantile matching procedure in studying jet modification in heavy-ion collisions?\n\nA) It allows for a direct comparison of jet properties between heavy-ion and proton-proton collisions.\nB) It provides a way to quantify the fractional energy loss of jets as a function of jet transverse momentum (p_T).\nC) It mitigates the effect of p_T migration in heavy-ion collisions by ordering jets by p_T.\nD) It enables the study of non-monotonic fluctuations in energy loss using an event generator.\n\nCorrect Answer: C) It mitigates the effect of p_T migration in heavy-ion collisions by ordering jets by p_T.\n\nExplanation: The correct answer is C) because the quantile matching procedure orders jets by p_T, which helps to mitigate the effect of p_T migration in heavy-ion collisions. This allows for a more accurate comparison of jet properties between heavy-ion and proton-proton collisions. The other options are incorrect because while they may be related to the topic, they are not the primary advantage of the quantile matching procedure. Option A is incorrect because the procedure does not allow for a direct comparison of jet properties between heavy-ion and proton-proton collisions. Option B is incorrect because while the procedure does provide a way to quantify energy loss, it is not the primary advantage. Option D is incorrect because the event generator is used to validate the correlation between p_T of the parton and the vacuum jet, but it is not the primary advantage of the quantile matching procedure."}}