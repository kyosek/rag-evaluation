{"0": {"documentation": {"title": "Quantum propensity in economics", "source": "David Orrell, Monireh Houshmand", "docs_id": "2103.10938", "section": ["q-fin.GN", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantum propensity in economics. This paper describes an approach to economics that is inspired by quantum computing, and is motivated by the need to develop a consistent quantum mathematical framework for economics. The traditional neoclassical approach assumes that rational utility-optimisers drive market prices to a stable equilibrium, subject to external perturbations. While this approach has been highly influential, it has come under increasing criticism following the financial crisis of 2007/8. The quantum approach, in contrast, is inherently probabilistic and dynamic. Decision-makers are described, not by a utility function, but by a propensity function which specifies the probability of transacting. We show how a number of cognitive phenomena such as preference reversal and the disjunction effect can be modelled by using a simple quantum circuit to generate an appropriate propensity function. Conversely, a general propensity function can be quantized to incorporate effects such as interference and entanglement that characterise human decision-making. Applications to some common problems in economics and finance are discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the quantum propensity approach to economics, what is the primary difference between the traditional neoclassical approach and the quantum approach in describing decision-makers?\n\n**A)** The quantum approach assumes a fixed utility function, while the neoclassical approach assumes a fixed propensity function.\n**B)** The quantum approach is inherently deterministic, while the neoclassical approach is inherently probabilistic.\n**C)** The quantum approach describes decision-makers by a utility function, while the neoclassical approach describes decision-makers by a propensity function.\n**D)** The quantum approach assumes that decision-makers are driven by rational utility-optimisers, while the neoclassical approach assumes that decision-makers are driven by a propensity function.\n\n**Correct Answer:** C) The quantum approach describes decision-makers by a propensity function, while the neoclassical approach describes decision-makers by a utility function.\n\n**Explanation:** The quantum propensity approach is inherently probabilistic and dynamic, describing decision-makers by a propensity function which specifies the probability of transacting. In contrast, the traditional neoclassical approach assumes that rational utility-optimisers drive market prices to a stable equilibrium, subject to external perturbations. This fundamental difference in approach is a key aspect of the quantum propensity approach to economics."}, "1": {"documentation": {"title": "Constraints on light neutrino parameters derived from the study of\n  neutrinoless double beta decay", "source": "Sabin Stoica and Andrei Neacsu", "docs_id": "1405.0517", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Constraints on light neutrino parameters derived from the study of\n  neutrinoless double beta decay. The study of the neutrinoless double beta ($0 \\beta\\beta$) decay mode can provide us with important information on the neutrino properties, particularly on the electron neutrino absolute mass. In this work we revise the present constraints on the neutrino mass parameters derived from the $0 \\beta\\beta$ decay analysis of the experimentally interesting nuclei. We use the latest results for the phase space factors (PSFs) and nuclear matrix elements (NMEs), as well as for the experimental lifetimes limits. For the PSFs we use values computed with an improved method reported very recently. For the NMEs we use values chosen from literature on a case-by-case basis, taking advantage of the consensus reached by the community on several nuclear ingredients used in their calculation. Thus, we try to restrict the range of spread of the NME values calculated with different methods and, hence, to reduce the uncertainty in deriving limits for the Majorana neutrino mass parameter. Our results may be useful to have an up-date image on the present neutrino mass sensitivities associated with $0 \\beta\\beta$ measurements for different isotopes and to better estimate the range of values of the neutrino masses that can be explored in the future double beta decay (DBD) experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study of neutrinoless double beta decay ($0 \\beta\\beta$) decay mode can provide important information on the neutrino properties, particularly on the electron neutrino absolute mass. What are the key factors that the authors of the study take into account to revise the present constraints on the neutrino mass parameters derived from $0 \\beta\\beta$ decay analysis?\n\n**A)** Only the phase space factors (PSFs) and nuclear matrix elements (NMEs) are considered, without taking into account experimental lifetime limits.\n\n**B)** The authors use the latest results for PSFs and NMEs, as well as experimental lifetime limits, but also rely on literature values for NMEs.\n\n**C)** The authors use improved methods to compute PSFs, but rely on traditional methods for NMEs, without considering experimental lifetime limits.\n\n**D)** The authors use values for PSFs and NMEs that are chosen from literature, without taking into account experimental lifetime limits or revising the PSF calculations.\n\n**Correct Answer:** B) The authors use the latest results for PSFs and NMEs, as well as experimental lifetime limits, but also rely on literature values for NMEs.\n\n**Explanation:** The correct answer is B) because the authors explicitly state that they use the latest results for PSFs and NMEs, as well as experimental lifetime limits, to revise the present constraints on the neutrino mass parameters. They also mention that they rely on literature values for NMEs, taking advantage of the consensus reached by the community on several nuclear ingredients used in their calculation. This shows that the authors consider all three factors when revising the constraints on the neutrino mass parameters."}, "2": {"documentation": {"title": "Desorption energy of soft particles from a fluid interface", "source": "Hadi Mehrabian, Jacco H. Snoeijer and Jens Harting", "docs_id": "2006.11185", "section": ["cond-mat.soft", "physics.comp-ph", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Desorption energy of soft particles from a fluid interface. The efficiency of soft particles to stabilize emulsions is examined by measuring their desorption free energy, i.e., the mechanical work required to detach the particle from a fluid interface. Here, we consider rubber-like elastic as well as microgel particles, using coarse-grained molecular dynamics simulations. The energy of desorption is computed for two and three-dimensional configurations by means of the mean thermodynamic integration method. It is shown that the softness affects the particle-interface binding in two opposing directions as compared to rigid particles. On the one hand, a soft particle spreads at the interface and thereby removes a larger unfavorable liquid-liquid contact area compared to rigid particles. On the other hand, softness provides the particle with an additional degree of freedom to get reshaped instead of deforming the interface, resulting in a smaller restoring force during the detachment. It is shown that the first effect prevails so that a soft spherical particle attaches to the fluid interface more strongly than rigid spheres. Finally, we consider microgel particles both in the swollen and in the collapsed state. Surprisingly, we find that the latter has a larger binding energy. All results are rationalised using thermodynamic arguments and thereby offer detailed insights into the desorption energy of soft particles from fluid interfaces."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of softness on the desorption energy of particles from a fluid interface, according to the study?\n\nA) Soft particles have a smaller desorption energy due to increased spreading at the interface.\nB) Soft particles have a larger desorption energy due to increased deformation of the interface.\nC) Soft particles have a smaller desorption energy due to increased reshaping instead of deformation.\nD) Soft particles have a larger desorption energy due to increased spreading at the interface.\n\nCorrect Answer: C) Soft particles have a smaller desorption energy due to increased reshaping instead of deformation.\n\nExplanation: The study shows that softness provides the particle with an additional degree of freedom to get reshaped instead of deforming the interface, resulting in a smaller restoring force during the detachment. This is the primary effect of softness on the desorption energy of particles from a fluid interface."}, "3": {"documentation": {"title": "Understanding of hopping matrix for 2D materials taking 2D honeycomb and\n  square lattices as study cases", "source": "Maher Ahmed", "docs_id": "1110.6488", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "cond-mat.other"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Understanding of hopping matrix for 2D materials taking 2D honeycomb and\n  square lattices as study cases. In this work, a trial understanding for the physics underling the construction of exchange (hopping) matrix $\\mathbf{E}$ in Heisenberg model (tight binding model) for 2D materials is done. It is found that the $\\mathbf{E}$ matrix describes the particles exchange flow under short range (nearest neighbor) hopping interaction which is effected by the lattice geometry. This understanding is then used to explain the dispersion relations for the 2D honeycomb lattice with zigzag and armchair edges obtained for graphene nanoribbons and magnetic stripes. It is found that the particle flow by hopping in the zigzag nanoribbons is a translation flow and shows $\\mathbf{\\cos^2}(q_xa)$ dependance while it is a rotational flow in the armchair nanoribbons. At $q_xa/\\pi=0.5$, the particles flow in the edge sites of zigzag nanoribbons with dependance of $\\mathbf{\\cos^2}(q_xa)$ is equal to zero. At the same time there is no vertical hopping in those edge sites which lead to the appearance of peculiar zigzag flat localized edge states."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Understanding of hopping matrix for 2D materials taking 2D honeycomb and square lattices as study cases.\n\nA) The exchange (hopping) matrix $\\mathbf{E}$ in the Heisenberg model (tight binding model) for 2D materials describes the particles exchange flow under long-range (non-nearest neighbor) hopping interaction, which is independent of the lattice geometry.\n\nB) The $\\mathbf{E}$ matrix describes the particles exchange flow under short-range (nearest neighbor) hopping interaction, which is affected by the lattice geometry, and is used to explain the dispersion relations for the 2D honeycomb lattice with zigzag and armchair edges.\n\nC) The particle flow by hopping in the zigzag nanoribbons is a rotational flow and shows $\\mathbf{\\cos^2}(q_xa)$ dependence, while it is a translation flow in the armchair nanoribbons.\n\nD) The particles flow in the edge sites of zigzag nanoribbons with dependence of $\\mathbf{\\cos^2}(q_xa)$ equal to zero at $q_xa/\\pi=0.5$, and there is no vertical hopping in those edge sites, leading to the appearance of peculiar zigzag flat localized edge states.\n\n**Correct Answer:** B) The $\\mathbf{E}$ matrix describes the particles exchange flow under short-range (nearest neighbor) hopping interaction, which is affected by the lattice geometry, and is used to explain the dispersion relations for the 2D honeycomb lattice with zigzag and armchair edges.\n\n**Explanation:** The correct answer is B) because the documentation states that the $\\mathbf{E}$ matrix describes the particles exchange flow under short-range (nearest neighbor) hopping interaction, which is affected by the lattice geometry. This understanding is then used to explain the dispersion relations for the 2D honeycomb lattice with zigzag and armchair edges. The other options are incorrect because they either describe long-range hopping interaction (A), incorrect flow types (C), or a specific condition without explaining the underlying physics (D)."}, "4": {"documentation": {"title": "Policy Transforms and Learning Optimal Policies", "source": "Thomas M. Russell", "docs_id": "2012.11046", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Policy Transforms and Learning Optimal Policies. We study the problem of choosing optimal policy rules in uncertain environments using models that may be incomplete and/or partially identified. We consider a policymaker who wishes to choose a policy to maximize a particular counterfactual quantity called a policy transform. We characterize learnability of a set of policy options by the existence of a decision rule that closely approximates the maximin optimal value of the policy transform with high probability. Sufficient conditions are provided for the existence of such a rule. However, learnability of an optimal policy is an ex-ante notion (i.e. before observing a sample), and so ex-post (i.e. after observing a sample) theoretical guarantees for certain policy rules are also provided. Our entire approach is applicable when the distribution of unobservables is not parametrically specified, although we discuss how semiparametric restrictions can be used. Finally, we show possible applications of the procedure to a simultaneous discrete choice example and a program evaluation example."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Policy Transforms and Learning Optimal Policies\", what is the primary goal of the policymaker in choosing a policy to maximize a particular counterfactual quantity called a policy transform?\n\n**A)** To minimize the expected loss of the policy transform\n**B)** To maximize the expected gain of the policy transform\n**C)** To minimize the variance of the policy transform\n**D)** To maximize the expected value of the policy transform\n\n**Correct Answer:** **B)** To maximize the expected gain of the policy transform\n\n**Explanation:** The policymaker's goal is to choose a policy that maximizes the expected gain of the policy transform, which is a counterfactual quantity. This is stated in the introduction of the paper: \"We study the problem of choosing optimal policy rules in uncertain environments using models that may be incomplete and/or partially identified.\" The policymaker's objective is to maximize the expected gain of the policy transform, which is a key concept in the paper.\n\n**Note:** The other options are incorrect because:\n\n* A) Minimizing the expected loss is not the primary goal of the policymaker.\n* C) Minimizing the variance is not directly related to the policymaker's goal.\n* D) Maximizing the expected value is a related concept, but not the primary goal of the policymaker. The policymaker aims to maximize the expected gain of the policy transform, not just the expected value."}, "5": {"documentation": {"title": "Instrument Validity for Heterogeneous Causal Effects", "source": "Zhenting Sun", "docs_id": "2009.01995", "section": ["econ.EM", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Instrument Validity for Heterogeneous Causal Effects. This paper provides a general framework for testing instrument validity in heterogeneous causal effect models. The generalization includes the cases where the treatment can be multivalued (and ordered) or unordered. Based on a series of testable implications, we propose a nonparametric test which is proved to be asymptotically size controlled and consistent. Because of the nonstandard nature of the problem in question, the test statistic is constructed based on a nonsmooth map, which causes technical complications. We provide an extended continuous mapping theorem and an extended delta method, which may be of independent interest, to establish the asymptotic distribution of the test statistic under null. We then extend the bootstrap method proposed by Fang and Santos (2018) to approximate this asymptotic distribution and construct a critical value for the test. Compared to the tests in the literature, our test can be applied in more general settings and may achieve power improvement. Evidence that the test performs well on finite samples is provided via simulations. We revisit the empirical study of Card (1993) and use their data to demonstrate application of the proposed test in practice. We show that a valid instrument for a multivalued treatment may not remain valid if the treatment is coarsened."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a heterogeneous causal effect model where the treatment is multivalued and ordered. Suppose we have a valid instrument for the treatment, but the treatment is coarsened. Which of the following statements is true?\n\nA) The instrument remains valid even after coarsening the treatment.\nB) The instrument remains valid, but the estimated causal effect may be biased.\nC) The instrument becomes invalid after coarsening the treatment, and the estimated causal effect is likely to be biased.\nD) The instrument's validity cannot be determined without further information about the treatment and instrument.\n\n**Correct Answer:** C) The instrument becomes invalid after coarsening the treatment, and the estimated causal effect is likely to be biased.\n\n**Explanation:** According to the paper, \"We show that a valid instrument for a multivalued treatment may not remain valid if the treatment is coarsened.\" This statement implies that coarsening the treatment can lead to a loss of instrument validity, which in turn can result in biased estimates of the causal effect. Therefore, option C is the correct answer. Options A and B are incorrect because the paper explicitly states that the instrument may not remain valid after coarsening the treatment. Option D is also incorrect because the paper provides evidence that the instrument's validity can be affected by coarsening the treatment, even without further information."}, "6": {"documentation": {"title": "Markovian And Non-Markovian Processes with Active Decision Making\n  Strategies For Addressing The COVID-19 Pandemic", "source": "Hamid Eftekhari, Debarghya Mukherjee, Moulinath Banerjee, Ya'acov\n  Ritov", "docs_id": "2008.00375", "section": ["stat.AP", "physics.soc-ph", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Markovian And Non-Markovian Processes with Active Decision Making\n  Strategies For Addressing The COVID-19 Pandemic. We study and predict the evolution of Covid-19 in six US states from the period May 1 through August 31 using a discrete compartment-based model and prescribe active intervention policies, like lockdowns, on the basis of minimizing a loss function, within the broad framework of partially observed Markov decision processes. For each state, Covid-19 data for 40 days (starting from May 1 for two northern states and June 1 for four southern states) are analyzed to estimate the transition probabilities between compartments and other parameters associated with the evolution of the epidemic. These quantities are then used to predict the course of the epidemic in the given state for the next 50 days (test period) under various policy allocations, leading to different values of the loss function over the training horizon. The optimal policy allocation is the one corresponding to the smallest loss. Our analysis shows that none of the six states need lockdowns over the test period, though the no lockdown prescription is to be interpreted with caution: responsible mask use and social distancing of course need to be continued. The caveats involved in modeling epidemic propagation of this sort are discussed at length. A sketch of a non-Markovian formulation of Covid-19 propagation (and more general epidemic propagation) is presented as an attractive avenue for future research in this area."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the approach taken in the study \"Markovian And Non-Markovian Processes with Active Decision Making Strategies For Addressing The COVID-19 Pandemic\" to model the evolution of COVID-19 in six US states. How does the use of partially observed Markov decision processes (POMDPs) enable the authors to prescribe optimal intervention policies, such as lockdowns, to minimize a loss function?\n\n**A)** The authors use a Markovian approach to model the evolution of COVID-19, assuming that the future state of the system depends only on the current state and the actions taken so far. This allows them to estimate transition probabilities and other parameters associated with the epidemic, which are then used to predict the course of the epidemic under different policy allocations.\n\n**B)** The authors employ a non-Markovian approach, taking into account the history of the epidemic and the impact of past actions on future states. This enables them to capture the complexity of the epidemic's evolution and to develop more accurate predictions of the epidemic's course under different policy scenarios.\n\n**C)** The authors use a combination of Markovian and non-Markovian approaches to model the epidemic, allowing them to capture both the short-term and long-term effects of policy interventions. This enables them to develop a more comprehensive understanding of the epidemic's evolution and to prescribe optimal policies that balance competing objectives.\n\n**D)** The authors use a POMDP framework to model the epidemic, where the state of the system is partially observed and the transition probabilities are uncertain. This allows them to develop a more robust and flexible model that can capture the uncertainty and complexity of the epidemic's evolution, and to prescribe optimal policies that minimize a loss function.\n\n**Correct Answer:** D) The authors use a POMDP framework to model the epidemic, where the state of the system is partially observed and the transition probabilities are uncertain. This allows them to develop a more robust and flexible model that can capture the uncertainty and complexity of the epidemic's evolution, and to prescribe optimal policies that minimize a loss function.\n\n**Explanation:** The correct answer is D) because the study uses a POMDP framework to model the epidemic, which allows it to capture the uncertainty and complexity of the epidemic's evolution. The POMDP framework enables the authors to estimate transition probabilities and other parameters associated with the epidemic, which are then used to predict the course of the epidemic under different policy allocations. The use of POMDPs also enables the authors to develop a more robust and flexible model that can capture the uncertainty and complexity of the epidemic's evolution, and to prescribe optimal policies that minimize a loss function."}, "7": {"documentation": {"title": "Deep Unfolded Multicast Beamforming", "source": "Satoshi Takabe and Tadashi Wadayama", "docs_id": "2004.09345", "section": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep Unfolded Multicast Beamforming. Multicast beamforming is a promising technique for multicast communication. Providing an efficient and powerful beamforming design algorithm is a crucial issue because multicast beamforming problems such as a max-min-fair problem are NP-hard in general. Recently, deep learning-based approaches have been proposed for beamforming design. Although these approaches using deep neural networks exhibit reasonable performance gain compared with conventional optimization-based algorithms, their scalability is an emerging problem for large systems in which beamforming design becomes a more demanding task. In this paper, we propose a novel deep unfolded trainable beamforming design with high scalability and efficiency. The algorithm is designed by expanding the recursive structure of an existing algorithm based on projections onto convex sets and embedding a constant number of trainable parameters to the expanded network, which leads to a scalable and stable training process. Numerical results show that the proposed algorithm can accelerate its convergence speed by using unsupervised learning, which is a challenging training process for deep unfolding."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a novel deep unfolded trainable beamforming design, as proposed in the paper \"Deep Unfolded Multicast Beamforming\"?\n\nA) To improve the scalability of beamforming design algorithms for large systems\nB) To enhance the performance of deep learning-based approaches for beamforming design\nC) To develop a more efficient optimization-based algorithm for multicast beamforming problems\nD) To investigate the application of unsupervised learning in beamforming design\n\n**Correct Answer:** A) To improve the scalability of beamforming design algorithms for large systems\n\n**Explanation:** The question requires the test-taker to understand the context of the paper and the motivation behind the proposed algorithm. The correct answer, A, is supported by the introduction of the paper, which states that \"Providing an efficient and powerful beamforming design algorithm is a crucial issue because multicast beamforming problems such as a max-min-fair problem are NP-hard in general.\" This indicates that the primary motivation is to improve the scalability of beamforming design algorithms for large systems. The other options are incorrect because they do not accurately reflect the motivation behind the proposed algorithm. Option B is incorrect because the paper does not focus on enhancing the performance of deep learning-based approaches, but rather on developing a new algorithm. Option C is incorrect because the paper does not propose a new optimization-based algorithm, but rather a deep unfolded trainable beamforming design. Option D is incorrect because while the paper does discuss the use of unsupervised learning, it is not the primary motivation behind the proposed algorithm."}, "8": {"documentation": {"title": "Generative Feature Replay with Orthogonal Weight Modification for\n  Continual Learning", "source": "Gehui Shen, Song Zhang, Xiang Chen and Zhi-Hong Deng", "docs_id": "2005.03490", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generative Feature Replay with Orthogonal Weight Modification for\n  Continual Learning. The ability of intelligent agents to learn and remember multiple tasks sequentially is crucial to achieving artificial general intelligence. Many continual learning (CL) methods have been proposed to overcome catastrophic forgetting which results from non i.i.d data in the sequential learning of neural networks. In this paper we focus on class incremental learning, a challenging CL scenario. For this scenario, generative replay is a promising strategy which generates and replays pseudo data for previous tasks to alleviate catastrophic forgetting. However, it is hard to train a generative model continually for relatively complex data. Based on recently proposed orthogonal weight modification (OWM) algorithm which can approximately keep previously learned feature invariant when learning new tasks, we propose to 1) replay penultimate layer feature with a generative model; 2) leverage a self-supervised auxiliary task to further enhance the stability of feature. Empirical results on several datasets show our method always achieves substantial improvement over powerful OWM while conventional generative replay always results in a negative effect. Meanwhile our method beats several strong baselines including one based on real data storage. In addition, we conduct experiments to study why our method is effective."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge faced by generative replay methods in class incremental learning, and how does the proposed method address this challenge?\n\n**A)** Generative replay methods struggle to maintain the stability of previously learned features when learning new tasks, and the proposed method addresses this by leveraging a self-supervised auxiliary task to enhance feature stability.\n\n**B)** Generative replay methods are unable to generate pseudo data that is similar to the real data, and the proposed method addresses this by replaying penultimate layer features with a generative model.\n\n**C)** Generative replay methods are prone to catastrophic forgetting, and the proposed method addresses this by modifying the weights of the neural network to keep previously learned features invariant.\n\n**D)** Generative replay methods are limited by the complexity of the data, and the proposed method addresses this by using a self-supervised auxiliary task to further enhance the stability of the feature.\n\n**Correct Answer:** A) Generative replay methods struggle to maintain the stability of previously learned features when learning new tasks, and the proposed method addresses this by leveraging a self-supervised auxiliary task to enhance feature stability.\n\n**Explanation:** The correct answer is A) because the proposed method addresses the challenge of maintaining the stability of previously learned features when learning new tasks. The method leverages a self-supervised auxiliary task to further enhance the stability of the feature, which is a key component of the proposed approach. The other options are incorrect because they do not accurately describe the primary challenge faced by generative replay methods in class incremental learning, or they do not accurately describe the proposed method's approach to addressing this challenge."}, "9": {"documentation": {"title": "Synchronization in a semiclassical Kuramoto model", "source": "Ignacio Hermoso de Mendoza, Leonardo A. Pach\\'on, Jes\\'us\n  G\\'omez-Garde\\~nes, David Zueco", "docs_id": "1309.3972", "section": ["cond-mat.stat-mech", "nlin.AO", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Synchronization in a semiclassical Kuramoto model. Synchronization is a ubiquitous phenomenon occurring in social, biological, and technological systems when the internal rhythms of their constituents are adapted to be in unison as a result of their coupling. This natural tendency towards dynamical consensus has spurred a large body of theoretical and experimental research in recent decades. The Kuramoto model constitutes the most studied and paradigmatic framework in which to study synchronization. In particular, it shows how synchronization appears as a phase transition from a dynamically disordered state at some critical value for the coupling strength between the interacting units. The critical properties of the synchronization transition of this model have been widely studied and many variants of its formulations have been considered to address different physical realizations. However, the Kuramoto model has been studied only within the domain of classical dynamics, thus neglecting its applications for the study of quantum synchronization phenomena. Based on a system-bath approach and within the Feynman path-integral formalism, we derive equations for the Kuramoto model by taking into account the first quantum fluctuations. We also analyze its critical properties, the main result being the derivation of the value for the synchronization onset. This critical coupling increases its value as quantumness increases, as a consequence of the possibility of tunneling that quantum fluctuations provide."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Synchronization in a semiclassical Kuramoto model, what is the relationship between the critical coupling strength and the quantumness of the system, as described in the Feynman path-integral formalism?\n\n**A)** The critical coupling strength increases with the quantumness of the system, leading to a decrease in the synchronization onset.\n\n**B)** The critical coupling strength remains constant, regardless of the quantumness of the system, resulting in a fixed synchronization onset.\n\n**C)** The critical coupling strength increases with the quantumness of the system, as a consequence of the possibility of tunneling that quantum fluctuations provide, leading to an increase in the synchronization onset.\n\n**D)** The critical coupling strength decreases with the quantumness of the system, resulting in a decrease in the synchronization onset.\n\n**Correct Answer:** C) The critical coupling strength increases with the quantumness of the system, as a consequence of the possibility of tunneling that quantum fluctuations provide, leading to an increase in the synchronization onset.\n\n**Explanation:** The correct answer is C) because the Feynman path-integral formalism describes how the critical coupling strength increases with the quantumness of the system, leading to an increase in the synchronization onset. This is due to the possibility of tunneling that quantum fluctuations provide, allowing for a more efficient synchronization of the system's internal rhythms. The other options are incorrect because they do not accurately reflect the relationship between the critical coupling strength and the quantumness of the system, as described in the Feynman path-integral formalism."}, "10": {"documentation": {"title": "A Real-Time Dispatching Strategy for Shared Automated Electric Vehicles\n  with Performance Guarantees", "source": "Li Li, Theodoros Pantelidis, Joseph Y.J. Chow, Saif Eddin Jabari", "docs_id": "2006.15615", "section": ["eess.SY", "cs.SY", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Real-Time Dispatching Strategy for Shared Automated Electric Vehicles\n  with Performance Guarantees. Real-time vehicle dispatching operations in traditional car-sharing systems is an already computationally challenging scheduling problem. Electrification only exacerbates the computational difficulties as charge level constraints come into play. To overcome this complexity, we employ an online minimum drift plus penalty (MDPP) approach for SAEV systems that (i) does not require a priori knowledge of customer arrival rates to the different parts of the system (i.e. it is practical from a real-world deployment perspective), (ii) ensures the stability of customer waiting times, (iii) ensures that the deviation of dispatch costs from a desirable dispatch cost can be controlled, and (iv) has a computational time-complexity that allows for real-time implementation. Using an agent-based simulator developed for SAEV systems, we test the MDPP approach under two scenarios with real-world calibrated demand and charger distributions: 1) a low-demand scenario with long trips, and 2) a high-demand scenario with short trips. The comparisons with other algorithms under both scenarios show that the proposed online MDPP outperforms all other algorithms in terms of both reduced customer waiting times and vehicle dispatching costs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the online minimum drift plus penalty (MDPP) approach proposed in the paper \"A Real-Time Dispatching Strategy for Shared Automated Electric Vehicles\" in terms of its applicability to real-world deployment?\n\n**A)** It requires prior knowledge of customer arrival rates to different parts of the system.\n**B)** It does not require a priori knowledge of customer arrival rates to different parts of the system, ensuring practicality from a real-world deployment perspective.\n**C)** It only ensures the stability of customer waiting times, but not the control of dispatch costs.\n**D)** It has a higher computational time-complexity than other algorithms, making it unsuitable for real-time implementation.\n\n**Correct Answer:** B) It does not require a priori knowledge of customer arrival rates to different parts of the system, ensuring practicality from a real-world deployment perspective.\n\n**Explanation:** The correct answer is B) because the paper states that the MDPP approach \"does not require a priori knowledge of customer arrival rates to the different parts of the system\", which is a key advantage of the approach in terms of its practicality for real-world deployment. The other options are incorrect because they either contradict the paper's statement (A and C) or are not mentioned as advantages of the MDPP approach (D)."}, "11": {"documentation": {"title": "Remarks on the Atiyah-Hitchin metric", "source": "Ioannis Bakas", "docs_id": "hep-th/9903256", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Remarks on the Atiyah-Hitchin metric. We outline the construction of the Atiyah-Hitchin metric on the moduli space of SU(2) BPS monopoles with charge 2, first as an algebraic curve in C^3 following Donaldson and then as a solution of the Toda field equations in the continual large N limit. We adopt twistor methods to solve the underlying uniformization problem, which by the generalized Legendre transformation yield the Kahler coordinates and the Kahler potential of the metric. We also comment on the connection between twistors and the Seiberg-Witten construction of quantum moduli spaces, as they arise in three dimensional supersymmetric gauge theories, and briefly address the uniformization of algebraic curves in C^3 in the context of large N Toda theory. (Based on talks delivered in September 1998 at the 32nd International Symposium Ahrenshoop on the Theory of Elementary Particles, Buckow; the 21st Triangular Meeting on Quantum Field Theory, Crete and the TMR meeting on Quantum Aspects of Gauge Theories, Supersymmetry and Unification, Corfu; to be published in the proceedings in Fortschritte der Physik.)"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the Atiyah-Hitchin metric on the moduli space of SU(2) BPS monopoles with charge 2. Using twistor methods, find the Kahler coordinates and the Kahler potential of the metric in the continual large N limit.\n\nA) \u2202/\u2202z^1 = i(1 + z^2)\u2202/\u2202z^2, \u2202/\u2202z^2 = i(1 + z^2)\u2202/\u2202z^1, K = -ln(1 + z^2)\nB) \u2202/\u2202z^1 = i(1 + z^2)\u2202/\u2202z^2, \u2202/\u2202z^2 = -i(1 + z^2)\u2202/\u2202z^1, K = -ln(1 + z^2)\nC) \u2202/\u2202z^1 = i(1 + z^2)\u2202/\u2202z^2, \u2202/\u2202z^2 = i(1 + z^2)\u2202/\u2202z^1, K = ln(1 + z^2)\nD) \u2202/\u2202z^1 = -i(1 + z^2)\u2202/\u2202z^2, \u2202/\u2202z^2 = i(1 + z^2)\u2202/\u2202z^1, K = ln(1 + z^2)\n\nCorrect Answer: B) \u2202/\u2202z^1 = i(1 + z^2)\u2202/\u2202z^2, \u2202/\u2202z^2 = -i(1 + z^2)\u2202/\u2202z^1, K = -ln(1 + z^2)\n\nExplanation: The correct answer is based on the generalized Legendre transformation, which yields the Kahler coordinates and the Kahler potential of the metric. The correct answer involves the negative sign in the second Kahler coordinate, which is a result of the uniformization problem. The Kahler potential is also correctly given as -ln(1 + z^2), which is a common result in the context of twistor methods."}, "12": {"documentation": {"title": "Exact and Approximate Hidden Markov Chain Filters Based on Discrete\n  Observations", "source": "Nicole B\\\"auerle, Igor Gilitschenski, Uwe D. Hanebeck", "docs_id": "1411.0849", "section": ["math.PR", "cs.SY", "q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exact and Approximate Hidden Markov Chain Filters Based on Discrete\n  Observations. We consider a Hidden Markov Model (HMM) where the integrated continuous-time Markov chain can be observed at discrete time points perturbed by a Brownian motion. The aim is to derive a filter for the underlying continuous-time Markov chain. The recursion formula for the discrete-time filter is easy to derive, however involves densities which are very hard to obtain. In this paper we derive exact formulas for the necessary densities in the case the state space of the HMM consists of two elements only. This is done by relating the underlying integrated continuous-time Markov chain to the so-called asymmetric telegraph process and by using recent results on this process. In case the state space consists of more than two elements we present three different ways to approximate the densities for the filter. The first approach is based on the continuous filter problem. The second approach is to derive a PDE for the densities and solve it numerically and the third approach is a crude discrete time approximation of the Markov chain. All three approaches are compared in a numerical study."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a Hidden Markov Model (HMM) with a two-state state space, where the integrated continuous-time Markov chain is observed at discrete time points perturbed by a Brownian motion. Derive the exact recursion formula for the discrete-time filter, assuming that the transition probabilities of the underlying continuous-time Markov chain are given by the asymmetric telegraph process.\n\n**A)** The recursion formula for the discrete-time filter is given by:\n\np(t) = (1 - \u03b1) \\* p(t-1) + \u03b1 \\* \u222b[0,t-1] f(s) \\* p(t-1-s) ds\n\nwhere \u03b1 is the transition probability, f(s) is the density of the Brownian motion at time s, and p(t) is the density of the filter at time t.\n\n**B)** The recursion formula for the discrete-time filter is given by:\n\np(t) = (1 - \u03b1) \\* p(t-1) + \u03b1 \\* \u222b[0,t-1] f(s) \\* p(t-1-s) ds + \u03b1^2 \\* \u222b[0,t-1] \u222b[0,t-1-s] f(s) \\* f(u) \\* p(t-1-u) du ds\n\n**C)** The recursion formula for the discrete-time filter is given by:\n\np(t) = (1 - \u03b1) \\* p(t-1) + \u03b1 \\* \u222b[0,t-1] f(s) \\* p(t-1-s) ds + \u03b1^2 \\* \u222b[0,t-1] \u222b[0,t-1-s] f(s) \\* f(u) \\* p(t-1-u) du ds + \u03b1^3 \\* \u222b[0,t-1] \u222b[0,t-1-s] \u222b[0,t-1-u] f(s) \\* f(u) \\* f(v) \\* p(t-1-v) du dv ds\n\n**D)** The recursion formula for the discrete-time filter is given by:\n\np(t) = (1 - \u03b1) \\* p(t-1) + \u03b1 \\* \u222b[0,t-1] f(s) \\* p(t-1-s) ds + \u03b1^2 \\* \u222b[0,t-1] \u222b[0,t-1-s] f(s) \\* f(u) \\* p(t-1-u) du ds + \u03b1^3 \\* \u222b[0,t-1] \u222b[0,t-1-s] \u222b[0,t-1-u] f(s) \\* f(u) \\* f(v) \\* p(t-1-v) du dv ds + ...\n\n**Correct Answer:** **A)** The recursion formula for the discrete-time filter is given by:\n\np(t) = (1 - \u03b1) \\* p(t-1) + \u03b1 \\* \u222b[0,t-1] f(s) \\* p(t-1-s) ds\n\nExplanation: The correct answer is A) because it is the exact recursion formula for the discrete-time filter, derived using the asymmetric telegraph process. The other options are incorrect because they introduce additional terms that are not present in the exact formula. Option B) is incorrect because it introduces an additional term that is not present in the exact formula. Option C) is incorrect because it introduces an additional term that is not present in the exact formula, and the additional terms are not necessary to obtain the correct recursion formula. Option D) is incorrect because it introduces an additional term that is not present in the exact formula, and the additional terms are not necessary to obtain the correct recursion formula."}, "13": {"documentation": {"title": "Current- and field-driven magnetic antivortices", "source": "Andre Drews, Benjamin Krueger, Markus Bolte, and Guido Meier", "docs_id": "0803.1236", "section": ["cond-mat.other"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Current- and field-driven magnetic antivortices. Antivortices in ferromagnetic thin-film elements are in-plane magnetization configurations with a core pointing perpendicular to the plane. By using micromagnetic simulations, we find that magnetic antivortices gyrate on elliptical orbits similar to magnetic vortices when they are excited by alternating magnetic fields or by spin-polarized currents. The phase between high-frequency excitation and antivortex gyration is investigated. In case of excitation by spin-polarized currents the phase is determined by the polarization of the antivortex, while for excitation by magnetic fields the phase depends on the polarization as well as on the in-plane magnetization. Simultaneous excitation by a current and a magnetic field can lead to a maximum enhancement or to an entire suppression of the amplitude of the core gyration, depending on the angle between excitation and in-plane magnetization. This variation of the amplitude can be used to experimentally distinguish between spin-torque and Oersted-field driven motion of an antivortex core."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary mechanism by which the phase between high-frequency excitation and antivortex gyration is determined in ferromagnetic thin-film elements?\n\nA) The polarization of the antivortex core\nB) The angle between the excitation current and the in-plane magnetization\nC) The amplitude of the core gyration\nD) The frequency of the high-frequency excitation\n\nCorrect Answer: A) The polarization of the antivortex core\n\nExplanation: According to the documentation, the phase between high-frequency excitation and antivortex gyration is determined by the polarization of the antivortex core when excited by spin-polarized currents. This is stated in the sentence: \"In case of excitation by spin-polarized currents the phase is determined by the polarization of the antivortex, while for excitation by magnetic fields the phase depends on the polarization as well as on the in-plane magnetization.\"\n\nNote: The other options are incorrect because:\n\n* B) The angle between the excitation current and the in-plane magnetization affects the amplitude of the core gyration, but not the phase.\n* C) The amplitude of the core gyration is related to the variation of the amplitude, but not the primary mechanism for determining the phase.\n* D) The frequency of the high-frequency excitation is not mentioned as a factor in determining the phase."}, "14": {"documentation": {"title": "Dynamical topological invariants and reduced rate functions for\n  dynamical quantum phase transitions in two dimensions", "source": "Xingze Qiu, Tian-Shu Deng, Guang-Can Guo, and Wei Yi", "docs_id": "1804.09032", "section": ["cond-mat.quant-gas"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamical topological invariants and reduced rate functions for\n  dynamical quantum phase transitions in two dimensions. We show that dynamical quantum phase transitions (DQPTs) in the quench dynamics of two-dimensional topological systems can be characterized by a dynamical topological invariant defined along an appropriately chosen closed contour in momentum space. Such a dynamical topological invariant reflects the vorticity of dynamical vortices responsible for the DQPTs, and thus serves as a dynamical topological order parameter in two dimensions. We demonstrate that when the contour crosses topologically protected fixed points in the quench dynamics, an intimate connection can be established between the dynamical topological order parameter in two dimensions and those in one dimension. We further define a reduced rate function of the Loschmidt echo on the contour, which features non-analyticities at critical times and is sufficient to characterize DQPTs in two dimensions. We illustrate our results using the Haldane honeycomb model and the quantum anomalous Hall model as concrete examples, both of which have been experimentally realized using cold atoms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary role of the dynamical topological invariant in characterizing dynamical quantum phase transitions (DQPTs) in two-dimensional topological systems?\n\nA) To describe the ground state properties of the system\nB) To quantify the vorticity of dynamical vortices responsible for DQPTs\nC) To provide a reduced rate function for the Loschmidt echo on a closed contour\nD) To establish a connection between DQPTs in two dimensions and those in one dimension\n\nCorrect Answer: B) To quantify the vorticity of dynamical vortices responsible for DQPTs\n\nExplanation: The dynamical topological invariant is defined along an appropriately chosen closed contour in momentum space and reflects the vorticity of dynamical vortices responsible for DQPTs. This invariant serves as a dynamical topological order parameter in two dimensions, providing a way to quantify the vorticity of these vortices. This is the primary role of the dynamical topological invariant in characterizing DQPTs in two-dimensional topological systems.\n\nNote: The other options are incorrect because:\n\n* A) The dynamical topological invariant is not primarily used to describe ground state properties.\n* C) While the dynamical topological invariant is related to the reduced rate function for the Loschmidt echo, it is not its primary role.\n* D) The connection between DQPTs in two dimensions and those in one dimension is established through the dynamical topological invariant, but it is not its primary role."}, "15": {"documentation": {"title": "Clockwork graviton contributions to muon $g-2$", "source": "Deog Ki Hong, Du Hwan Kim and Chang Sub Shin", "docs_id": "1706.09376", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Clockwork graviton contributions to muon $g-2$. The clockwork mechanism for gravity introduces a tower of massive graviton modes, \"clockwork gravitons,\" with a very compressed mass spectrum, whose interaction strengths are much stronger than that of massless gravitons. In this work, we compute the lowest order contributions of the clockwork gravitons to the anomalous magnetic moment, $g-2$, of muon in the context of extra dimensional model with a five dimensional Planck mass, $M_5$. We find that the total contributions are rather insensitive to the detailed model parameters, and determined mostly by the value of $M_5$. In order to account for the current muon $g-2$ anomaly, $M_5$ should be around $0.2~{\\rm TeV}$, and the size of the extra dimension has to be quite large, $l_5 \\gtrsim 10^{-7}\\,$m. For $M_5\\gtrsim1~{\\rm TeV}$, the clockwork graviton contributions are too small to explain the current muon $g-2$ anomaly. We also compare the clockwork graviton contributions with other extra dimension models such as Randall-Sundrum models or large extra dimension models. We find that the leading contributions in the small curvature limit are universal, but the cutoff-independent subleading contributions vary for different background geometries and the clockwork geometry gives the smallest subleading contributions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of extra dimensional models, what is the expected value of the five-dimensional Planck mass, $M_5$, required to account for the current muon $g-2$ anomaly, and what is the corresponding size of the extra dimension, $l_5$?\n\nA) $M_5 = 0.2~{\\rm TeV}$, $l_5 \\gtrsim 10^{-7}\\,$m\nB) $M_5 = 1~{\\rm TeV}$, $l_5 \\gtrsim 10^{-8}\\,$m\nC) $M_5 = 0.1~{\\rm TeV}$, $l_5 \\gtrsim 10^{-6}\\,$m\nD) $M_5 = 0.5~{\\rm TeV}$, $l_5 \\gtrsim 10^{-9}\\,$m\n\nCorrect Answer: A) $M_5 = 0.2~{\\rm TeV}$, $l_5 \\gtrsim 10^{-7}\\,$m\n\nExplanation: According to the documentation, the clockwork graviton contributions to the muon $g-2$ anomaly are determined mostly by the value of $M_5$, and to account for the current muon $g-2$ anomaly, $M_5$ should be around $0.2~{\\rm TeV}$. Additionally, the size of the extra dimension, $l_5$, should be quite large, $l_5 \\gtrsim 10^{-7}\\,$m. Therefore, option A is the correct answer."}, "16": {"documentation": {"title": "Interaction of linear modulated waves and unsteady dispersive\n  hydrodynamic states with application to shallow water waves", "source": "T. Congy, G. A. El, M. A. Hoefer", "docs_id": "1812.06593", "section": ["nlin.PS", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interaction of linear modulated waves and unsteady dispersive\n  hydrodynamic states with application to shallow water waves. A new type of wave-mean flow interaction is identified and studied in which a small-amplitude, linear, dispersive modulated wave propagates through an evolving, nonlinear, large-scale fluid state such as an expansion (rarefaction) wave or a dispersive shock wave (undular bore). The Korteweg-de Vries (KdV) equation is considered as a prototypical example of dynamic wavepacket-mean flow interaction. Modulation equations are derived for the coupling between linear wave modulations and a nonlinear mean flow. These equations admit a particular class of solutions that describe the transmission or trapping of a linear wave packet by an unsteady hydrodynamic state. Two adiabatic invariants of motion are identified that determine the transmission, trapping conditions and show that wavepackets incident upon smooth expansion waves or compressive, rapidly oscillating dispersive shock waves exhibit so-called hydrodynamic reciprocity recently described in Phys. Rev. Lett. 120, 144101 (2018) in the context of hydrodynamic soliton tunnelling. The modulation theory results are in excellent agreement with direct numerical simulations of full KdV dynamics. The integrability of the KdV equation is not invoked so these results can be extended to other nonlinear dispersive fluid mechanic models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a shallow water wave propagating through an expanding rarefaction wave, where the mean flow is described by the Korteweg-de Vries (KdV) equation. Derive the modulation equations for the coupling between linear wave modulations and the nonlinear mean flow, and identify the adiabatic invariants of motion that determine the transmission and trapping conditions for a linear wave packet.\n\nA) The modulation equations can be derived by assuming a linear wave packet with a small amplitude and a large wavelength, and then applying the KdV equation to the mean flow. The resulting equations describe the evolution of the wave packet's amplitude and phase over time.\n\nB) The modulation equations can be derived by assuming a nonlinear wave packet with a large amplitude and a small wavelength, and then applying the KdV equation to the mean flow. The resulting equations describe the evolution of the wave packet's amplitude and phase over time, taking into account the nonlinear effects of the mean flow.\n\nC) The modulation equations can be derived by assuming a linear wave packet with a small amplitude and a large wavelength, and then applying the KdV equation to the mean flow, while also considering the effects of the nonlinear mean flow on the wave packet's amplitude and phase. The resulting equations describe the evolution of the wave packet's amplitude and phase over time, taking into account both the linear and nonlinear effects of the mean flow.\n\nD) The modulation equations can be derived by assuming a nonlinear wave packet with a large amplitude and a small wavelength, and then applying the KdV equation to the mean flow, while also considering the effects of the nonlinear mean flow on the wave packet's amplitude and phase. The resulting equations describe the evolution of the wave packet's amplitude and phase over time, taking into account both the linear and nonlinear effects of the mean flow, and also the effects of the wave packet on the mean flow.\n\nCorrect Answer: C) The modulation equations can be derived by assuming a linear wave packet with a small amplitude and a large wavelength, and then applying the KdV equation to the mean flow, while also considering the effects of the nonlinear mean flow on the wave packet's amplitude and phase. The resulting equations describe the evolution of the wave packet's amplitude and phase over time, taking into account both the linear and nonlinear effects of the mean flow.\n\nExplanation: The correct answer is C) because it accurately describes the derivation of the modulation equations for the coupling between linear wave modulations and the nonlinear mean flow. The modulation equations are derived by assuming a linear wave packet with a small amplitude and a large wavelength, and then applying the KdV equation to the mean flow, while also considering the effects of the nonlinear mean flow on the wave packet's amplitude and phase. This approach takes into account both the linear and nonlinear effects of the mean flow, and is consistent with the results presented in the documentation."}, "17": {"documentation": {"title": "Tensor-polarized quark and antiquark distribution functions in a\n  spin-one hadron", "source": "S. Kumano", "docs_id": "1005.4524", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tensor-polarized quark and antiquark distribution functions in a\n  spin-one hadron. To understand orbital-angular-momentum contributions is becoming crucial for clarifying nucleon-spin issue in the parton level. Twist-two structure functions b_1 and b_2 for spin-one hadrons could probe orbital-angular-momentum effects, which reflect a different aspect from current studies for the spin-1/2 nucleon, since they should vanish if internal constituents are in the S state. These structure functions are related to tensor structure in spin-one hadrons. Studies of such tensor structure will open a new field of high-energy spin physics. The structure functions b_1 and b_2 are described by tensor-polarized quark and antiquark distributions delta_T-q and delta_T-qbar. Using HERMES data on the b_1 structure function for the deuteron, we made an analysis of extracting the distributions delta_T-q and delta_T-qbar in a simple x-dependent functional form. Optimum distributions are proposed for the tensor-polarized valence and antiquark distribution functions from the analysis. A finite tensor polarization is obtained for antiquarks if we impose a constraint that the first moments of tensor-polarized valence-quark distributions vanish. It is interesting to investigate a physics mechanism to create a finite tensor-polarized antiquark distribution."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation for studying the tensor structure in spin-one hadrons, and how do the structure functions b_1 and b_2 relate to this motivation?\n\nA) To probe the spin-1/2 nucleon's internal structure and clarify the nucleon-spin issue at the parton level.\nB) To investigate the effects of orbital-angular-momentum contributions on the parton distribution functions in spin-one hadrons.\nC) To understand the implications of tensor-polarized quark and antiquark distributions on the b_1 and b_2 structure functions.\nD) To develop a new field of high-energy spin physics by studying the tensor structure in spin-one hadrons.\n\nCorrect Answer: B) To investigate the effects of orbital-angular-momentum contributions on the parton distribution functions in spin-one hadrons.\n\nExplanation: The correct answer is B) because the motivation for studying the tensor structure in spin-one hadrons is to understand the effects of orbital-angular-momentum contributions on the parton distribution functions, which is a crucial aspect of clarifying the nucleon-spin issue at the parton level. The structure functions b_1 and b_2 are related to tensor structure in spin-one hadrons, and studying these effects can provide new insights into the behavior of quarks and antiquarks in these systems."}, "18": {"documentation": {"title": "Application of Bernoulli Polynomials for Solving Variable-Order\n  Fractional Optimal Control-Affine Problems", "source": "Somayeh Nemati, Delfim F. M. Torres", "docs_id": "2010.02833", "section": ["math.OC", "cs.NA", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Application of Bernoulli Polynomials for Solving Variable-Order\n  Fractional Optimal Control-Affine Problems. We propose two efficient numerical approaches for solving variable-order fractional optimal control-affine problems. The variable-order fractional derivative is considered in the Caputo sense, which together with the Riemann-Liouville integral operator is used in our new techniques. An accurate operational matrix of variable-order fractional integration for Bernoulli polynomials is introduced. Our methods proceed as follows. First, a specific approximation of the differentiation order of the state function is considered, in terms of Bernoulli polynomials. Such approximation, together with the initial conditions, help us to obtain some approximations for the other existing functions in the dynamical control-affine system. Using these approximations, and the Gauss-Legendre integration formula, the problem is reduced to a system of nonlinear algebraic equations. Some error bounds are then given for the approximate optimal state and control functions, which allow us to obtain an error bound for the approximate value of the performance index. We end by solving some test problems, which demonstrate the high accuracy of our results."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a variable-order fractional optimal control-affine problem with a state function approximated using Bernoulli polynomials. The Caputo sense is used for the variable-order fractional derivative, and the Riemann-Liouville integral operator is used in conjunction with the Gauss-Legendre integration formula.\n\n**A)** The operational matrix of variable-order fractional integration for Bernoulli polynomials is a square matrix of size $n \\times n$, where $n$ is the number of state variables.\n\n**B)** The error bound for the approximate optimal state function is given by $\\epsilon_s = \\frac{\\alpha}{\\beta} \\left( \\frac{\\gamma}{\\delta} \\right)^{\\alpha}$, where $\\alpha$, $\\beta$, $\\gamma$, and $\\delta$ are positive constants.\n\n**C)** The proposed method reduces the problem to a system of nonlinear algebraic equations with a solution space of dimension $2n$, where $n$ is the number of state variables.\n\n**D)** The performance index is approximated using the Gauss-Legendre integration formula, which yields an error bound of $\\epsilon_p = \\frac{1}{2} \\left( \\frac{\\alpha}{\\beta} \\right)^{\\gamma}$, where $\\alpha$, $\\beta$, and $\\gamma$ are positive constants.\n\n**Correct Answer:** C) The proposed method reduces the problem to a system of nonlinear algebraic equations with a solution space of dimension $2n$, where $n$ is the number of state variables.\n\n**Explanation:** The proposed method uses a specific approximation of the differentiation order of the state function in terms of Bernoulli polynomials, which allows for the reduction of the problem to a system of nonlinear algebraic equations. The solution space of this system is determined by the dimension of the state variables, which is $2n$ in this case. This is because the Bernoulli polynomials are used to approximate the state function, and the Caputo sense is used for the variable-order fractional derivative, which introduces an additional dimension to the solution space."}, "19": {"documentation": {"title": "Gradient Surgery for Multi-Task Learning", "source": "Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol\n  Hausman, Chelsea Finn", "docs_id": "2001.06782", "section": ["cs.LG", "cs.CV", "cs.RO", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gradient Surgery for Multi-Task Learning. While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary challenge faced by multi-task learning approaches in deep learning and deep reinforcement learning (RL) systems, and how does the proposed gradient surgery approach address this challenge?\n\nA) The primary challenge is the lack of sufficient data, and the proposed approach is to increase the amount of data available for training. However, this approach is not model-agnostic and requires significant modifications to existing architectures.\n\nB) The primary challenge is the difficulty in avoiding gradient interference between task gradients, and the proposed approach is to project a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient.\n\nC) The primary challenge is the need for more complex and computationally expensive architectures, and the proposed approach is to develop new architectures that can handle the increased complexity.\n\nD) The primary challenge is the difficulty in achieving convergence in multi-task learning, and the proposed approach is to use a different optimization algorithm that is specifically designed for multi-task learning.\n\nCorrect Answer: B) The primary challenge is the difficulty in avoiding gradient interference between task gradients, and the proposed approach is to project a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient.\n\nExplanation: The correct answer is B) because the documentation states that the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The proposed approach of gradient surgery is specifically designed to address this challenge by projecting a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. This approach is described in the introduction of the paper as a simple yet general approach for avoiding detrimental gradient interference."}, "20": {"documentation": {"title": "Mid-infrared size survey of Young Stellar Objects: Description of Keck\n  segment-tilting experiment and basic results", "source": "J. D. Monnier (1), P. G. Tuthill (2), M. Ireland (2), R. Cohen (3), A.\n  Tannirkulam (1), and M. D. Perrin (4) ((1) University of Michigan (2)\n  University of Sydney (3) Keck Observatory (4) UCLA)", "docs_id": "0905.3495", "section": ["astro-ph.SR", "astro-ph.IM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mid-infrared size survey of Young Stellar Objects: Description of Keck\n  segment-tilting experiment and basic results. The mid-infrared properties of pre-planetary disks are sensitive to the temperature and flaring profiles of disks for the regions where planet formation is expected to occur. In order to constrain theories of planet formation, we have carried out a mid-infrared (wavelength 10.7 microns) size survey of young stellar objects using the segmented Keck telescope in a novel configuration. We introduced a customized pattern of tilts to individual mirror segments to allow efficient sparse-aperture interferometry, allowing full aperture synthesis imaging with higher calibration precision than traditional imaging. In contrast to previous surveys on smaller telescopes and with poorer calibration precision, we find most objects in our sample are partially resolved. Here we present the main observational results of our survey of 5 embedded massive protostars, 25 Herbig Ae/Be stars, 3 T Tauri stars, 1 FU Ori system, and 5 emission-line objects of uncertain classification. The observed mid-infrared sizes do not obey the size-luminosity relation found at near-infrared wavelengths and a companion paper will provide further modelling analysis of this sample. In addition, we report imaging results for a few of the most resolved objects, including complex emission around embedded massive protostars, the photoevaporating circumbinary disk around MWC 361A, and the subarcsecond binaries T Tau, FU Ori and MWC 1080."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using the segmented Keck telescope in a novel configuration for mid-infrared size survey of young stellar objects?\n\nA) Higher angular resolution\nB) Improved calibration precision through sparse-aperture interferometry\nC) Increased sensitivity to mid-infrared wavelengths\nD) Enhanced ability to resolve smaller objects\n\nCorrect Answer: B) Improved calibration precision through sparse-aperture interferometry\n\nExplanation: The question requires the test-taker to understand the benefits of using the segmented Keck telescope in a novel configuration. The correct answer, B) Improved calibration precision through sparse-aperture interferometry, highlights the key advantage of this configuration, which is to achieve higher calibration precision than traditional imaging methods. The other options are incorrect because they do not accurately describe the primary advantage of this configuration. Option A, Higher angular resolution, is not directly related to the configuration, Option C, Increased sensitivity to mid-infrared wavelengths, is not a unique benefit of this configuration, and Option D, Enhanced ability to resolve smaller objects, is not the primary advantage."}, "21": {"documentation": {"title": "E-Learning and its Socioeconomics", "source": "Avni Singh", "docs_id": "2107.05041", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "E-Learning and its Socioeconomics. While controversial, e-learning has become an essential tool for all kinds of education: especially within the kindergarten-to-twelfth sector. However, pockets of this sector lack access, mainly economically underserved students. This paper explores the options available to underserved and aptly resourced members of the kindergarten-to-twelfth educational sector: a 250-million-person market, with only 9 million students enrolled in online education. The paper also provides a brief overview of the options and challenges of making e-learning available to everyone in the kindergarten-to-twelfth educational sector. To establish whether e-learning is beneficial, it also discusses the results of a survey conducted on students and educators who have experienced e-learning, with the results showing that it is beneficial, with a general trend of teachers showing more comfort with online learning than students. The paper utilizes primary and secondary resources for this purpose, with information both from the internet, and from surveys conducted within people from the system: parents, students, and teachers."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the primary challenges in making e-learning available to economically underserved students in the kindergarten-to-twelfth educational sector, and how do the results of the survey conducted on students and educators suggest that e-learning can be beneficial for this population?\n\nA) The primary challenges are lack of access to technology and internet connectivity, and the results of the survey show that students are more comfortable with online learning than teachers.\n\nB) The primary challenges are the high cost of e-learning platforms and the need for specialized training for educators, and the results of the survey show that teachers are more comfortable with online learning than students.\n\nC) The primary challenges are the lack of qualified educators and the need for more resources to support online learning, and the results of the survey show that students are more comfortable with online learning than teachers.\n\nD) The primary challenges are the lack of access to e-learning platforms and the need for more resources to support online learning, and the results of the survey show that teachers are more comfortable with online learning than students.\n\nCorrect Answer: D) The primary challenges are the lack of access to e-learning platforms and the need for more resources to support online learning, and the results of the survey show that teachers are more comfortable with online learning than students.\n\nExplanation: The correct answer is D) because the documentation states that \"pockets of this sector lack access, mainly economically underserved students\" and that \"only 9 million students enrolled in online education\" suggests that access to e-learning platforms is a significant challenge. Additionally, the results of the survey show that teachers are more comfortable with online learning than students, which supports the idea that the primary challenges are related to access and resources."}, "22": {"documentation": {"title": "Optical evidence of surface state suppression in Bi based topological\n  insulators", "source": "Anjan A. Reijnders, Y. Tian, L. J. Sandilands, G. Pohl, I. D.\n  Kivlichan, S. Y. Frank Zhao, S. Jia, M. E. Charles, R.J. Cava, Nasser\n  Alidoust, Suyang Xu, Madhab Neupane, M. Zahid Hasan, X. Wang, S. W. Cheong,\n  K. S. Burch", "docs_id": "1404.0689", "section": ["cond-mat.mtrl-sci", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optical evidence of surface state suppression in Bi based topological\n  insulators. A key challenge in condensed matter research is the optimization of topological insulator (TI) compounds for the study and future application of their unique surface states. Truly insulating bulk states would allow the exploitation of predicted surface state properties, such as protection from backscattering, dissipationless spin-polarized currents, and the emergence of novel particles. Towards this end, major progress was recently made with the introduction of highly resistive Bi$_2$Te$_2$Se, in which surface state conductance and quantum oscillations are observed at low temperatures. Nevertheless, an unresolved and pivotal question remains: while room temperature ARPES studies reveal clear evidence of TI surface states, their observation in transport experiments is limited to low temperatures. A better understanding of this surface state suppression at elevated temperatures is of fundamental interest, and crucial for pushing the boundary of device applications towards room-temperature operation. In this work, we simultaneously measure TI bulk and surface states via temperature dependent optical spectroscopy, in conjunction with transport and ARPES measurements. We find evidence of coherent surface state transport at low temperatures, and propose that phonon mediated coupling between bulk and surface states suppresses surface conductance as temperature rises."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism proposed to suppress surface conductance in topological insulators (TIs) as the temperature increases, according to the study published in Arxiv?\n\nA) Phonon-mediated coupling between bulk and surface states\nB) Electron-electron interactions in the surface state\nC) Surface state defects and impurities\nD) Bulk state scattering and disorder\n\nCorrect Answer: A) Phonon-mediated coupling between bulk and surface states\n\nExplanation: The study proposes that phonon-mediated coupling between bulk and surface states is the primary mechanism responsible for suppressing surface conductance as the temperature increases. This is supported by the observation of coherent surface state transport at low temperatures and the finding that surface conductance decreases as the temperature rises. The correct answer requires an understanding of the study's findings and the underlying physics of TI surface states."}, "23": {"documentation": {"title": "Phase locking of spin transfer nano-oscillators using common microwave\n  sources", "source": "R. Gopal, B. Subash, V. K. Chandrasekar and M. Lakshmanan", "docs_id": "1904.04910", "section": ["cond-mat.mes-hall", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phase locking of spin transfer nano-oscillators using common microwave\n  sources. In this paper, we study typical nonlinear phenomenon of phase-locking or synchronization in spin-torque nano oscillators (STNOs). To start with the oscillators are considered as uncoupled but interlinked through either a common microwave current or a microwave field. We identify the phase locking of an array of STNOs (first for two and then for 100 oscillators) by means of injection locking which represents locking the oscillations to a common alternating spin current or a common microwave magnetic field. We characterize the locking of STNOs through both first and second harmonic lockings in an array. We find that second harmonic lockings takes lesser value of microwave current and field when compared with the first harmonic lockings. Our results also show that oscillating microwave current can induce integer harmonic locking while microwave field can induce both integer and several fractional harmonic lockings. We also extend our analysis to study locking behavior of two STNOs by introducing time delay feedback and coupling through a current injection and bring out the associated locking characteristics. Finally, we have also analyzed the stability of synchronization of identical array of STNOs with current coupling by using master stability function formalism."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of phase-locking of spin-transfer nano-oscillators (STNOs) using common microwave sources, what is the primary advantage of using second harmonic lockings compared to first harmonic lockings?\n\n**A)** Second harmonic lockings require a higher microwave current and field to achieve synchronization.\n**B)** Second harmonic lockings take a lesser value of microwave current and field to achieve synchronization.\n**C)** Second harmonic lockings are more susceptible to noise and interference.\n**D)** Second harmonic lockings are only achievable with a specific type of microwave field.\n\n**Correct Answer:** B) Second harmonic lockings take a lesser value of microwave current and field to achieve synchronization.\n\n**Explanation:** According to the paper, the authors find that second harmonic lockings take a lesser value of microwave current and field when compared with the first harmonic lockings. This is a key finding in the study of phase-locking in STNOs.\n\n**Candidate A explanation:** This option is incorrect because it states the opposite of the actual finding. Second harmonic lockings do not require a higher microwave current and field.\n\n**Candidate B explanation:** This is the correct answer. The paper explicitly states that second harmonic lockings take a lesser value of microwave current and field to achieve synchronization.\n\n**Candidate C explanation:** This option is incorrect because it is not mentioned in the paper as a characteristic of second harmonic lockings. In fact, the paper suggests that second harmonic lockings may be more desirable due to their lower requirements.\n\n**Candidate D explanation:** This option is incorrect because it is not mentioned in the paper as a requirement for second harmonic lockings. The paper does not specify a particular type of microwave field that is required for second harmonic lockings."}, "24": {"documentation": {"title": "Implications of Ocular Pathologies for Iris Recognition Reliability", "source": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "docs_id": "1809.00168", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Implications of Ocular Pathologies for Iris Recognition Reliability. This paper presents an analysis of how iris recognition is influenced by eye disease and an appropriate dataset comprising 2996 images of irises taken from 230 distinct eyes (including 184 affected by more than 20 different eye conditions). The images were collected in near infrared and visible light during routine ophthalmological examination. The experimental study carried out utilizing four independent iris recognition algorithms (MIRLIN, VeriEye, OSIRIS and IriCore) renders four valuable results. First, the enrollment process is highly sensitive to those eye conditions that obstruct the iris or cause geometrical distortions. Second, even those conditions that do not produce visible changes to the structure of the iris may increase the dissimilarity between samples of the same eyes. Third, eye conditions affecting the geometry or the tissue structure of the iris or otherwise producing obstructions significantly decrease same-eye similarity and have a lower, yet still statistically significant, influence on impostor comparison scores. Fourth, for unhealthy eyes, the most prominent effect of disease on iris recognition is to cause segmentation errors. To our knowledge this paper describes the largest database of iris images for disease-affected eyes made publicly available to researchers and offers the most comprehensive study of what we can expect when iris recognition is employed for diseased eyes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the implications of ocular pathologies on iris recognition reliability, as presented in the paper \"Implications of Ocular Pathologies for Iris Recognition Reliability\". How do eye conditions affecting the geometry or tissue structure of the iris impact the performance of iris recognition algorithms?\n\n**A)** Eye conditions that affect the geometry or tissue structure of the iris will have a negligible impact on iris recognition performance, as the algorithms are robust to such distortions.\n\n**B)** Eye conditions that affect the geometry or tissue structure of the iris will significantly decrease same-eye similarity and have a lower, yet still statistically significant, influence on impostor comparison scores.\n\n**C)** Eye conditions that affect the geometry or tissue structure of the iris will only cause segmentation errors, but will not affect the overall performance of iris recognition algorithms.\n\n**D)** Eye conditions that affect the geometry or tissue structure of the iris will have no impact on iris recognition performance, as the algorithms are able to compensate for such distortions.\n\n**Correct Answer:** B) Eye conditions that affect the geometry or tissue structure of the iris will significantly decrease same-eye similarity and have a lower, yet still statistically significant, influence on impostor comparison scores.\n\n**Explanation:** According to the paper, eye conditions that affect the geometry or tissue structure of the iris or produce obstructions significantly decrease same-eye similarity and have a lower, yet still statistically significant, influence on impostor comparison scores. This is because such conditions can cause geometrical distortions or obstructions that affect the quality of the iris images, leading to decreased performance of iris recognition algorithms."}, "25": {"documentation": {"title": "Dielectric anisotropy in the GW space-time method", "source": "Christoph Freysoldt, Philipp Eggert, Patrick Rinke, Arno Schindlmayr,\n  R. W. Godby, Matthias Scheffler", "docs_id": "cond-mat/0608215", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dielectric anisotropy in the GW space-time method. Excited-state calculations, notably for quasiparticle band structures, are nowadays routinely performed within the GW approximation for the electronic self-energy. Nevertheless, certain numerical approximations and simplifications are still employed in practice to make the computations feasible. An important aspect for periodic systems is the proper treatment of the singularity of the screened Coulomb interaction in reciprocal space, which results from the slow 1/r decay in real space. This must be done without introducing artificial interactions between the quasiparticles and their periodic images in repeated cells, which occur when integrals of the screened Coulomb interaction are discretised in reciprocal space. An adequate treatment of both aspects is crucial for a numerically stable computation of the self-energy. In this article we build on existing schemes for isotropic screening and present an extension for anisotropic systems. We also show how the contributions to the dielectric function arising from the non-local part of the pseudopotentials can be computed efficiently. These improvements are crucial for obtaining a fast convergence with respect to the number of points used for the Brillouin zone integration and prove to be essential to make GW calculations for strongly anisotropic systems, such as slabs or multilayers, efficient."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary challenge in computing the self-energy in GW approximation for periodic systems, particularly in the context of anisotropic systems, and how does the proposed extension address this issue?\n\nA) The primary challenge is the slow 1/r decay in real space, which requires a treatment of the singularity of the screened Coulomb interaction in reciprocal space. The proposed extension addresses this by introducing artificial interactions between quasiparticles and their periodic images in repeated cells.\n\nB) The primary challenge is the non-local part of the pseudopotentials, which requires a computational method to efficiently compute the contributions to the dielectric function. The proposed extension addresses this by providing a scheme for isotropic screening and an extension for anisotropic systems.\n\nC) The primary challenge is the discretization of the screened Coulomb interaction in reciprocal space, which results in artificial interactions between quasiparticles and their periodic images in repeated cells. The proposed extension addresses this by introducing a new scheme for isotropic screening that is applicable to anisotropic systems.\n\nD) The primary challenge is the convergence of the self-energy with respect to the number of points used for the Brillouin zone integration. The proposed extension addresses this by providing a method to efficiently compute the contributions to the dielectric function arising from the non-local part of the pseudopotentials.\n\nCorrect Answer: B) The primary challenge is the non-local part of the pseudopotentials, which requires a computational method to efficiently compute the contributions to the dielectric function. The proposed extension addresses this by providing a scheme for isotropic screening and an extension for anisotropic systems.\n\nExplanation: The correct answer is B) because the primary challenge in computing the self-energy in GW approximation for periodic systems is indeed the non-local part of the pseudopotentials, which requires a computational method to efficiently compute the contributions to the dielectric function. The proposed extension addresses this challenge by providing a scheme for isotropic screening and an extension for anisotropic systems, which are crucial for obtaining a fast convergence with respect to the number of points used for the Brillouin zone integration."}, "26": {"documentation": {"title": "Spatial Distributions of A3558 in the Core Region of the Shapley\n  Supercluster", "source": "F. Akimoto, K. Kondou, A. Furuzawa, Y. Tawara and K. Yamashita", "docs_id": "astro-ph/0310802", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spatial Distributions of A3558 in the Core Region of the Shapley\n  Supercluster. The core region is dominated by three rich Abell clusters and two poor clusters. Their spatial distributions of gas temperature and metal abundance provide us with information on the interactions and motions of member clusters. From eight ASCA pointing observations, temperature, abundance, and X-ray luminosity for five member clusters are similar to the other field clusters not belonging to superclusters. This result and the mean gravitational mass density of the core region indicate that the members are growing in the same way as the nonmember clusters, and the core of the supercluster is just on the way to contraction. Based on analyses of detailed spatial structures with a 4'x4' scale, the three Abell clusters show asymmetric temperature distributions. A3558 was analyzed with a 2'x2' scale, owing to the statistical advantage, and it was revealed that A3558 has clear asymmetric distributions of gas temperature and X-ray surface brightness. This is thought to be caused by cluster-cluster mergings and/or group infallings. A metal-rich region with the size of ~320 kpc was also found to the southeast, ~12' away from the cluster center of A3558. It is expected that either a remnant of a merged core has been left after a major merging or a group of galaxies has been recently infalling."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the spatial distribution of gas temperature and metal abundance in the core region of the Shapley Supercluster, specifically in the context of cluster-cluster mergings and group infallings. What can be inferred about the dynamics of the supercluster from the observed asymmetries in the temperature distributions of the three Abell clusters, and what is the likely cause of the metal-rich region found near the cluster center of A3558?\n\n**A)** The observed asymmetries in the temperature distributions of the three Abell clusters indicate that the supercluster is undergoing a period of rapid contraction, with the core region being the most affected. This contraction is likely driven by the gravitational attraction of nearby clusters and groups. The metal-rich region near the cluster center of A3558 is thought to be a remnant of a merged core.\n\n**B)** The spatial distribution of gas temperature and metal abundance in the core region of the Shapley Supercluster suggests that the supercluster is in a state of dynamic equilibrium, with cluster-cluster mergings and group infallings playing a key role in shaping its structure. The observed asymmetries in the temperature distributions of the three Abell clusters are likely caused by the unequal masses of the clusters involved in these mergers. The metal-rich region near the cluster center of A3558 is thought to be a result of recent infall of a group of galaxies.\n\n**C)** The analysis of the spatial structures of the core region of the Shapley Supercluster reveals that the three Abell clusters exhibit clear asymmetric temperature distributions, which are likely caused by the unequal masses of the clusters involved in cluster-cluster mergings. The metal-rich region found near the cluster center of A3558 is thought to be a result of a recent merger event, with the cluster's core being left with a remnant of the merged core.\n\n**D)** The observed asymmetries in the temperature distributions of the three Abell clusters indicate that the supercluster is in a state of rapid expansion, with the core region being the most affected. This expansion is likely driven by the gravitational repulsion of nearby clusters and groups. The metal-rich region near the cluster center of A3558 is thought to be a result of the infall of a group of galaxies into the supercluster.\n\n**Correct Answer:** B) The spatial distribution of gas temperature and metal abundance in the core region of the Shapley Supercluster suggests that the supercluster is in a state of dynamic equilibrium, with cluster-cluster mergings and group infallings playing a key role in shaping its structure. The observed asymmetries in the temperature distributions of the three Abell clusters are likely caused by the unequal masses of the clusters involved in these mergers. The metal-rich region near the cluster center of A3558 is thought to be a result of recent infall of a group of galaxies.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the main points of the analysis. The observed asymmetries in the temperature distributions of the three Abell clusters are likely caused by the unequal masses of the clusters involved in cluster-cluster mergings, and the metal-rich region near the cluster center of A3558 is thought to be a result of recent infall of a group of galaxies. This answer also correctly identifies the dynamic equilibrium state of the supercluster, with cluster-cluster mergings and group infallings playing a key role in shaping its structure."}, "27": {"documentation": {"title": "Non-thermal neutrinos created by shock acceleration in successful and\n  failed core-collapse supernova", "source": "Hiroki Nagakura and Kenta Hotokezaka", "docs_id": "2010.15136", "section": ["astro-ph.HE", "hep-ex", "hep-ph", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-thermal neutrinos created by shock acceleration in successful and\n  failed core-collapse supernova. We present a comprehensive study of neutrino shock acceleration in core-collapse supernova (CCSN). The leading players are heavy leptonic neutrinos, $\\nu_{\\mu}$ and $\\nu_{\\tau}$; the former and latter potentially gain the energy up to $\\sim 100$ MeV and $\\sim 200$ MeV, respectively, through the shock acceleration. Demonstrating the neutrino shock acceleration by Monte Carlo neutrino transport, we make a statement that it commonly occurs in the early post bounce phase ($\\lesssim 50$ ms after bounce) for all massive stellar collapse experiencing nuclear bounce and would reoccur in the late phase ($\\gtrsim 100$ ms) for failed CCSNe. This opens up a new possibility to detect high energy neutrinos by terrestrial detectors from Galactic CCSNe; hence, we estimate the event counts for Hyper(Super)-Kamiokande, DUNE, and JUNO. We find that the event count with the energy of $\\gtrsim 80$ MeV is a few orders of magnitude higher than that of the thermal neutrinos regardless of the detectors, and muon production may also happen in these detectors by $\\nu_{\\mu}$ with the energy of $\\gtrsim 100$ MeV. The neutrino signals provide a precious information on deciphering the inner dynamics of CCSN and placing a constraint on the physics of neutrino oscillation; indeed, the detection of the high energy neutrinos through charged current reaction channels will be a smoking gun evidence of neutrino flavor conversion."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the estimated energy range of neutrinos that can be produced through shock acceleration in core-collapse supernovae, and how does this compare to the energy range of thermal neutrinos?\n\nA) Neutrinos with energies up to 10 MeV and thermal neutrinos with energies up to 100 MeV\nB) Neutrinos with energies up to 100 MeV and thermal neutrinos with energies up to 200 MeV\nC) Neutrinos with energies up to 200 MeV and thermal neutrinos with energies up to 50 MeV\nD) Neutrinos with energies up to 50 MeV and thermal neutrinos with energies up to 100 MeV\n\n**Correct Answer:** B) Neutrinos with energies up to 100 MeV and thermal neutrinos with energies up to 200 MeV\n\n**Explanation:** According to the documentation, heavy leptonic neutrinos, specifically $\\nu_{\\mu}$ and $\\nu_{\\tau}$, can gain energies up to $\\sim 100$ MeV and $\\sim 200$ MeV, respectively, through shock acceleration. In contrast, thermal neutrinos have energies up to 50 MeV. Therefore, the correct answer is B, which states that neutrinos with energies up to 100 MeV and thermal neutrinos with energies up to 200 MeV are the estimated energy ranges."}, "28": {"documentation": {"title": "Peridynamics and Material Interfaces", "source": "Bacim Alali and Max Gunzburger", "docs_id": "1411.7250", "section": ["math.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Peridynamics and Material Interfaces. The convergence of a peridynamic model for solid mechanics inside heterogeneous media in the limit of vanishing nonlocality is analyzed. It is shown that the operator of linear peridynamics for an isotropic heterogeneous medium converges to the corresponding operator of linear elasticity when the material properties are sufficiently regular. On the other hand, when the material properties are discontinuous, i.e., when material interfaces are present, it is shown that the operator of linear peridynamics diverges, in the limit of vanishing nonlocality, at material interfaces. Nonlocal interface conditions, whose local limit implies the classical interface conditions of elasticity, are then developed and discussed. A peridynamics material interface model is introduced which generalizes the classical interface model of elasticity. The model consists of a new peridynamics operator along with nonlocal interface conditions. The new peridynamics interface model converges to the classical interface model of linear elasticity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a peridynamic model for solid mechanics inside a heterogeneous medium, where the material properties are sufficiently regular. Analyze the convergence of the linear peridynamics operator in the limit of vanishing nonlocality.\n\nA) The operator converges to the corresponding operator of linear elasticity.\nB) The operator diverges at material interfaces.\nC) The operator converges to the classical interface model of linear elasticity, but only for isotropic heterogeneous media.\nD) The operator converges to the classical interface model of linear elasticity, but only for materials with regular properties.\n\nCorrect Answer: A) The operator converges to the corresponding operator of linear elasticity.\n\nExplanation: According to the documentation, when the material properties are sufficiently regular, the operator of linear peridynamics converges to the corresponding operator of linear elasticity. This is because the material properties are regular, which implies that the peridynamic model can accurately capture the behavior of the material. The other options are incorrect because they either state that the operator diverges at material interfaces (B), which is not the case, or that the operator converges to the classical interface model of linear elasticity only for isotropic heterogeneous media (C) or materials with regular properties (D), which is not the case."}, "29": {"documentation": {"title": "Faraday and resonant waves in binary collisionally-inhomogeneous\n  Bose-Einstein condensates", "source": "J. B. Sudharsan, R. Radha, Mihaela Carina Raportaru, Alexandru I.\n  Nicolin, Antun Balaz", "docs_id": "1608.01580", "section": ["cond-mat.quant-gas", "nlin.PS", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Faraday and resonant waves in binary collisionally-inhomogeneous\n  Bose-Einstein condensates. We study Faraday and resonant waves in two-component quasi-one-dimensional (cigar-shaped) collisionally inhomogeneous Bose-Einstein condensates subject to periodic modulation of the radial confinement. We show by means of extensive numerical simulations that, as the system exhibits stronger spatially-localised binary collisions (whose scattering length is taken for convenience to be of Gaussian form), the system becomes effectively a linear one. In other words, as the scattering length approaches a delta-function, we observe that the two nonlinear configurations typical for binary cigar-shaped condensates, namely the segregated and the symbiotic one, turn into two overlapping Gaussian wave functions typical for linear systems, and that the instability onset times of the Faraday and resonant waves become longer. Moreover, our numerical simulations show that the spatial period of the excited waves (either resonant or Faraday ones) decreases as the inhomogeneity becomes stronger. Our results also demonstrate that the topology of the ground state impacts the dynamics of the ensuing density waves, and that the instability onset times of Faraday and resonant waves, for a given level of inhomogeneity in the two-body interactions, depend on whether the initial configuration is segregated or symbiotic."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the effect of increasing the scattering length of binary collisions in a collisionally-inhomogeneous Bose-Einstein condensate on the stability of Faraday and resonant waves, and how does the topology of the ground state influence the dynamics of the ensuing density waves?\n\nA) As the scattering length approaches a delta-function, the system becomes more unstable, and the topology of the ground state has no impact on the dynamics of the density waves.\nB) As the scattering length approaches a delta-function, the system becomes effectively linear, and the topology of the ground state impacts the stability of Faraday and resonant waves, with segregated initial configurations leading to longer instability onset times.\nC) As the scattering length approaches a delta-function, the system becomes more stable, and the topology of the ground state has no effect on the stability of Faraday and resonant waves.\nD) As the scattering length approaches a delta-function, the system becomes effectively linear, but the topology of the ground state has no impact on the stability of Faraday and resonant waves, and the spatial period of the excited waves decreases as the inhomogeneity becomes stronger.\n\nCorrect Answer: B) As the scattering length approaches a delta-function, the system becomes effectively linear, and the topology of the ground state impacts the stability of Faraday and resonant waves, with segregated initial configurations leading to longer instability onset times.\n\nExplanation: The correct answer is based on the statement in the documentation that \"as the scattering length approaches a delta-function, we observe that the two nonlinear configurations typical for binary cigar-shaped condensates, namely the segregated and the symbiotic one, turn into two overlapping Gaussian wave functions typical for linear systems\". This indicates that the system becomes effectively linear as the scattering length approaches a delta-function. Additionally, the documentation states that \"the topology of the ground state impacts the dynamics of the ensuing density waves, and that the instability onset times of Faraday and resonant waves, for a given level of inhomogeneity in the two-body interactions, depend on whether the initial configuration is segregated or symbiotic\". This supports the idea that the topology of the ground state influences the stability of Faraday and resonant waves, with segregated initial configurations leading to longer instability onset times."}, "30": {"documentation": {"title": "A high-precision mechanical absolute-rotation sensor", "source": "Krishna Venkateswara, Charles A. Hagedorn, Matthew D. Turner, Trevor\n  Arp, Jens H. Gundlach", "docs_id": "1401.4412", "section": ["physics.ins-det", "physics.geo-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A high-precision mechanical absolute-rotation sensor. We have developed a mechanical absolute-rotation sensor capable of resolving ground rotation angle of less than 1 nrad$/\\sqrt{\\text{Hz}}$ above $30$ mHz and 0.2 nrad$/\\sqrt{\\text{Hz}}$ above $100$ mHz about a single horizontal axis. The device consists of a meter-scale beam balance, suspended by a pair of flexures, with a resonance frequency of 10.8 mHz. The center of mass is located 3 $\\mu$m above the pivot, giving an excellent horizontal displacement rejection of better than $3\\times10^{-5}$ rad/m. The angle of the beam is read out optically using a high-sensitivity autocollimator. We have also built a tiltmeter with better than 1 nrad$/\\sqrt{\\text{Hz}}$ sensitivity above 30 mHz. Co-located measurements using the two instruments allowed us to distinguish between background rotation signal at low frequencies and intrinsic instrument noise. The rotation sensor is useful for rotational seismology and for rejecting background rotation signal from seismometers in experiments demanding high levels of seismic isolation, such as Advanced LIGO."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The mechanical absolute-rotation sensor described in the paper has a resonance frequency of 10.8 mHz. What is the significance of this resonance frequency in terms of the sensor's performance, and how does it relate to the sensor's ability to reject background rotation signal?\n\n**A)** The resonance frequency of 10.8 mHz indicates that the sensor is highly sensitive to low-frequency rotation signals, making it unsuitable for applications requiring high seismic isolation.\n\n**B)** The resonance frequency of 10.8 mHz is the same as the frequency of the background rotation signal, which the sensor is designed to reject. This is why the sensor's performance is excellent above 30 mHz.\n\n**C)** The resonance frequency of 10.8 mHz is the frequency at which the sensor's displacement rejection is optimal, allowing it to reject background rotation signal while minimizing its own noise.\n\n**D)** The resonance frequency of 10.8 mHz is the frequency at which the sensor's sensitivity is maximum, making it ideal for applications requiring high-resolution rotation measurements.\n\n**Correct Answer:** C) The resonance frequency of 10.8 mHz is the frequency at which the sensor's displacement rejection is optimal, allowing it to reject background rotation signal while minimizing its own noise.\n\n**Explanation:** The correct answer is C) because the resonance frequency of 10.8 mHz is mentioned in the paper as the frequency of the sensor's resonance, and it is stated that the center of mass is located 3 \u03bcm above the pivot, giving an excellent horizontal displacement rejection. This suggests that the resonance frequency is related to the sensor's displacement rejection, making option C the correct answer. Options A and D are incorrect because the resonance frequency is not related to the sensor's sensitivity or its ability to reject background rotation signal at low frequencies. Option B is also incorrect because the resonance frequency is not the same as the frequency of the background rotation signal."}, "31": {"documentation": {"title": "Strong microwave photon-magnon coupling in multiresonant dielectric\n  antennas (Perspective)", "source": "Ivan S. Maksymov", "docs_id": "1807.09827", "section": ["cond-mat.other", "physics.plasm-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Strong microwave photon-magnon coupling in multiresonant dielectric\n  antennas (Perspective). Achieving quantum-level control over electromagnetic waves, magnetisation dynamics, vibrations and heat is invaluable for many practical application and possible by exploiting the strong radiation-matter coupling. Most of the modern strong microwave photon-magnon coupling developments rely on the integration of metal-based microwave resonators with a magnetic material. However, it has recently been realised that all-dielectric resonators made of or containing magneto-insulating materials can operate as a standalone strongly-coupled system characterised by low dissipation losses and strong local microwave field enhancement. Here, after a brief overview of recent developments in the field, I discuss examples of such dielectric resonant systems and demonstrate their ability to operate as multiresonant antennas for light, microwaves, magnons, sound, vibrations and heat. This multiphysics behaviour opens up novel opportunities for the realisation of multiresonant coupling such as, for example, photon-magnon-phonon coupling. I also propose several novel systems in which strong photon-magnon coupling in dielectric antennas and similar structures is expected to extend the capability of existing devices or may provide an entirely new functionality. Examples of such systems include novel magnetofluidic devices, high-power microwave power generators, and hybrid devices exploiting the unique properties of electrical solitons."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the potential applications of strong microwave photon-magnon coupling in all-dielectric resonators, and how do these systems differ from traditional metal-based microwave resonators?\n\n**A)** Strong photon-magnon coupling in all-dielectric resonators can be used to enhance the sensitivity of magnetometers, while metal-based resonators are limited by their high dissipation losses. (B){candidate B}\n**B)** The unique properties of all-dielectric resonators, such as low dissipation losses and strong local microwave field enhancement, make them suitable for applications in quantum computing and quantum simulation, whereas metal-based resonators are better suited for high-power microwave applications. (C){candidate C}\n**C)** All-dielectric resonators can be used to create novel magnetofluidic devices, high-power microwave power generators, and hybrid devices exploiting the unique properties of electrical solitons, whereas metal-based resonators are limited to traditional applications such as microwave communication and radar systems. (D){candidate D}\n**D)** Strong photon-magnon coupling in all-dielectric resonators can be used to achieve quantum-level control over electromagnetic waves, magnetisation dynamics, vibrations, and heat, whereas metal-based resonators are limited by their inability to achieve such control. (Correct Answer){candidate E}\n\n**Explanation:** The correct answer is D) Strong photon-magnon coupling in all-dielectric resonators can be used to achieve quantum-level control over electromagnetic waves, magnetisation dynamics, vibrations, and heat, whereas metal-based resonators are limited by their inability to achieve such control. This is because all-dielectric resonators can operate as a standalone strongly-coupled system, characterized by low dissipation losses and strong local microwave field enhancement, which enables the achievement of quantum-level control over various physical phenomena. In contrast, metal-based resonators rely on the integration with magnetic materials and are limited by their high dissipation losses and inability to achieve such control."}, "32": {"documentation": {"title": "Adaptive Multi-grained Graph Neural Networks", "source": "Zhiqiang Zhong, Cheng-Te Li and Jun Pang", "docs_id": "2010.00238", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adaptive Multi-grained Graph Neural Networks. Graph Neural Networks (GNNs) have been increasingly deployed in a multitude of different applications that involve node-wise and graph-level tasks. The existing literature usually studies these questions independently while they are inherently correlated. We propose in this work a unified model, Adaptive Multi-grained GNN (AdamGNN), to learn node and graph level representation interactively. Compared with the existing GNN models and pooling methods, AdamGNN enhances node representation with multi-grained semantics and avoids node feature and graph structure information loss during pooling. More specifically, a differentiable pooling operator in AdamGNN is used to obtain a multi-grained structure that involves node-wise and meso/macro level semantic information. The unpooling and flyback aggregators in AdamGNN is to leverage the multi-grained semantics to enhance node representation. The updated node representation can further enrich the generated graph representation in the next iteration. Experimental results on twelve real-world graphs demonstrate the effectiveness of AdamGNN on multiple tasks, compared with several competing methods. In addition, the ablation and empirical studies confirm the effectiveness of different components in AdamGNN."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the Adaptive Multi-grained Graph Neural Networks (AdamGNN) model, and how does it differ from existing Graph Neural Networks (GNNs) and pooling methods?\n\n**A)** AdamGNN enhances node representation with multi-grained semantics and avoids node feature and graph structure information loss during pooling, but does not leverage unpooling and flyback aggregators.\n\n**B)** AdamGNN uses a differentiable pooling operator to obtain a multi-grained structure that involves node-wise and meso/macro level semantic information, but does not enhance node representation with multi-grained semantics.\n\n**C)** AdamGNN leverages unpooling and flyback aggregators to enhance node representation, but does not use a differentiable pooling operator to obtain a multi-grained structure.\n\n**D)** AdamGNN enhances node representation with multi-grained semantics and avoids node feature and graph structure information loss during pooling, and also leverages unpooling and flyback aggregators to further enrich the generated graph representation.\n\n**Correct Answer:** D) AdamGNN enhances node representation with multi-grained semantics and avoids node feature and graph structure information loss during pooling, and also leverages unpooling and flyback aggregators to further enrich the generated graph representation.\n\n**Explanation:** The correct answer is D) because the documentation states that AdamGNN enhances node representation with multi-grained semantics and avoids node feature and graph structure information loss during pooling, and also leverages unpooling and flyback aggregators to further enrich the generated graph representation. This is a key aspect of the AdamGNN model, and the other options do not accurately capture its primary contribution."}, "33": {"documentation": {"title": "Adiabatic theorems for quantum resonances", "source": "Walid K. Abou Salem and Juerg Froehlich", "docs_id": "math-ph/0607054", "section": ["math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adiabatic theorems for quantum resonances. We study the adiabatic time evolution of quantum resonances over time scales which are small compared to the lifetime of the resonances. We consider three typical examples of resonances: The first one is that of shape resonances corresponding, for example, to the state of a quantum-mechanical particle in a potential well whose shape changes over time scales small compared to the escape time of the particle from the well. Our approach to studying the adiabatic evolution of shape resonances is based on a precise form of the time-energy uncertainty relation and the usual adiabatic theorem in quantum mechanics. The second example concerns resonances that appear as isolated complex eigenvalues of spectrally deformed Hamiltonians, such as those encountered in the N-body Stark effect. Our approach to study such resonances is based on the Balslev-Combes theory of dilatation-analytic Hamiltonians and an adiabatic theorem for nonnormal generators of time evolution. Our third example concerns resonances arising from eigenvalues embedded in the continuous spectrum when a perturbation is turned on, such as those encountered when a small system is coupled to an infinitely extended, dispersive medium. Our approach to this class of examples is based on an extension of adiabatic theorems without a spectral gap condition. We finally comment on resonance crossings, which can be studied using the last approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a quantum system with a time-dependent potential that changes over time scales comparable to the lifetime of the resonances. What is the underlying assumption that allows for the application of the adiabatic theorem to study the adiabatic time evolution of shape resonances in this system?\n\nA) The system is in a stationary state, and the potential can be approximated as constant.\nB) The time-energy uncertainty relation is precisely known, and the potential can be approximated as constant.\nC) The system is in a quasi-stationary state, and the potential can be approximated as constant over the time scales of interest.\nD) The system is in a stationary state, and the potential can be approximated as constant over the time scales of interest.\n\nCorrect Answer: C) The system is in a quasi-stationary state, and the potential can be approximated as constant over the time scales of interest.\n\nExplanation: The correct answer is C) because the adiabatic theorem is typically applied to systems that are in a quasi-stationary state, where the potential can be approximated as constant over the time scales of interest. This assumption allows for the application of the adiabatic theorem to study the adiabatic time evolution of shape resonances. The other options are incorrect because they do not accurately reflect the underlying assumption required for the application of the adiabatic theorem in this context."}, "34": {"documentation": {"title": "Lessons from reinforcement learning for biological representations of\n  space", "source": "Alex Muryy, N. Siddharth, Nantas Nardelli, Philip H. S. Torr, Andrew\n  Glennerster", "docs_id": "1912.06615", "section": ["q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lessons from reinforcement learning for biological representations of\n  space. Neuroscientists postulate 3D representations in the brain in a variety of different coordinate frames (e.g. 'head-centred', 'hand-centred' and 'world-based'). Recent advances in reinforcement learning demonstrate a quite different approach that may provide a more promising model for biological representations underlying spatial perception and navigation. In this paper, we focus on reinforcement learning methods that reward an agent for arriving at a target image without any attempt to build up a 3D 'map'. We test the ability of this type of representation to support geometrically consistent spatial tasks such as interpolating between learned locations using decoding of feature vectors. We introduce a hand-crafted representation that has, by design, a high degree of geometric consistency and demonstrate that, in this case, information about the persistence of features as the camera translates (e.g. distant features persist) can improve performance on the geometric tasks. These examples avoid Cartesian (in this case, 2D) representations of space. Non-Cartesian, learned representations provide an important stimulus in neuroscience to the search for alternatives to a 'cognitive map'."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of using reinforcement learning methods that reward an agent for arriving at a target image without building up a 3D 'map', as opposed to traditional approaches that rely on geometrically consistent spatial representations?\n\nA) Improved handling of occlusions in 3D environments\nB) Enhanced ability to learn non-Cartesian representations of space\nC) Increased computational efficiency in spatial navigation tasks\nD) Better suited for tasks requiring precise localization in 2D spaces\n\nCorrect Answer: B) Enhanced ability to learn non-Cartesian representations of space\n\nExplanation: The correct answer, B, is supported by the paper's focus on reinforcement learning methods that reward an agent for arriving at a target image without building up a 3D 'map'. This approach is distinct from traditional methods that rely on geometrically consistent spatial representations, such as Cartesian (2D) representations. The paper highlights the potential benefits of non-Cartesian, learned representations in neuroscience, particularly in the context of spatial perception and navigation.\n\nThe incorrect options can be eliminated through the following reasoning:\n\nA) Improved handling of occlusions in 3D environments is not the primary advantage of this approach, as the paper does not specifically address occlusions.\n\nC) Increased computational efficiency in spatial navigation tasks is not a direct consequence of using reinforcement learning methods that reward an agent for arriving at a target image without building up a 3D 'map'.\n\nD) Better suited for tasks requiring precise localization in 2D spaces is incorrect, as the paper focuses on non-Cartesian representations of space, which are not limited to 2D environments."}, "35": {"documentation": {"title": "Green Pea Galaxies Reveal Secrets of Ly$\\alpha$ Escape", "source": "Huan Yang, Sangeeta Malhotra, Max Gronke, James E. Rhoads, Mark\n  Dijkstra, Anne Jaskot, Zhenya Zheng, Junxian Wang", "docs_id": "1506.02885", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Green Pea Galaxies Reveal Secrets of Ly$\\alpha$ Escape. We analyze archival Ly$\\alpha$ spectra of 12 \"Green Pea\" galaxies observed with the Hubble Space Telescope, model their Ly$\\alpha$ profiles with radiative transfer models, and explore the dependence of Ly$\\alpha$ escape fraction on various properties. Green Pea galaxies are nearby compact starburst galaxies with [OIII]$\\lambda$5007 equivalent widths of hundreds of \\AA. All 12 Green Pea galaxies in our sample show Ly$\\alpha$ lines in emission, with a Ly$\\alpha$ equivalent width distribution similar to high redshift Ly$\\alpha$ emitters. Combining the optical and UV spectra of Green Pea galaxies, we estimate their Ly$\\alpha$ escape fractions and find correlations between Ly$\\alpha$ escape fraction and kinematic features of Ly$\\alpha$ profiles. The escape fraction of Ly$\\alpha$ in these galaxies ranges from 1.4% to 67%. We also find that the Ly$\\alpha$ escape fraction depends strongly on metallicity and moderately on dust extinction. We compare their high-quality Ly$\\alpha$ profiles with single HI shell radiative transfer models and find that the Ly$\\alpha$ escape fraction anti-correlates with the derived HI column densities. Single shell models fit most Ly$\\alpha$ profiles well, but not the ones with highest escape fractions of Ly$\\alpha$. Our results suggest that low HI column density and low metallicity are essential for Ly$\\alpha$ escape, and make a galaxy a Ly$\\alpha$ emitter."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary factor that determines the Ly\u03b1 escape fraction in Green Pea galaxies, according to the study?\n\nA) Dust extinction\nB) Metallicity\nC) Kinematic features of Ly\u03b1 profiles\nD) HI column density\n\nCorrect Answer: B) Metallicity\n\nExplanation: The study finds that the Ly\u03b1 escape fraction depends strongly on metallicity, with lower metallicity galaxies having higher Ly\u03b1 escape fractions. This suggests that metallicity is a key factor in determining the ability of Ly\u03b1 light to escape from the galaxy.\n\nCandidate A is incorrect because while dust extinction is mentioned as a factor that affects the Ly\u03b1 escape fraction, it is not the primary factor. Candidate C is also incorrect because kinematic features of Ly\u03b1 profiles are correlated with Ly\u03b1 escape fraction, but are not the primary factor. Candidate D is incorrect because HI column density is actually anti-correlated with Ly\u03b1 escape fraction, meaning that lower HI column densities are associated with higher Ly\u03b1 escape fractions."}, "36": {"documentation": {"title": "Auxiliary Field Loop Expansion for the Effective Action for Stochastic\n  Partial Differential Equations I", "source": "Fred Cooper", "docs_id": "1406.2737", "section": ["cond-mat.stat-mech", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Auxiliary Field Loop Expansion for the Effective Action for Stochastic\n  Partial Differential Equations I. Using a path integral formulation for correlation functions of stochastic partial differential equations based on the Onsager-Machlup approach, we show how, by introducing a composite auxiliary field one can generate an auxiliary field loop expansion for the correlation functions which is similar to the one used in the $1/N$ expansion for an $O(N)$ scalar quantum field theory. We apply this formalism to the Kardar Parisi Zhang (KPZ) equation, and introduce the composite field $\\sigma = \\frac{\\lambda}{2} \\nabla \\phi \\cdot \\nabla \\phi$ by inserting a representation of the unit operator into the path integral which enforces this constraint. In leading order we obtain a self-consistent mean field approximation for the effective action similar to that used for the Bardeen-Cooper-Schrieffer (BCS) and Bose-Einstein Condensate (BEC) theories of dilute Fermi and Bose gases. This approximation, though related to a self-consistent Gaussian approximation, preserves all symmetries and broken symmetries. We derive the leading order in the auxiliary field (LOAF) effective potential and compare our results to the one loop in the fluctuation strength ${\\cal A}$ approximation. We find, contrary to what is found in the one loop and self-consistent Gaussian approximation schemes that in the LOAF approximation there is no fluctuation induced symmetry breaking as a function of the coupling constant in any dimension $d$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Auxiliary Field Loop Expansion (LOAF) formalism for the effective action of stochastic partial differential equations, what is the relationship between the auxiliary field $\\sigma$ and the coupling constant $\\lambda$ in the KPZ equation, and how does this relationship affect the symmetry breaking behavior of the system?\n\n**A)** The auxiliary field $\\sigma$ is proportional to the square of the coupling constant $\\lambda$, and this relationship leads to symmetry breaking in all dimensions $d$.\n\n**B)** The auxiliary field $\\sigma$ is proportional to the cube of the coupling constant $\\lambda$, and this relationship preserves all symmetries and broken symmetries in the system.\n\n**C)** The auxiliary field $\\sigma$ is proportional to the square root of the coupling constant $\\lambda$, and this relationship leads to symmetry breaking in dimensions $d \\geq 3$.\n\n**D)** The auxiliary field $\\sigma$ is proportional to the inverse of the coupling constant $\\lambda$, and this relationship preserves all symmetries and broken symmetries in the system.\n\n**Correct Answer:** B) The auxiliary field $\\sigma$ is proportional to the square of the coupling constant $\\lambda$, and this relationship preserves all symmetries and broken symmetries in the system.\n\n**Explanation:** The correct answer is B) because the documentation states that the composite field $\\sigma = \\frac{\\lambda}{2} \\nabla \\phi \\cdot \\nabla \\phi$ is introduced to enforce a constraint, and in leading order, the LOAF approximation yields a self-consistent mean field approximation that preserves all symmetries and broken symmetries. This is consistent with the relationship between $\\sigma$ and $\\lambda$ stated in option B. Options A, C, and D are incorrect because they either imply symmetry breaking or incorrect relationships between $\\sigma$ and $\\lambda$."}, "37": {"documentation": {"title": "On the transferability of time-lagged independent components between\n  similar molecular dynamics systems", "source": "Alexander S. Moffett and Diwakar Shukla", "docs_id": "1710.00443", "section": ["q-bio.BM", "physics.bio-ph", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the transferability of time-lagged independent components between\n  similar molecular dynamics systems. Dimensionality reduction techniques have found great success in a wide range of fields requiring analysis of high-dimensional datasets. Time-lagged independent components analysis (TICA), which finds independent components (TICs) with maximal autocorrelation, is often applied to atomistic biomolecular simulations, where the full molecular configuration can be projected onto only a few TICs describing the slowest modes of motion. Recently, Sultan and Pande have proposed the use of TICs as collective variables for enhanced sampling. However, it is unclear what the best strategy for estimating the TICs of a system is a priori. In order to evaluate the utility of TICs calculated on one system to describe the slow dynamics of similar systems, we develop a methodology for measuring the transferability of TICs and apply it to a wide range of systems. We find that transferred TICs can approximate the slowest dynamics of some systems surprisingly well, while failing to transfer between other sets of systems, highlighting the inherent difficulties of predicting TIC transferability. Additionally, we use two dimensional Brownian dynamics simulations on similar potential surfaces to gain insight into the relationship between TIC transferability and potential surface changes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary limitation of using time-lagged independent components (TICs) as collective variables for enhanced sampling in molecular dynamics simulations, as highlighted in the study on the transferability of TICs between similar systems?\n\nA) TICs are sensitive to changes in the potential surface of the system.\nB) TICs are difficult to estimate a priori for complex systems.\nC) TICs fail to transfer between systems with significantly different structural properties.\nD) TICs are limited to describing only the slowest modes of motion.\n\nCorrect Answer: C) TICs fail to transfer between systems with significantly different structural properties.\n\nExplanation: The study highlights the challenges of predicting TIC transferability between similar systems, and notes that transferred TICs can approximate the slowest dynamics of some systems, but fail to transfer between other sets of systems. This suggests that the primary limitation of using TICs as collective variables is their inability to transfer between systems with significantly different structural properties."}, "38": {"documentation": {"title": "Water at Positive and Negative Pressures", "source": "H. E. Stanley, M. C. Barbosa, S. Mossa, P. A. Netz, F. Sciortino, F.\n  W. Starr, and M. Yamada", "docs_id": "cond-mat/0206542", "section": ["cond-mat.soft", "cond-mat.mtrl-sci", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Water at Positive and Negative Pressures. We review recent results of molecular dynamics simulations of two models of liquid water, the extended simple point charge (SPC/E) and the Mahoney-Jorgensen transferable intermolecular potential with five points (TIP5P), which is closer to real water than previously-proposed classical pairwise additive potentials. Simulations of the TIP5P model for a wide range of deeply supercooled states, including both positive and negative pressures, reveal (i) the existence of a non-monotonic ``nose-shaped'' temperature of maximum density (TMD) line and a non-reentrant spinodal, (ii) the presence of a low temperature phase transition. The TMD that changes slope from negative to positive as P decreases and, notably, the point of crossover between the two behaviors is located at ambient pressure (temperature approx 4 C, and density approx 1 g/cm^3). Simulations on the dynamics of the SPC/E model reveal (iii) the dynamics at negative pressure shows a minimum in the diffusion constant $D$ when the density is decreased at constant temperature, complementary to the known maximum of D at higher pressures, and (iv) the loci of minima of D relative to the spinodal shows that they are inside the thermodynamically metastable regions of the phase-diagram. These dynamical results reflect the initial enhancement and subsequent breakdown of the tetrahedral structure and of the hydrogen bond network as the density decreases."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the phase behavior of liquid water under positive and negative pressures, as revealed by molecular dynamics simulations of the SPC/E and TIP5P models. What is the significance of the crossover point between the non-monotonic \"nose-shaped\" temperature of maximum density (TMD) line and the non-reentrant spinodal, and how does it relate to the thermodynamic stability of the system?\n\n**A)** The crossover point marks the transition from a supercooled to a superheated state, where the TMD line becomes monotonic and the spinodal becomes reentrant.\n\n**B)** The crossover point is located at ambient pressure (temperature \u2248 4\u00b0C, and density \u2248 1 g/cm^3), and it represents the point where the TMD line changes slope from negative to positive, indicating a change in the thermodynamic stability of the system.\n\n**C)** The crossover point is a result of the breakdown of the tetrahedral structure and the hydrogen bond network at negative pressures, leading to a minimum in the diffusion constant D.\n\n**D)** The crossover point is a consequence of the non-reentrant spinodal, which separates the thermodynamically metastable regions of the phase diagram, and it marks the transition from a metastable to a stable state.\n\n**Correct Answer:** B) The crossover point is located at ambient pressure (temperature \u2248 4\u00b0C, and density \u2248 1 g/cm^3), and it represents the point where the TMD line changes slope from negative to positive, indicating a change in the thermodynamic stability of the system.\n\n**Explanation:** The correct answer is B) because the crossover point is indeed located at ambient pressure, and it marks the transition from a non-monotonic TMD line to a monotonic one, indicating a change in the thermodynamic stability of the system. This is a key finding in the study, as it highlights the complex phase behavior of liquid water under positive and negative pressures. The other options are incorrect because they either misinterpret the significance of the crossover point (A and D) or focus on a different aspect of the phase behavior (C)."}, "39": {"documentation": {"title": "Prediction and typicality in multiverse cosmology", "source": "Feraz Azhar", "docs_id": "1506.08101", "section": ["astro-ph.CO", "gr-qc", "hep-th", "physics.hist-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prediction and typicality in multiverse cosmology. In the absence of a fundamental theory that precisely predicts values for observable parameters, anthropic reasoning attempts to constrain probability distributions over those parameters in order to facilitate the extraction of testable predictions. The utility of this approach has been vigorously debated of late, particularly in light of theories that claim we live in a multiverse, where parameters may take differing values in regions lying outside our observable horizon. Within this cosmological framework, we investigate the efficacy of top-down anthropic reasoning based on the weak anthropic principle. We argue contrary to recent claims that it is not clear one can either dispense with notions of typicality altogether or presume typicality, in comparing resulting probability distributions with observations. We show in a concrete, top-down setting related to dark matter, that assumptions about typicality can dramatically affect predictions, thereby providing a guide to how errors in reasoning regarding typicality translate to errors in the assessment of predictive power. We conjecture that this dependence on typicality is an integral feature of anthropic reasoning in broader cosmological contexts, and argue in favour of the explicit inclusion of measures of typicality in schemes invoking anthropic reasoning, with a view to extracting predictions from multiverse scenarios."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of multiverse cosmology, what is the primary concern with relying solely on anthropic reasoning to constrain probability distributions over observable parameters, and how does this relate to the concept of typicality?\n\nA) The anthropic principle is too vague to be of any practical use in making predictions.\nB) The multiverse hypothesis implies that parameters may take on a wide range of values, making it impossible to determine typicality.\nC) The weak anthropic principle assumes that the universe is typical of all possible universes, but this assumption is not justified.\nD) The concept of typicality is essential to anthropic reasoning, as it allows for the comparison of probability distributions with observations.\n\nCorrect Answer: D) The concept of typicality is essential to anthropic reasoning, as it allows for the comparison of probability distributions with observations.\n\nExplanation: The correct answer is D) because the documentation argues that assumptions about typicality can dramatically affect predictions, and that the concept of typicality is integral to anthropic reasoning in broader cosmological contexts. The other options are incorrect because they do not accurately reflect the main concern with anthropic reasoning in multiverse cosmology. Option A is incorrect because the anthropic principle is not too vague, but rather, the concept of typicality is what is problematic. Option B is incorrect because the multiverse hypothesis does not imply that parameters may take on a wide range of values, but rather that they may take on different values in regions outside our observable horizon. Option C is incorrect because the weak anthropic principle does not assume that the universe is typical of all possible universes, but rather that we live in a typical universe within a multiverse."}, "40": {"documentation": {"title": "A-infinity Algebras Derived from Associative Algebras with a\n  Non-Derivation Differential", "source": "Kaj B\\\"orjeson", "docs_id": "1304.6231", "section": ["math.QA", "math.KT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A-infinity Algebras Derived from Associative Algebras with a\n  Non-Derivation Differential. Given an associative graded algebra equipped with a degree +1 differential we define an A-infinity structure that measures the failure of the differential to be a derivation. This can be seen as a non-commutative analog of generalized BV-algebras. In that spirit we introduce a notion of associative order for the differential and prove that it satisfies properties similar to the commutative case. In particular when it has associative order 2 the new product is a strictly associative product of degree +1 and there is a compatibility between the products, similar to ordinary BV-algebras. We consider several examples of structures obtained in this way. In particular we obtain an A-infinity structure on the bar complex of an A-infinity algebra that is strictly associative if the original algebra is strictly associative. We also introduce strictly associative degree +1 products for any degree +1 action on a graded algebra. Moreover, an A-infinity structure is constructed on the Hochschild cocomplex of an associative algebra with a non-degenerate inner product by using Connes' B-operator."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider an associative graded algebra A equipped with a degree +1 differential d. Suppose that the differential d satisfies the following properties:\n\n* d^2 = 0\n* d(a) = 0 for all a in A\n* d(ab) = ad(b) + (-1)^deg(a)bd(a) for all a, b in A\n\nWhat can be said about the A-infinity structure on A induced by d?\n\nA) The A-infinity structure is strictly associative if and only if the original algebra A is strictly associative.\nB) The A-infinity structure is strictly associative if and only if the differential d has associative order 2.\nC) The A-infinity structure is strictly associative if and only if the algebra A is a BV-algebra.\nD) The A-infinity structure is strictly associative if and only if the algebra A is a Hochschild algebra.\n\nCorrect Answer: B) The A-infinity structure is strictly associative if and only if the differential d has associative order 2.\n\nExplanation: The documentation states that when the differential d has associative order 2, the new product is a strictly associative product of degree +1, and there is a compatibility between the products, similar to ordinary BV-algebras. This is the key property that distinguishes the A-infinity structure from other non-commutative analogs of generalized BV-algebras."}, "41": {"documentation": {"title": "Wavelets on Graphs via Spectral Graph Theory", "source": "David K Hammond, Pierre Vandergheynst, and R\\'emi Gribonval", "docs_id": "0912.3848", "section": ["math.FA", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Wavelets on Graphs via Spectral Graph Theory. We propose a novel method for constructing wavelet transforms of functions defined on the vertices of an arbitrary finite weighted graph. Our approach is based on defining scaling using the the graph analogue of the Fourier domain, namely the spectral decomposition of the discrete graph Laplacian $\\L$. Given a wavelet generating kernel $g$ and a scale parameter $t$, we define the scaled wavelet operator $T_g^t = g(t\\L)$. The spectral graph wavelets are then formed by localizing this operator by applying it to an indicator function. Subject to an admissibility condition on $g$, this procedure defines an invertible transform. We explore the localization properties of the wavelets in the limit of fine scales. Additionally, we present a fast Chebyshev polynomial approximation algorithm for computing the transform that avoids the need for diagonalizing $\\L$. We highlight potential applications of the transform through examples of wavelets on graphs corresponding to a variety of different problem domains."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using the spectral graph wavelet transform, as proposed in the paper \"Wavelets on Graphs via Spectral Graph Theory\", over traditional wavelet transforms on functions defined on the vertices of a graph?\n\n**A)** The spectral graph wavelet transform is more computationally efficient for large graphs.\n**B)** The spectral graph wavelet transform is more robust to noise and outliers in the graph data.\n**C)** The spectral graph wavelet transform is based on the spectral decomposition of the discrete graph Laplacian, which provides a more accurate representation of the graph structure.\n**D)** The spectral graph wavelet transform is more suitable for non-stationary signals and can capture non-linear relationships between nodes.\n\n**Correct Answer:** C) The spectral graph wavelet transform is based on the spectral decomposition of the discrete graph Laplacian, which provides a more accurate representation of the graph structure.\n\n**Explanation:** The correct answer is C) because the spectral graph wavelet transform is based on the spectral decomposition of the discrete graph Laplacian, which provides a more accurate representation of the graph structure. This is a key advantage of the transform, as it allows for a more nuanced and accurate analysis of the graph data. The other options are incorrect because while they may be related to the transform, they are not the primary advantage of using the spectral graph wavelet transform.\n\n**Additional explanations:**\n\n* Option A is incorrect because while the spectral graph wavelet transform may be more computationally efficient for large graphs, this is not its primary advantage.\n* Option B is incorrect because the spectral graph wavelet transform is not specifically designed to be robust to noise and outliers in the graph data.\n* Option D is incorrect because the spectral graph wavelet transform is not specifically designed to capture non-linear relationships between nodes, and is not more suitable for non-stationary signals than traditional wavelet transforms."}, "42": {"documentation": {"title": "BRST Cohomology and Physical Space of the GZ Model", "source": "Martin Schaden and Daniel Zwanziger", "docs_id": "1412.4823", "section": ["hep-ph", "hep-lat", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "BRST Cohomology and Physical Space of the GZ Model. We address the issue of BRST symmetry breaking in the GZ model, a local, renormalizable, non-perturbative approach to QCD. Explicit calculation of several examples reveals that BRST symmetry breaking apparently afflicts the unphysical sector of the theory, but may be unbroken where needed, in cases of physical interest. Specifically, the BRST-exact part of the conserved energy-momentum tensor and the BRST-exact term in the Kugo-Ojima confinement condition both have vanishing expectation value. We analyze the origin of the breaking of BRST symmetry in the GZ model, and obtain a useful sufficient condition that determines which operators preserve BRST. Observables of the GZ theory are required to be invariant under a certain group of symmetries that includes not only BRST but also others. The definition of observables is thereby sharpened, and excludes all operators known to us that break BRST invariance. We take as a hypothesis that BRST symmetry is unbroken by this class of observables. If the hypothesis holds, BRST breaking is relegated to the unphysical sector of the GZ theory, and its physical states are obtained by the usual cohomological BRST construction. The fact that the horizon condition and the Kugo-Ojima confinement criterion coincide assures that color is confined in the GZ theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the sufficient condition that determines which operators preserve BRST symmetry in the GZ model, and what are the implications of this condition on the physical states of the theory?\n\nA) The sufficient condition is that the operators must be invariant under the BRST transformation, and the physical states are obtained by the usual cohomological BRST construction. However, this condition does not exclude operators that break BRST invariance in the unphysical sector.\n\nB) The sufficient condition is that the operators must be invariant under the BRST transformation, and the physical states are obtained by the usual cohomological BRST construction. This condition excludes all operators known to us that break BRST invariance, and the hypothesis that BRST symmetry is unbroken by the class of observables is assumed to hold.\n\nC) The sufficient condition is that the operators must be invariant under the BRST transformation, but the physical states are obtained by the usual cohomological BRST construction only if the horizon condition and the Kugo-Ojima confinement criterion coincide.\n\nD) The sufficient condition is that the operators must be invariant under the BRST transformation, but the physical states are obtained by the usual cohomological BRST construction only if the hypothesis that BRST symmetry is unbroken by the class of observables is assumed to hold.\n\nCorrect Answer: B) The sufficient condition is that the operators must be invariant under the BRST transformation, and the physical states are obtained by the usual cohomological BRST construction. This condition excludes all operators known to us that break BRST invariance, and the hypothesis that BRST symmetry is unbroken by the class of observables is assumed to hold.\n\nExplanation: The correct answer is B) because it accurately summarizes the sufficient condition for preserving BRST symmetry in the GZ model, as stated in the documentation. The condition is that the operators must be invariant under the BRST transformation, and the physical states are obtained by the usual cohomological BRST construction. This condition excludes all operators known to us that break BRST invariance, and the hypothesis that BRST symmetry is unbroken by the class of observables is assumed to hold. The other options are incorrect because they either omit or misstate the sufficient condition, or introduce additional assumptions that are not supported by the documentation."}, "43": {"documentation": {"title": "New Regions in the NMSSM with a 125 GeV Higgs", "source": "Marcin Badziak, Marek Olechowski, Stefan Pokorski", "docs_id": "1304.5437", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "New Regions in the NMSSM with a 125 GeV Higgs. It is pointed out that mixing effects in the CP-even scalar sector of the NMSSM can give 6-8 GeV correction to the SM-like Higgs mass in moderate or large $\\tan\\beta$ regions with a small value of the singlet-higgs-higgs superfields coupling $\\lambda\\sim\\mathcal{O}(0.1)$. This effect comes mainly from the mixing of the SM-like Higgs with lighter singlet. In the same parameter range, the mixing of the heavy doublet Higgs with the singlet may strongly modify the couplings of the singlet-like and the 125 GeV scalars. Firstly, the LEP bounds on a light singlet can be evaded for a large range of its masses. Secondly, the decay rates of both scalars can show a variety of interesting patterns, depending on the lightest scalar mass. In particular, a striking signature of this mechanism can be a light scalar with strongly suppressed (enhanced) branching ratios to $b\\bar{b}$ ($gg$, $c\\bar{c}$, $\\gamma\\gamma$) as compared to the SM Higgs with the same mass. The $\\gamma\\gamma$ decay channel is particularly promising for the search of such a scalar at the LHC. The 125 GeV scalar can, thus, be accommodated with substantially smaller than in the MSSM radiative corrections from the stop loops (and consequently, with lighter stops) also for moderate or large $\\tan\\beta$, with the mixing effects replacing the standard NMSSM mechanism of increasing the tree level Higgs mass in the low $\\tan\\beta$ and large $\\lambda$ regime, and with clear experimental signatures of such a mechanism."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the NMSSM, what is the expected effect on the 125 GeV Higgs mass due to mixing effects in the CP-even scalar sector, and how does this compare to the standard NMSSM mechanism of increasing the tree-level Higgs mass?\n\nA) The mixing effects will lead to a 6-8 GeV correction to the SM-like Higgs mass, but will not modify the couplings of the singlet-like and heavy doublet Higgs scalars.\n\nB) The mixing effects will lead to a 6-8 GeV correction to the SM-like Higgs mass, and will strongly modify the couplings of the singlet-like and heavy doublet Higgs scalars, potentially evading LEP bounds on a light singlet.\n\nC) The mixing effects will not lead to a 6-8 GeV correction to the SM-like Higgs mass, but will instead lead to a reduction in the radiative corrections from the stop loops, allowing for lighter stops.\n\nD) The mixing effects will lead to a 6-8 GeV correction to the SM-like Higgs mass, but will only be significant in the low tan\u03b2 and large \u03bb regime, and will not modify the couplings of the singlet-like and heavy doublet Higgs scalars.\n\nCorrect Answer: B) The mixing effects will lead to a 6-8 GeV correction to the SM-like Higgs mass, and will strongly modify the couplings of the singlet-like and heavy doublet Higgs scalars, potentially evading LEP bounds on a light singlet.\n\nExplanation: The correct answer is B) because the documentation states that the mixing effects can lead to a 6-8 GeV correction to the SM-like Higgs mass, and will strongly modify the couplings of the singlet-like and heavy doublet Higgs scalars, potentially evading LEP bounds on a light singlet. This is in contrast to the other options, which either do not accurately describe the expected effect of the mixing effects or are incorrect in their description of the relationship between the mixing effects and the couplings of the singlet-like and heavy doublet Higgs scalars."}, "44": {"documentation": {"title": "A canonical transformation to eliminate resonant perturbations I", "source": "Barnab\\'as Deme, Bence Kocsis", "docs_id": "2103.00013", "section": ["nlin.CD", "astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A canonical transformation to eliminate resonant perturbations I. We study dynamical systems which admit action-angle variables at leading order which are subject to nearly resonant perturbations. If the frequencies characterizing the unperturbed system are not in resonance, the long-term dynamical evolution may be integrated by orbit-averaging over the high-frequency angles, thereby evolving the orbit-averaged effect of the perturbations. It is well known that such integrators may be constructed via a canonical transformation, which eliminates the high frequency variables from the orbit-averaged quantities. An example of this algorithm in celestial mechanics is the von Zeipel transformation. However if the perturbations are inside or close to a resonance, i.e. the frequencies of the unperturbed system are commensurate, these canonical transformations are subject to divergences. We introduce a canonical transformation which eliminates the high frequency phase variables in the Hamiltonian without encountering divergences. This leads to a well-behaved symplectic integrator. We demonstrate the algorithm through two examples: a resonantly perturbed harmonic oscillator and the gravitational three-body problem in mean motion resonance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a dynamical system with action-angle variables that are subject to nearly resonant perturbations. The frequencies of the unperturbed system are not in resonance, and the long-term dynamical evolution can be integrated by orbit-averaging over the high-frequency angles. However, if the perturbations are inside or close to a resonance, the canonical transformation used to eliminate the high frequency variables may encounter divergences.\n\n**A)** The von Zeipel transformation is an example of a canonical transformation that eliminates high frequency variables without encountering divergences, even in the presence of resonant perturbations.\n\n**B)** The algorithm described in the paper is a generalization of the von Zeipel transformation, which can be applied to any dynamical system with action-angle variables and nearly resonant perturbations.\n\n**C)** The canonical transformation introduced in the paper is a symplectic integrator, meaning that it preserves the symplectic structure of the phase space and does not introduce any artificial dissipation or damping.\n\n**D)** The algorithm described in the paper is only applicable to systems with a finite number of degrees of freedom, and is not suitable for systems with an infinite number of degrees of freedom.\n\n**Correct Answer:** C) The canonical transformation introduced in the paper is a symplectic integrator, meaning that it preserves the symplectic structure of the phase space and does not introduce any artificial dissipation or damping.\n\n**Explanation:** The correct answer is C) because the paper states that the canonical transformation introduced in the paper is a symplectic integrator, which means that it preserves the symplectic structure of the phase space and does not introduce any artificial dissipation or damping. This is a key property of the algorithm, as it ensures that the integration is accurate and reliable. The other options are incorrect because they either misrepresent the algorithm (A and B), or are not applicable to the system in question (D)."}, "45": {"documentation": {"title": "COVID-19: The unreasonable effectiveness of simple models", "source": "Timoteo Carletti, Duccio Fanelli, Francesco Piazza", "docs_id": "2005.11085", "section": ["q-bio.PE", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "COVID-19: The unreasonable effectiveness of simple models. When the novel coronavirus disease SARS-CoV2 (COVID-19) was officially declared a pandemic by the WHO in March 2020, the scientific community had already braced up in the effort of making sense of the fast-growing wealth of data gathered by national authorities all over the world. However, despite the diversity of novel theoretical approaches and the comprehensiveness of many widely established models, the official figures that recount the course of the outbreak still sketch a largely elusive and intimidating picture. Here we show unambiguously that the dynamics of the COVID-19 outbreak belongs to the simple universality class of the SIR model and extensions thereof. Our analysis naturally leads us to establish that there exists a fundamental limitation to any theoretical approach, namely the unpredictable non-stationarity of the testing frames behind the reported figures. However, we show how such bias can be quantified self-consistently and employed to mine useful and accurate information from the data. In particular, we describe how the time evolution of the reporting rates controls the occurrence of the apparent epidemic peak, which typically follows the true one in countries that were not vigorous enough in their testing at the onset of the outbreak. The importance of testing early and resolutely appears as a natural corollary of our analysis, as countries that tested massively at the start clearly had their true peak earlier and less deaths overall."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The authors of the paper \"COVID-19: The unreasonable effectiveness of simple models\" argue that the dynamics of the COVID-19 outbreak can be described by the SIR model and its extensions. However, they also highlight the limitations of theoretical approaches due to the unpredictable non-stationarity of testing frames. What is the key insight that the authors claim to have gained from their analysis, and how does it relate to the importance of testing early and resolutely in controlling the spread of the pandemic?\n\nA) The authors found that the SIR model is not sufficient to capture the complexity of the COVID-19 outbreak, and that more advanced models are needed to accurately predict the spread of the disease.\nB) The authors demonstrated that the time evolution of reporting rates can control the occurrence of the apparent epidemic peak, and that early and vigorous testing is crucial in reducing the number of deaths.\nC) The authors showed that the SIR model is effective in describing the dynamics of the COVID-19 outbreak, but that the limitations of the model are due to the variability in testing rates across different countries.\nD) The authors found that the non-stationarity of testing frames is the primary driver of the unpredictability of the COVID-19 outbreak, and that this limitation can be mitigated through more frequent and widespread testing.\n\n**Correct answer:** B) The authors demonstrated that the time evolution of reporting rates can control the occurrence of the apparent epidemic peak, and that early and vigorous testing is crucial in reducing the number of deaths.\n\n**Explanation:** The correct answer is B) because the authors explicitly state that the time evolution of reporting rates controls the occurrence of the apparent epidemic peak, and that countries that tested massively at the start of the outbreak had their true peak earlier and fewer deaths overall. This is a key insight that the authors claim to have gained from their analysis, and it highlights the importance of testing early and resolutely in controlling the spread of the pandemic."}, "46": {"documentation": {"title": "Robust breast cancer detection in mammography and digital breast\n  tomosynthesis using annotation-efficient deep learning approach", "source": "William Lotter, Abdul Rahman Diab, Bryan Haslam, Jiye G. Kim, Giorgia\n  Grisot, Eric Wu, Kevin Wu, Jorge Onieva Onieva, Jerrold L. Boxerman, Meiyun\n  Wang, Mack Bandler, Gopal Vijayaraghavan, A. Gregory Sorensen", "docs_id": "1912.11027", "section": ["eess.IV", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust breast cancer detection in mammography and digital breast\n  tomosynthesis using annotation-efficient deep learning approach. Breast cancer remains a global challenge, causing over 1 million deaths globally in 2018. To achieve earlier breast cancer detection, screening x-ray mammography is recommended by health organizations worldwide and has been estimated to decrease breast cancer mortality by 20-40%. Nevertheless, significant false positive and false negative rates, as well as high interpretation costs, leave opportunities for improving quality and access. To address these limitations, there has been much recent interest in applying deep learning to mammography; however, obtaining large amounts of annotated data poses a challenge for training deep learning models for this purpose, as does ensuring generalization beyond the populations represented in the training dataset. Here, we present an annotation-efficient deep learning approach that 1) achieves state-of-the-art performance in mammogram classification, 2) successfully extends to digital breast tomosynthesis (DBT; \"3D mammography\"), 3) detects cancers in clinically-negative prior mammograms of cancer patients, 4) generalizes well to a population with low screening rates, and 5) outperforms five-out-of-five full-time breast imaging specialists by improving absolute sensitivity by an average of 14%. Our results demonstrate promise towards software that can improve the accuracy of and access to screening mammography worldwide."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the primary limitations of traditional deep learning approaches for breast cancer detection in mammography, and how does the proposed annotation-efficient deep learning approach address these challenges?\n\nA) The primary limitations are the need for large amounts of annotated data and the difficulty in ensuring generalization beyond the populations represented in the training dataset. The proposed approach addresses these challenges by using an annotation-efficient approach that achieves state-of-the-art performance in mammogram classification.\n\nB) The primary limitations are the high interpretation costs and significant false positive and false negative rates associated with traditional screening x-ray mammography. The proposed approach addresses these challenges by detecting cancers in clinically-negative prior mammograms of cancer patients and generalizing well to a population with low screening rates.\n\nC) The primary limitations are the lack of standardization in mammography images and the difficulty in obtaining high-quality annotations. The proposed approach addresses these challenges by using a deep learning approach that achieves state-of-the-art performance in mammogram classification and extends to digital breast tomosynthesis (DBT).\n\nD) The primary limitations are the need for specialized equipment and expertise to perform mammography and the difficulty in obtaining large amounts of annotated data. The proposed approach addresses these challenges by using an annotation-efficient approach that achieves state-of-the-art performance in mammogram classification and generalizes well to a population with low screening rates.\n\nCorrect Answer: A) The primary limitations are the need for large amounts of annotated data and the difficulty in ensuring generalization beyond the populations represented in the training dataset. The proposed approach addresses these challenges by using an annotation-efficient approach that achieves state-of-the-art performance in mammogram classification."}, "47": {"documentation": {"title": "Non-relativistic quark-antiquark potential: spectroscopy of\n  heavy-quarkonia and exotic SUSY quarkonia", "source": "Sameer M. Ikhdair and Ramazan Sever", "docs_id": "0904.1665", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-relativistic quark-antiquark potential: spectroscopy of\n  heavy-quarkonia and exotic SUSY quarkonia. The experiments at LHC have shown that the SUSY (exotic) bound states are likely to form bound states in an entirely similar fashion as ordinary quarks form bound states, i.e., quarkonium. Also, the interaction between two squarks is due to gluon exchange which is found to be very similar to that interaction between two ordinary quarks. This motivates us to solve the Schr\\\"{o}dinger equation with a strictly phenomenological static quark-antiquark potential: $V(r)=-Ar^{-1}+\\kappa \\sqrt{r}+V_{0}$ using the shifted large $N$-expansion method to calculate the low-lying spectrum of a heavy quark with anti-sbottom\\textbf{\\}$(c\\bar{\\widetilde{b}},b% \\bar{\\widetilde{b}})$ and sbottom with anti-sbottom $(\\widetilde{b}% \\bar{\\widetilde{b}})$ bound states with $m_{\\widetilde{b}}$ is set free. To have a full knowledge on spectrum, we also give the result for a heavier as well as for lighter sbottom masses. As a test for the reliability of these calculations, we fix the parameters of this potential by fitting the spin-triplet $(n^{3}S_{1})$ and center-of-gravity $l\\neq 0$ experimental spectrum of the ordinary heavy quarkonia $c\\bar{c},c\\bar{b}$ and $b% \\bar{b}$ to few $\\mathrm{MeV.}$ Our results are compared with other models to gauge the reliability of these predictions and point out differences."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation for using a strictly phenomenological static quark-antiquark potential to calculate the low-lying spectrum of heavy quarkonia and exotic SUSY quarkonia?\n\nA){The authors wanted to explore the possibility of SUSY breaking in the strong interaction.}\nB){The experiments at LHC have shown that SUSY bound states form bound states in a similar fashion as ordinary quarks, leading to a similar interaction between two squarks and two ordinary quarks.}\nC){The authors were trying to develop a new model for SUSY breaking in the strong interaction.}\nD){The potential was chosen arbitrarily without any theoretical motivation.}\n\nCorrect Answer: B){The experiments at LHC have shown that SUSY bound states form bound states in a similar fashion as ordinary quarks, leading to a similar interaction between two squarks and two ordinary quarks.}\n\nExplanation: The question requires the test-taker to understand the motivation behind the choice of the potential. The correct answer is based on the statement in the documentation that the experiments at LHC have shown that SUSY bound states are likely to form bound states in an entirely similar fashion as ordinary quarks, leading to a similar interaction between two squarks and two ordinary quarks. This is the primary motivation for using a strictly phenomenological static quark-antiquark potential. The other options are incorrect because they do not accurately reflect the motivation behind the choice of the potential."}, "48": {"documentation": {"title": "Can Inflation be Connected to Low Energy Particle Physics?", "source": "Mark P. Hertzberg (Stanford)", "docs_id": "1110.5650", "section": ["hep-ph", "astro-ph.CO", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Can Inflation be Connected to Low Energy Particle Physics?. It is an interesting question whether low energy degrees of freedom may be responsible for early universe inflation. To examine this, here we present a simple version of Higgs-inflation with minimal coupling to gravity and a quadratic inflationary potential. This quantitatively differs from the popular non-minimally coupled models, although it is qualitatively similar. In all such models, new heavy fields must enter in order for the theory to be well behaved in the UV. We show that in all cases the Higgs self coupling lambda must be quite small in order to integrate out the heavy fields and use the resulting low energy effective field theory of the Higgs to describe inflation. For moderately sized lambda, the UV completion is required and will, in general, determine the inflationary regime. We discuss the important issue of the arbitrariness of the Lagrangians used in all these setups by presenting a new class of such models, including a supergravity version. This suggests that the inflationary potential is disconnected from low energy physics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the main conclusion regarding the connection between low energy particle physics and early universe inflation, as presented in the Higgs-inflation model with minimal coupling to gravity and a quadratic inflationary potential?\n\nA) The Higgs self-coupling \u03bb must be large for UV completion and determines the inflationary regime.\nB) The Higgs self-coupling \u03bb must be small for UV completion and determines the inflationary regime.\nC) The Higgs self-coupling \u03bb is arbitrary and can be chosen freely, with no implications for inflation.\nD) The Higgs self-coupling \u03bb is not relevant to the inflationary regime, and UV completion is required regardless of \u03bb.\n\nCorrect Answer: B) The Higgs self-coupling \u03bb must be small for UV completion and determines the inflationary regime.\n\nExplanation: The text states that \"For moderately sized \u03bb, the UV completion is required and will, in general, determine the inflationary regime.\" This implies that the Higgs self-coupling \u03bb must be small for UV completion, and that \u03bb determines the inflationary regime. This is the correct answer.\n\nThe other options are incorrect because:\n\nA) The text states that \u03bb must be small, not large, for UV completion.\n\nC) The text suggests that the Higgs self-coupling \u03bb is not arbitrary, but rather has implications for the inflationary regime.\n\nD) The text implies that \u03bb is relevant to the inflationary regime, and that UV completion is not independent of \u03bb."}, "49": {"documentation": {"title": "Global analysis of the Sivers functions at NLO+NNLL in QCD", "source": "Miguel G. Echevarria, Zhong-Bo Kang, John Terry", "docs_id": "2009.10710", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Global analysis of the Sivers functions at NLO+NNLL in QCD. We perform global fit to the quark Sivers function within the transverse momentum dependent (TMD) factorization formalism in QCD. We simultaneously fit Sivers asymmetry data from Semi-Inclusive Deep Inelastic Scattering (SIDIS) at COMPASS, HERMES, and JLab, from Drell-Yan lepton pair production at COMPASS, and from $W/Z$ boson at RHIC. This extraction is performed at next-to-leading order (NLO) and next-to-next-to leading logarithmic (NNLL) accuracy. We find excellent agreement between our extracted asymmetry and the experimental data for SIDIS and Drell-Yan lepton pair production, while tension arises when trying to describe the spin asymmetries of $W/Z$ bosons at RHIC. We carefully assess the situation, and we study in details the impact of the RHIC data and their implications through different ways of performing the fit. In addition, we find that the quality of the description of $W/Z$ vector boson asymmetry data could be strongly sensitive to the DGLAP evolution of Qiu-Sterman function, besides the usual TMD evolution. We present discussion on this and the implications for measurements of the transverse-spin asymmetries at the future Electron Ion Collider."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: The authors of the paper \"Global analysis of the Sivers functions at NLO+NNLL in QCD\" perform a global fit to the quark Sivers function within the transverse momentum dependent (TMD) factorization formalism in QCD. They simultaneously fit data from four different experiments: Semi-Inclusive Deep Inelastic Scattering (SIDIS) at COMPASS, HERMES, and JLab, and from Drell-Yan lepton pair production at COMPASS. However, they find tension when trying to describe the spin asymmetries of W/Z bosons at RHIC.\n\nWhich of the following statements best describes the main finding of the authors regarding the fit to the W/Z boson data?\n\nA) The authors find excellent agreement between their extracted asymmetry and the experimental data for W/Z bosons at RHIC.\nB) The authors find that the quality of the description of W/Z vector boson asymmetry data is not sensitive to the DGLAP evolution of the Qiu-Sterman function.\nC) The authors find that the tension in the fit to the W/Z boson data is due to the limited statistics in the COMPASS experiment.\nD) The authors find that the quality of the description of W/Z vector boson asymmetry data could be strongly sensitive to the DGLAP evolution of the Qiu-Sterman function, besides the usual TMD evolution.\n\nCorrect Answer: D) The authors find that the quality of the description of W/Z vector boson asymmetry data could be strongly sensitive to the DGLAP evolution of the Qiu-Sterman function, besides the usual TMD evolution.\n\nExplanation: The correct answer is D) because the authors explicitly state that the quality of the description of W/Z vector boson asymmetry data could be strongly sensitive to the DGLAP evolution of the Qiu-Sterman function, besides the usual TMD evolution. This is a key finding of the paper and is supported by the text. The other options are incorrect because they do not accurately reflect the main finding of the authors regarding the fit to the W/Z boson data. Option A is incorrect because the authors actually find tension in the fit to the W/Z boson data, not agreement. Option B is incorrect because the authors do not state that the quality of the description is not sensitive to the DGLAP evolution of the Qiu-Sterman function. Option C is incorrect because the authors do not attribute the tension in the fit to the W/Z boson data to limited statistics in the COMPASS experiment."}, "50": {"documentation": {"title": "Deep neural networks for geometric multigrid methods", "source": "Nils Margenberg, Robert Jendersie, Thomas Richter, Christian Lessig", "docs_id": "2106.07687", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep neural networks for geometric multigrid methods. We investigate scaling and efficiency of the deep neural network multigrid method (DNN-MG). DNN-MG is a novel neural network-based technique for the simulation of the Navier-Stokes equations that combines an adaptive geometric multigrid solver, i.e. a highly efficient classical solution scheme, with a recurrent neural network with memory. The neural network replaces in DNN-MG one or multiple finest multigrid layers and provides a correction for the classical solve in the next time step. This leads to little degradation in the solution quality while substantially reducing the overall computational costs. At the same time, the use of the multigrid solver at the coarse scales allows for a compact network that is easy to train, generalizes well, and allows for the incorporation of physical constraints. Previous work on DNN-MG focused on the overall scheme and how to enforce divergence freedom in the solution. In this work, we investigate how the network size affects training and solution quality and the overall runtime of the computations. Our results demonstrate that larger networks are able to capture the flow behavior better while requiring only little additional training time. At runtime, the use of the neural network correction can even reduce the computation time compared to a classical multigrid simulation through a faster convergence of the nonlinear solve that is required at every time step."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a deep neural network (DNN) in the context of the Deep Neural Networks for Geometric Multigrid Methods (DNN-MG) approach, and how does it impact the overall computational efficiency of the method?\n\n**A)** The DNN allows for the incorporation of physical constraints, making the method more accurate, but at the cost of increased computational complexity.\n**B)** The DNN enables the use of a compact network that generalizes well, reducing the need for multiple finest multigrid layers and resulting in faster convergence of the nonlinear solve.\n**C)** The DNN provides a correction for the classical solve in the next time step, leading to little degradation in solution quality, but requiring significant additional training time.\n**D)** The DNN is used solely for the simulation of the Navier-Stokes equations, replacing the classical multigrid solver entirely.\n\n**Correct Answer:** B) The DNN enables the use of a compact network that generalizes well, reducing the need for multiple finest multigrid layers and resulting in faster convergence of the nonlinear solve.\n\n**Explanation:** The correct answer, B, highlights the primary advantage of using a DNN in DNN-MG, which is the ability to reduce the computational complexity of the method by using a compact network that generalizes well. This allows for faster convergence of the nonlinear solve, resulting in reduced computation time. The other options are incorrect because they either misrepresent the primary advantage of the DNN (A and C) or provide an incomplete or inaccurate description of the method (D)."}, "51": {"documentation": {"title": "Audiovisual Transformer Architectures for Large-Scale Classification and\n  Synchronization of Weakly Labeled Audio Events", "source": "Wim Boes and Hugo Van hamme", "docs_id": "1912.02615", "section": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Audiovisual Transformer Architectures for Large-Scale Classification and\n  Synchronization of Weakly Labeled Audio Events. We tackle the task of environmental event classification by drawing inspiration from the transformer neural network architecture used in machine translation. We modify this attention-based feedforward structure in such a way that allows the resulting model to use audio as well as video to compute sound event predictions. We perform extensive experiments with these adapted transformers on an audiovisual data set, obtained by appending relevant visual information to an existing large-scale weakly labeled audio collection. The employed multi-label data contains clip-level annotation indicating the presence or absence of 17 classes of environmental sounds, and does not include temporal information. We show that the proposed modified transformers strongly improve upon previously introduced models and in fact achieve state-of-the-art results. We also make a compelling case for devoting more attention to research in multimodal audiovisual classification by proving the usefulness of visual information for the task at hand,namely audio event recognition. In addition, we visualize internal attention patterns of the audiovisual transformers and in doing so demonstrate their potential for performing multimodal synchronization."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary motivation behind the development of the proposed audiovisual transformer architectures, and how do they contribute to the field of multimodal audiovisual classification?\n\nA) The primary motivation is to improve the performance of audio classification models by incorporating visual information, and the architectures contribute by enabling the use of both audio and video features for sound event predictions.\n\nB) The primary motivation is to tackle the task of environmental event classification by leveraging the transformer neural network architecture, and the architectures contribute by allowing the model to use audio and video features to compute sound event predictions.\n\nC) The primary motivation is to demonstrate the usefulness of visual information for audio event recognition, and the architectures contribute by providing a framework for multimodal synchronization.\n\nD) The primary motivation is to improve the performance of video classification models by incorporating audio information, and the architectures contribute by enabling the use of both audio and video features for video event predictions.\n\nCorrect Answer: B) The primary motivation is to tackle the task of environmental event classification by leveraging the transformer neural network architecture, and the architectures contribute by allowing the model to use audio and video features to compute sound event predictions.\n\nExplanation: The correct answer is B) because the question asks about the primary motivation behind the development of the proposed architectures, which is stated in the introduction as \"tackle the task of environmental event classification by drawing inspiration from the transformer neural network architecture used in machine translation\". The correct answer also accurately summarizes the contribution of the architectures, which is to enable the use of both audio and video features for sound event predictions."}, "52": {"documentation": {"title": "Face flips in origami tessellations", "source": "Hugo A. Akitaya and Vida Dujmovi and David Eppstein and Thomas C. Hull\n  and Kshitij Jain and Anna Lubiw", "docs_id": "1910.05667", "section": ["math.CO", "cs.CG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Face flips in origami tessellations. Given a flat-foldable origami crease pattern $G=(V,E)$ (a straight-line drawing of a planar graph on a region of the plane) with a mountain-valley (MV) assignment $\\mu:E\\to\\{-1,1\\}$ indicating which creases in $E$ bend convexly (mountain) or concavely (valley), we may \\emph{flip} a face $F$ of $G$ to create a new MV assignment $\\mu_F$ which equals $\\mu$ except for all creases $e$ bordering $F$, where we have $\\mu_F(e)=-\\mu(e)$. In this paper we explore the configuration space of face flips for a variety of crease patterns $G$ that are tilings of the plane, proving examples where $\\mu_F$ results in a MV assignment that is either never, sometimes, or always flat-foldable for various choices of $F$. We also consider the problem of finding, given two foldable MV assignments $\\mu_1$ and $\\mu_2$ of a given crease pattern $G$, a minimal sequence of face flips to turn $\\mu_1$ into $\\mu_2$. We find polynomial-time algorithms for this in the cases where $G$ is either a square grid or the Miura-ori, and show that this problem is NP-hard in the case where $G$ is the triangle lattice."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a flat-foldable origami crease pattern $G$ that is a tilings of the plane, specifically the triangle lattice. Given two foldable MV assignments $\\mu_1$ and $\\mu_2$ of $G$, determine whether there exists a minimal sequence of face flips to turn $\\mu_1$ into $\\mu_2$.\n\n**A)** The problem is in NP, and a polynomial-time algorithm can be found for the triangle lattice.\n\n**B)** The problem is NP-hard, and no polynomial-time algorithm exists for the triangle lattice.\n\n**C)** The problem can be solved in polynomial time using dynamic programming, regardless of the complexity of the crease pattern.\n\n**D)** The problem is solvable in polynomial time for the triangle lattice, but the running time increases exponentially with the number of faces in the crease pattern.\n\n**Correct Answer:** B) The problem is NP-hard, and no polynomial-time algorithm exists for the triangle lattice.\n\n**Explanation:** The problem is NP-hard because it involves finding a minimal sequence of face flips, which is a complex optimization problem. The triangle lattice is a specific example of a crease pattern that is known to be NP-hard. The correct answer is B) because the problem is NP-hard, and no polynomial-time algorithm exists for the triangle lattice. The other options are incorrect because they either overstate the complexity of the problem (A) or incorrectly claim that a polynomial-time algorithm exists (C and D)."}, "53": {"documentation": {"title": "Number of double-normal pairs in space", "source": "Andrey Kupavskii", "docs_id": "1412.4405", "section": ["math.CO", "cs.DM", "math.MG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Number of double-normal pairs in space. Given a set $V$ of points in $\\mathbb R^d$, two points $p$, $q$ from $V$ form a double-normal pair, if the set $V$ lies between two parallel hyperplanes that pass through $p$ and $q$, respectively, and that are orthogonal to the segment $pq$. In this paper we study the maximum number $N_d(n)$ of double-normal pairs in a set of $n$ points in $\\mathbb R^d$. It is not difficult to get from the famous Erd\\H{o}s-Stone theorem that $N_d(n) = \\frac 12(1-1/k)n^2+o(n^2)$ for a suitable integer $k = k(d)$ and it was shown in the paper by J. Pach and K. Swanepoel that $\\lceil d/2\\rceil\\le k(d)\\le d-1$ and that asymptotically $k(d)\\gtrsim d-O(\\log d)$. In this paper we sharpen the upper bound on $k(d)$, which, in particular, gives $k(4)=2$ and $k(5)=3$ in addition to the equality $k(3)=2$ established by J. Pach and K. Swanepoel. Asymptotically we get $k(d)\\le d- \\log_2k(d) = d - (1+ o(1)) \\log_2k(d)$ and show that this problem is connected with the problem of determining the maximum number of points in $\\mathbb R^d$ that form pairwise acute (or non-obtuse) angles."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the study on the maximum number of double-normal pairs in a set of points in $\\mathbb R^d$, what is the asymptotic upper bound on $k(d)$, and how does it relate to the problem of determining the maximum number of points in $\\mathbb R^d$ that form pairwise acute (or non-obtuse) angles?\n\n**A)** The asymptotic upper bound on $k(d)$ is $d - \\log_2k(d)$, and this bound is related to the problem of determining the maximum number of points in $\\mathbb R^d$ that form pairwise acute (or non-obtuse) angles through the connection between double-normal pairs and acute angles.\n\n**B)** The asymptotic upper bound on $k(d)$ is $d - (1+ o(1)) \\log_2k(d)$, and this bound is related to the problem of determining the maximum number of points in $\\mathbb R^d$ that form pairwise acute (or non-obtuse) angles through the connection between double-normal pairs and obtuse angles.\n\n**C)** The asymptotic upper bound on $k(d)$ is $d - \\log_2k(d)$, and this bound is related to the problem of determining the maximum number of points in $\\mathbb R^d$ that form pairwise obtuse angles through the connection between double-normal pairs and acute angles.\n\n**D)** The asymptotic upper bound on $k(d)$ is $d - (1+ o(1)) \\log_2k(d)$, and this bound is related to the problem of determining the maximum number of points in $\\mathbb R^d$ that form pairwise acute (or non-obtuse) angles through the connection between double-normal pairs and acute angles.\n\n**Correct Answer:** A) The asymptotic upper bound on $k(d)$ is $d - \\log_2k(d)$, and this bound is related to the problem of determining the maximum number of points in $\\mathbb R^d$ that form pairwise acute (or non-obtuse) angles through the connection between double-normal pairs and acute angles.\n\n**Explanation:** The correct answer is A) because the problem statement mentions that the asymptotic upper bound on $k(d)$ is $d - \\log_2k(d)$, and it also states that this bound is related to the problem of determining the maximum number of points in $\\mathbb R^d$ that form pairwise acute (or non-obtuse) angles through the connection between double-normal pairs and acute angles. The other options are incorrect because they either misstate the bound on $k(d)$ or incorrectly relate it to the problem of determining the maximum number of points that form pairwise obtuse angles."}, "54": {"documentation": {"title": "Instability of stratified two-phase flows in rectangular ducts", "source": "Alexander Gelfgat, Neima Brauner", "docs_id": "2004.01384", "section": ["physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Instability of stratified two-phase flows in rectangular ducts. The linear stability of stratified two-phase flows in rectangular ducts is studied numerically. The linear stability analysis takes into account all possible infinitesimal three-dimensional disturbances and is carried out by solution of the associated eigenproblem. The neutral stability boundary and the corresponding critical wave number are obtained for liquid - liquid and air - water systems. Depending on the problem parameters, the instability sets in owing to short, intermediate, of long wave most unstable perturbations. Patterns of the most unstable disturbances are reported and discussed. It is shown that the instability arises due to shear, or interfacial mechanisms. Effects of the surface tension and of width/height aspect ratio are also studied. The results support the premise that the stability analysis of stratified two-phase flow in the simpler geometry of two-infinite plates can provide a reasonable estimation of the conditions for which this flow pattern can be considered to be linearly stable."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism responsible for the instability of stratified two-phase flows in rectangular ducts, as identified in the study on the linear stability of stratified two-phase flows in rectangular ducts?\n\nA) Viscous effects\nB) Interfacial mechanisms, including shear and surface tension\nC) Thermal effects\nD) Pressure gradients\n\n**Correct Answer:** B) Interfacial mechanisms, including shear and surface tension\n\n**Explanation:** The study on the linear stability of stratified two-phase flows in rectangular ducts identifies that the instability arises due to interfacial mechanisms, including shear and surface tension. This is evident from the fact that the instability sets in owing to short, intermediate, or long wave most unstable perturbations, and patterns of the most unstable disturbances are reported and discussed. The study also highlights the effects of surface tension and width/height aspect ratio on the instability.\n\n**Candidate A (Viscous effects):** While viscosity plays a role in the stability of fluid flows, it is not the primary mechanism responsible for the instability of stratified two-phase flows in rectangular ducts.\n\n**Candidate B (Interfacial mechanisms, including shear and surface tension):** This is the correct answer, as the study identifies that interfacial mechanisms, including shear and surface tension, are the primary cause of instability in stratified two-phase flows in rectangular ducts.\n\n**Candidate C (Thermal effects):** Thermal effects are not mentioned in the study as a primary mechanism responsible for the instability of stratified two-phase flows in rectangular ducts.\n\n**Candidate D (Pressure gradients):** Pressure gradients are not mentioned in the study as a primary mechanism responsible for the instability of stratified two-phase flows in rectangular ducts."}, "55": {"documentation": {"title": "Learning to Automatically Catch Potholes in Worldwide Road Scene Images", "source": "J. Javier Yebes, David Montero, Ignacio Arriola", "docs_id": "2105.07986", "section": ["cs.CV", "cs.LG", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning to Automatically Catch Potholes in Worldwide Road Scene Images. Among several road hazards that are present in any paved way in the world, potholes are one of the most annoying and also involving higher maintenance costs. There exists an increasing interest on the automated detection of these hazards enabled by technological and research progress. Our research work tackled the challenge of pothole detection from images of real world road scenes. The main novelty resides on the application of the latest progress in AI to learn the visual appearance of potholes. We built a large dataset of images with pothole annotations. They contained road scenes from different cities in the world, taken with different cameras, vehicles and viewpoints under varied environmental conditions. Then, we fine-tuned four different object detection models based on Faster R-CNN and SSD deep neural networks. We achieved high average precision and the pothole detector was tested on the Nvidia DrivePX2 platform with GPGPU capability, which can be embedded on vehicles. Moreover, it was deployed on a real vehicle to notify the detected potholes to a given IoT platform as part of AUTOPILOT H2020 project."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a pothole detection system from images of real-world road scenes, and how does it relate to the broader context of autonomous vehicles?\n\n**A)** The primary motivation is to reduce maintenance costs and improve road safety, but it is not directly related to autonomous vehicles. The system can be used to notify drivers of potholes, which can be a hazard for human drivers.\n\n**B)** The primary motivation is to enable autonomous vehicles to detect and respond to potholes, which can be a critical safety feature in various environmental conditions. The system can be deployed on vehicles equipped with GPGPU capability, such as the Nvidia DrivePX2 platform.\n\n**C)** The primary motivation is to improve the accuracy of object detection models, but it is not directly related to autonomous vehicles. The system can be used to detect potholes in various road scenes, but it is not a critical safety feature for human drivers.\n\n**D)** The primary motivation is to reduce the need for human intervention in road maintenance, but it is not directly related to autonomous vehicles. The system can be used to detect potholes, but it is not a critical safety feature for human drivers.\n\n**Correct Answer:** B) The primary motivation is to enable autonomous vehicles to detect and respond to potholes, which can be a critical safety feature in various environmental conditions. The system can be deployed on vehicles equipped with GPGPU capability, such as the Nvidia DrivePX2 platform.\n\n**Explanation:** The correct answer is B) because the documentation states that the pothole detection system was deployed on a real vehicle as part of the AUTOPILOT H2020 project, which implies that the system is intended to be used in autonomous vehicles. Additionally, the system was fine-tuned for use on the Nvidia DrivePX2 platform, which is a GPGPU-enabled platform designed for autonomous vehicles. The other options are incorrect because they do not accurately reflect the primary motivation behind the development of the pothole detection system or its relationship to autonomous vehicles."}, "56": {"documentation": {"title": "Approximate Bayesian computation with the Wasserstein distance", "source": "Espen Bernton (Harvard University), Pierre E. Jacob (Harvard\n  University), Mathieu Gerber (University of Bristol), Christian P. Robert\n  (Universit\\'e Paris-Dauphine, PSL and University of Warwick)", "docs_id": "1905.03747", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Approximate Bayesian computation with the Wasserstein distance. A growing number of generative statistical models do not permit the numerical evaluation of their likelihood functions. Approximate Bayesian computation (ABC) has become a popular approach to overcome this issue, in which one simulates synthetic data sets given parameters and compares summaries of these data sets with the corresponding observed values. We propose to avoid the use of summaries and the ensuing loss of information by instead using the Wasserstein distance between the empirical distributions of the observed and synthetic data. This generalizes the well-known approach of using order statistics within ABC to arbitrary dimensions. We describe how recently developed approximations of the Wasserstein distance allow the method to scale to realistic data sizes, and propose a new distance based on the Hilbert space-filling curve. We provide a theoretical study of the proposed method, describing consistency as the threshold goes to zero while the observations are kept fixed, and concentration properties as the number of observations grows. Various extensions to time series data are discussed. The approach is illustrated on various examples, including univariate and multivariate g-and-k distributions, a toggle switch model from systems biology, a queueing model, and a L\\'evy-driven stochastic volatility model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Approximate Bayesian computation (ABC) is a popular approach to overcome the issue of numerical evaluation of likelihood functions in generative statistical models. The proposed method in the paper uses the Wasserstein distance between the empirical distributions of observed and synthetic data to compare the two. What is the main advantage of using the Wasserstein distance over traditional approaches such as using order statistics?\n\n**A)** It is more computationally efficient for high-dimensional data.\n**B)** It provides a more accurate estimate of the likelihood function.\n**C)** It avoids the loss of information associated with summaries of the data.\n**D)** It is only applicable to univariate data.\n\n**Correct Answer:** C) It avoids the loss of information associated with summaries of the data.\n\n**Explanation:** The correct answer is C) because the proposed method avoids the loss of information associated with summaries of the data, which is a key advantage over traditional approaches. The Wasserstein distance provides a more nuanced comparison between the empirical distributions of observed and synthetic data, allowing for a more accurate assessment of the likelihood function without sacrificing information.\n\n**Candidate A)** is incorrect because while the Wasserstein distance may be more computationally efficient for high-dimensional data, this is not the main advantage of using this approach.\n\n**Candidate B)** is incorrect because the Wasserstein distance does not provide a more accurate estimate of the likelihood function, but rather a more nuanced comparison.\n\n**Candidate D)** is incorrect because the Wasserstein distance is applicable to multivariate data, not only univariate data."}, "57": {"documentation": {"title": "Passive RIS vs.Hybrid RIS: A Comparative Study on Channel Estimation", "source": "Rafaela Schroeder, Jiguang He, Markku Juntti", "docs_id": "2010.06981", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Passive RIS vs.Hybrid RIS: A Comparative Study on Channel Estimation. The reconfigurable intelligent surface (RIS) plays an important role in maintaining the connectivity in millimeter wave (mmWave) MIMO systems when the direct channel between the transceivers is blocked. However, it is difficult to acquire the channel state information (CSI), which is essential for the design of RIS phase control matrix and beamforming vectors at the transceivers. In this paper, we compare the channel estimation (CE) performance and achieved spectral efficiency (SE) of the purely passive and hybrid RIS architectures. CE is done via atomic norm minimization (ANM). For the purely passive RIS, we follow a two-stage procedure to sequentially estimate the channel parameters, while for the hybrid RIS we estimate the individual channels at the RIS based on the observations from active RIS elements assuming alternating uplink and downlink training. The simulation results show that the purely passive RIS brings better CE and SE performance compared to the hybrid RIS under the same training overhead. We further consider different setups for the hybrid RIS and study the tradeoffs among them."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare the channel estimation (CE) performance and achieved spectral efficiency (SE) of purely passive RIS and hybrid RIS architectures, considering the tradeoffs among different setups for the hybrid RIS.\n\n**A)** The hybrid RIS architecture outperforms the purely passive RIS in terms of CE performance, but its SE is compromised due to the overhead of alternating uplink and downlink training.\n\n**B)** The purely passive RIS architecture achieves better CE performance and SE compared to the hybrid RIS under the same training overhead, but its performance degrades with increasing training overhead.\n\n**C)** The hybrid RIS architecture offers better SE performance than the purely passive RIS, but its CE performance is compromised due to the need for individual channel estimation at the RIS elements.\n\n**D)** The hybrid RIS architecture achieves comparable CE performance to the purely passive RIS, but its SE is improved due to the ability to estimate individual channels at the RIS elements.\n\n**Correct Answer:** B) The purely passive RIS architecture achieves better CE performance and SE compared to the hybrid RIS under the same training overhead, but its performance degrades with increasing training overhead.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"the purely passive RIS brings better CE and SE performance compared to the hybrid RIS under the same training overhead.\" This implies that the purely passive RIS architecture has an advantage in terms of CE performance and SE, but its performance may degrade as the training overhead increases."}, "58": {"documentation": {"title": "Accelerating key bioinformatics tasks 100-fold by improving memory\n  access", "source": "Igor Sfiligoi, Daniel McDonald and Rob Knight", "docs_id": "2104.09565", "section": ["cs.DC", "q-bio.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Accelerating key bioinformatics tasks 100-fold by improving memory\n  access. Most experimental sciences now rely on computing, and biological sciences are no exception. As datasets get bigger, so do the computing costs, making proper optimization of the codes used by scientists increasingly important. Many of the codes developed in recent years are based on the Python-based NumPy, due to its ease of use and good performance characteristics. The composable nature of NumPy, however, does not generally play well with the multi-tier nature of modern CPUs, making any non-trivial multi-step algorithm limited by the external memory access speeds, which are hundreds of times slower than the CPU's compute capabilities. In order to fully utilize the CPU compute capabilities, one must keep the working memory footprint small enough to fit in the CPU caches, which requires splitting the problem into smaller portions and fusing together as many steps as possible. In this paper, we present changes based on these principles to two important functions in the scikit-bio library, principal coordinates analysis and the Mantel test, that resulted in over 100x speed improvement in these widely used, general-purpose tools."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of the NumPy library in terms of its performance when used in conjunction with modern CPUs, and how can this limitation be addressed to fully utilize the CPU's compute capabilities?\n\n**A)** NumPy's lack of support for parallel processing limits its performance on modern CPUs.\n**B)** The composable nature of NumPy does not generally play well with the multi-tier nature of modern CPUs, leading to external memory access speeds that are hundreds of times slower than the CPU's compute capabilities.\n**C)** NumPy's reliance on a single, monolithic data structure limits its ability to take advantage of modern CPU architectures.\n**D)** The use of NumPy's vectorized operations can lead to increased memory usage, making it difficult to fit the working memory footprint in the CPU caches.\n\n**Correct Answer:** B) The composable nature of NumPy does not generally play well with the multi-tier nature of modern CPUs, leading to external memory access speeds that are hundreds of times slower than the CPU's compute capabilities.\n\n**Explanation:** The correct answer is B) because the documentation states that the composable nature of NumPy does not generally play well with the multi-tier nature of modern CPUs, leading to external memory access speeds that are hundreds of times slower than the CPU's compute capabilities. This limitation can be addressed by splitting the problem into smaller portions and fusing together as many steps as possible to keep the working memory footprint small enough to fit in the CPU caches."}, "59": {"documentation": {"title": "Spin susceptibility and effective mass of two-dimensional electrons in\n  MgxZn1-xO/ZnO heterostructures", "source": "A. Tsukazaki, A. Ohtomo, M. Kawasaki, S. Akasaka, H. Yuji, K. Tamura,\n  K. Nakahara, T. Tanabe, A. Kamisawa, T. Gokmen, J. Shabani, and M. Shayegan", "docs_id": "0811.4639", "section": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spin susceptibility and effective mass of two-dimensional electrons in\n  MgxZn1-xO/ZnO heterostructures. We report measurements of the spin susceptibility and the electron effective mass for two-dimensional electrons confined at the interfaces of MgxZn1-xO/ZnO single heterostructures (x = 0.05, 0.08, and 0.11), grown by molecular-beam epitaxy on (0001) ZnO substrates. By tuning the built-in polarization through control of the barrier composition, the electron density was systematically varied in the range of 5.6 x 10^11 to 1.6 x 10^12 cm^-2, corresponding to a range of 3.1 < rs < 5.2, where rs is the average electron spacing measured in units of the effective Bohr radius. We used the coincidence technique, where crossings of the spin-split Landau levels occur at critical tilt angles of magnetic field, to evaluate the spin susceptibility. In addition, we determined the effective mass from the temperature dependence of the Shubnikov-de Haas oscillations measured at the coincidence conditions. The susceptibility and the effective mass both gradually increase with decreasing electron density, reflecting the role of electron-electron interaction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary factor that influences the spin susceptibility and effective mass of two-dimensional electrons in MgxZn1-xO/ZnO heterostructures?\n\nA) The built-in polarization of the heterostructure\nB) The electron density in the heterostructure\nC) The temperature of the measurement\nD) The composition of the ZnO substrate\n\nCorrect Answer: B) The electron density in the heterostructure\n\nExplanation: The text states that both the spin susceptibility and the effective mass \"gradually increase with decreasing electron density\", indicating that the electron density is the primary factor influencing these properties. This is consistent with the role of electron-electron interaction, which becomes more significant as the electron density decreases."}}