{"0": {"documentation": {"title": "Bounds on axion-like particles from the diffuse supernova flux", "source": "Francesca Calore, Pierluca Carenza, Maurizio Giannotti, Joerg Jaeckel,\n  Alessandro Mirizzi", "docs_id": "2008.11741", "section": ["hep-ph", "astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bounds on axion-like particles from the diffuse supernova flux. The cumulative emission of Axion-Like Particles (ALPs) from all past core-collapse supernovae (SNe) would lead to a diffuse flux with energies ${\\mathcal O}(50)$ MeV. We use this to constrain ALPs featuring couplings to photons and to nucleons. ALPs coupled only to photons are produced in the SN core via the Primakoff process, and then converted into gamma rays in the Galactic magnetic field. We set a bound on $g_{a\\gamma} \\lesssim 5 \\times 10^{-10}~{\\rm GeV}^{-1}$ for $m_a \\lesssim 10^{-11}~{\\rm eV}$, using recent measurements of the diffuse gamma-ray flux observed by the Fermi-LAT telescope. However, if ALPs couple also with nucleons, their production rate in SN can be considerably enhanced due to the ALPs nucleon-nucleon bremsstrahlung process. Assuming the largest ALP-nucleon coupling phenomenologically allowed, bounds on the diffuse gamma-ray flux lead to a much stronger $g_{a\\gamma} \\lesssim 6 \\times 10^{-13}~{\\rm GeV}^{-1}$ for the same mass range. If ALPs are heavier than $\\sim$ keV, the decay into photons becomes significant, leading again to a diffuse gamma-ray flux. In the case of only photon coupling, we find, e.g. $g_{a\\gamma} \\lesssim 5 \\times 10^{-11}~{\\rm GeV}^{-1}$ for $m_a \\sim 5~{\\rm keV}$. Allowing for a (maximal) coupling to nucleons, the limit improves to the level of $g_{a\\gamma} \\lesssim 10^{-19}~{\\rm GeV}^{-1}$ for $m_a \\sim 20~{\\rm MeV}$, which represents the strongest constraint to date."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: What is the strongest constraint on the axion-like particle (ALP) coupling to nucleons, and what is the corresponding mass range for this constraint?\n\nA) $g_{a\\gamma} \\lesssim 5 \\times 10^{-10}~{\\rm GeV}^{-1}$ for $m_a \\lesssim 10^{-11}~{\\rm eV}$\nB) $g_{a\\gamma} \\lesssim 6 \\times 10^{-13}~{\\rm GeV}^{-1}$ for $m_a \\lesssim 10^{-11}~{\\rm eV}$\nC) $g_{a\\gamma} \\lesssim 10^{-19}~{\\rm GeV}^{-1}$ for $m_a \\sim 20~{\\rm MeV}$\nD) $g_{a\\gamma} \\lesssim 5 \\times 10^{-11}~{\\rm GeV}^{-1}$ for $m_a \\sim 5~{\\rm keV}$\n\nCorrect Answer: C) $g_{a\\gamma} \\lesssim 10^{-19}~{\\rm GeV}^{-1}$ for $m_a \\sim 20~{\\rm MeV}$\n\nExplanation: The correct answer is C) $g_{a\\gamma} \\lesssim 10^{-19}~{\\rm GeV}^{-1}$ for $m_a \\sim 20~{\\rm MeV}$. This is the strongest constraint on the ALP coupling to nucleons, as stated in the text. The mass range for this constraint is $m_a \\sim 20~{\\rm MeV}$, which is a significant improvement over the previous bounds. The other options are incorrect because they either refer to the coupling to photons (A and D) or a weaker bound on the coupling to nucleons (B)."}, "1": {"documentation": {"title": "Encoder Based Lifelong Learning", "source": "Amal Rannen Triki, Rahaf Aljundi, Mathew B. Blaschko and Tinne\n  Tuytelaars", "docs_id": "1704.01920", "section": ["cs.CV", "cs.AI", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Encoder Based Lifelong Learning. This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks. The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously. Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders. For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement. When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying. At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled. The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-of-the-art"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Encoder Based Lifelong Learning paper, what is the primary goal of using under-complete autoencoders for each task, and how does this contribute to the preservation of knowledge from previous tasks?\n\n**A)** To increase the capacity of the model to learn new features, while reducing the impact of forgetting previous tasks.\n**B)** To prevent the model from overfitting to the most recent task, by limiting the number of parameters that can be updated.\n**C)** To preserve the information on which previous tasks are mainly relying, by preventing the reconstructions of features from changing.\n**D)** To improve the model's ability to generalize to new environments, by allowing features to adjust to the most recent environment while preserving the low-dimensional submanifold.\n\n**Correct Answer:** C) To preserve the information on which previous tasks are mainly relying, by preventing the reconstructions of features from changing.\n\n**Explanation:** The correct answer is C) because the paper states that \"When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying.\" This indicates that the primary goal of using under-complete autoencoders is to prevent the model from forgetting previous tasks by preserving the information on which they are relying."}, "2": {"documentation": {"title": "RSO: A Gradient Free Sampling Based Approach For Training Deep Neural\n  Networks", "source": "Rohun Tripathi and Bharat Singh", "docs_id": "2005.05955", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "RSO: A Gradient Free Sampling Based Approach For Training Deep Neural\n  Networks. We propose RSO (random search optimization), a gradient free Markov Chain Monte Carlo search based approach for training deep neural networks. To this end, RSO adds a perturbation to a weight in a deep neural network and tests if it reduces the loss on a mini-batch. If this reduces the loss, the weight is updated, otherwise the existing weight is retained. Surprisingly, we find that repeating this process a few times for each weight is sufficient to train a deep neural network. The number of weight updates for RSO is an order of magnitude lesser when compared to backpropagation with SGD. RSO can make aggressive weight updates in each step as there is no concept of learning rate. The weight update step for individual layers is also not coupled with the magnitude of the loss. RSO is evaluated on classification tasks on MNIST and CIFAR-10 datasets with deep neural networks of 6 to 10 layers where it achieves an accuracy of 99.1% and 81.8% respectively. We also find that after updating the weights just 5 times, the algorithm obtains a classification accuracy of 98% on MNIST."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the Random Search Optimization (RSO) approach proposed in the paper, and how does it differ from traditional backpropagation with SGD?\n\n**A)** RSO is more computationally expensive than backpropagation with SGD, but it can achieve better results with fewer weight updates.\n**B)** RSO is a gradient-based optimization method that uses a learning rate to control the magnitude of weight updates, whereas backpropagation with SGD is gradient-free and does not require a learning rate.\n**C)** RSO is a Markov Chain Monte Carlo search-based approach that can make aggressive weight updates in each step, whereas backpropagation with SGD is a more conservative approach that updates weights based on the magnitude of the loss.\n**D)** RSO is a traditional optimization method that uses backpropagation with SGD as a subroutine, whereas backpropagation with SGD is a more general-purpose optimization method that can be used for a wide range of problems.\n\n**Correct Answer:** C) RSO is a Markov Chain Monte Carlo search-based approach that can make aggressive weight updates in each step, whereas backpropagation with SGD is a more conservative approach that updates weights based on the magnitude of the loss.\n\n**Explanation:** The correct answer is C) because RSO is a gradient-free Markov Chain Monte Carlo search-based approach that can make aggressive weight updates in each step, whereas backpropagation with SGD is a more conservative approach that updates weights based on the magnitude of the loss. This is a key advantage of RSO, as it allows for more aggressive weight updates and can lead to better results. The other options are incorrect because they do not accurately describe the primary advantage of RSO or its differences from backpropagation with SGD."}, "3": {"documentation": {"title": "Enhancing Visual Fashion Recommendations with Users in the Loop", "source": "Anurag Bhardwaj, Vignesh Jagadeesh, Wei Di, Robinson Piramuthu,\n  Elizabeth Churchill", "docs_id": "1405.4013", "section": ["cs.HC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Enhancing Visual Fashion Recommendations with Users in the Loop. We describe a completely automated large scale visual recommendation system for fashion. Existing approaches have primarily relied on purely computational models to solving this problem that ignore the role of users in the system. In this paper, we propose to overcome this limitation by incorporating a user-centric design of visual fashion recommendations. Specifically, we propose a technique that augments 'user preferences' in models by exploiting elasticity in fashion choices. We further design a user study on these choices and gather results from the 'wisdom of crowd' for deeper analysis. Our key insights learnt through these results suggest that fashion preferences when constrained to a particular class, contain important behavioral signals that are often ignored in recommendation design. Further, presence of such classes also reflect strong correlations to visual perception which can be utilized to provide aesthetically pleasing user experiences. Finally, we illustrate that user approval of visual fashion recommendations can be substantially improved by carefully incorporating these user-centric feedback into the system framework."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Enhancing Visual Fashion Recommendations with Users in the Loop\n\n**A)** The proposed system in the paper \"Enhancing Visual Fashion Recommendations with Users in the Loop\" relies solely on computational models to solve the problem of visual fashion recommendations, ignoring the role of users in the system.\n\n**B)** The authors of the paper suggest that incorporating user-centric design into visual fashion recommendations can improve user approval rates, but do not provide any evidence to support this claim.\n\n**C)** The key insights learned from the user study in the paper indicate that fashion preferences when constrained to a particular class contain important behavioral signals that are often ignored in recommendation design.\n\n**D)** The paper proposes a technique that augments 'user preferences' in models by exploiting elasticity in fashion choices, but does not provide any information on how this technique is implemented.\n\n**Correct Answer:** C) The key insights learned from the user study in the paper indicate that fashion preferences when constrained to a particular class contain important behavioral signals that are often ignored in recommendation design.\n\n**Explanation:** This question requires the test-taker to carefully read and understand the paper's abstract and introduction. The correct answer (C) is supported by the paper's statement that \"fashion preferences when constrained to a particular class, contain important behavioral signals that are often ignored in recommendation design.\" This is a key insight learned from the user study, and the test-taker must be able to identify it as the correct answer. The incorrect answers (A, B, and D) are designed to be plausible but incorrect, requiring the test-taker to carefully evaluate the information provided in the paper."}, "4": {"documentation": {"title": "Mercury-related health benefits from retrofitting coal-fired power\n  plants in China", "source": "Jiashuo Li, Sili Zhou, Wendong Wei, Jianchuan Qi, Yumeng Li, Bin Chen,\n  Ning Zhang, Dabo Guan, Haoqi Qian, Xiaohui Wu, Jiawen Miao, Long Chen, Sai\n  Liang, Kuishuang Feng", "docs_id": "2005.07346", "section": ["econ.GN", "physics.soc-ph", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mercury-related health benefits from retrofitting coal-fired power\n  plants in China. China has implemented retrofitting measures in coal-fired power plants (CFPPs) to reduce air pollution through small unit shutdown (SUS), the installation of air pollution control devices (APCDs) and power generation efficiency (PGE) improvement. The reductions in highly toxic Hg emissions and their related health impacts by these measures have not been well studied. To refine mitigation options, we evaluated the health benefits of reduced Hg emissions via retrofitting measures during China's 12th Five-Year Plan by combining plant-level Hg emission inventories with the China Hg Risk Source-Tracking Model. We found that the measures reduced Hg emissions by 23.5 tons (approximately 1/5 of that from CFPPs in 2010), preventing 0.0021 points of per-foetus intelligence quotient (IQ) decrements and 114 deaths from fatal heart attacks. These benefits were dominated by CFPP shutdowns and APCD installations. Provincial health benefits were largely attributable to Hg reductions in other regions. We also demonstrated the necessity of considering human health impacts, rather than just Hg emission reductions, in selecting Hg control devices. This study also suggests that Hg control strategies should consider various factors, such as CFPP locations, population densities and trade-offs between reductions of total Hg (THg) and Hg2+."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the primary factors that contribute to the health benefits of reducing Hg emissions from coal-fired power plants in China, according to the study?\n\n**A)** CFPP shutdowns and APCD installations\n**B)** Hg emission reductions in urban areas and industrial zones\n**C)** Power generation efficiency improvements and Hg control device installations\n**D)** Total Hg (THg) reductions and Hg2+ emissions\n\n**Correct Answer:** A) CFPP shutdowns and APCD installations\n\n**Explanation:** The study found that CFPP shutdowns and APCD installations were the primary factors that contributed to the health benefits of reducing Hg emissions in China. These measures resulted in a 23.5-ton reduction in Hg emissions, which in turn prevented 0.0021 points of per-foetus intelligence quotient (IQ) decrements and 114 deaths from fatal heart attacks. While other factors, such as Hg emission reductions in urban areas and industrial zones, and power generation efficiency improvements, may also contribute to health benefits, the study highlights the importance of CFPP shutdowns and APCD installations as the primary drivers of these benefits."}, "5": {"documentation": {"title": "Managing Innovation in Technical Education: Revisiting the Developmental\n  Strategies of Politeknik Brunei", "source": "Bashir Ahmed Bhuiyan, Mohammad Shahansha Molla, and Masud Alam", "docs_id": "2111.02850", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Managing Innovation in Technical Education: Revisiting the Developmental\n  Strategies of Politeknik Brunei. The present study aims at exploring the strategies for managing innovation in technical education by using blended learning philosophy and practices with special reference to Politeknik Brunei. Based on literature review and desk research, the study found out salient characteristics, explored constraining factors, elicited strategies of Politeknik Brunei, and suggested some options and a framework for innovations management and development of effective blended teaching and learning. The limiting factors identified are the unwillingness of the top-level management, lack of structural support, dearth of readiness of the stakeholders, the gap between teacher's expectations and changed students characteristics, and blended teaching myopia on the way of effective application of blended learning strategies. Notable suggestions for strategic development are developing wide-angle vision and self-renewal processes, analyzing the environment for needs determination. Clarity of purpose and tasks, technological adaptability, data-driven decision making, prompt feedback, flipped classroom, and development of learning clusters are other dimensions that may go a long way toward innovating teaching-learning and the overall development of an academic institution. Finally, the study suggested important guidelines for applying the strategies and proposed framework for quality blended learning and managing innovations in technical education."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the key limiting factors identified in the study on managing innovation in technical education at Politeknik Brunei, and how can they be addressed to implement effective blended learning strategies?\n\n**A)** The study found that the top-level management's willingness to adopt new strategies, lack of structural support, and dearth of readiness among stakeholders are the primary limiting factors. To address these, the institution should develop a wide-angle vision and self-renewal processes, analyze the environment for needs determination, and provide clarity of purpose and tasks.\n\n**B)** The study identified the unwillingness of top-level management, lack of technological adaptability, and dearth of readiness among stakeholders as the primary limiting factors. To address these, the institution should develop a technological adaptability framework, provide prompt feedback, and implement a flipped classroom approach.\n\n**C)** The study found that the limiting factors are the gap between teacher's expectations and changed student characteristics, blended teaching myopia, and lack of data-driven decision making. To address these, the institution should develop learning clusters, provide clarity of purpose and tasks, and use data-driven decision making to inform blended learning strategies.\n\n**D)** The study identified the lack of structural support, dearth of readiness among stakeholders, and blended teaching myopia as the primary limiting factors. To address these, the institution should develop a wide-angle vision and self-renewal processes, provide prompt feedback, and implement a flipped classroom approach.\n\n**Correct Answer:** A) The study found that the top-level management's willingness to adopt new strategies, lack of structural support, and dearth of readiness among stakeholders are the primary limiting factors. To address these, the institution should develop a wide-angle vision and self-renewal processes, analyze the environment for needs determination, and provide clarity of purpose and tasks.\n\n**Explanation:** The correct answer is A) because the study specifically identifies the top-level management's unwillingness, lack of structural support, and dearth of readiness among stakeholders as the primary limiting factors. The other options, while related to the topic, do not accurately capture the key limiting factors identified in the study. Option A provides a comprehensive explanation of the limiting factors and suggests specific strategies for addressing them, making it the most accurate answer."}, "6": {"documentation": {"title": "Correlations probed in direct two-nucleon removal reactions", "source": "E C Simpson and J A Tostevin", "docs_id": "1010.1619", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Correlations probed in direct two-nucleon removal reactions. Final-state-exclusive momentum distributions of fast, forward travelling residual nuclei, following two nucleon removal from fast secondary radioactive beams of projectile nuclei, can and have now been measured. Assuming that the most important reaction mechanism is the sudden direct removal of a pair of nucleons from a set of relatively simple, active shell-model orbital configurations, such distributions were predicted to depend strongly on the total angular momentum I carried by the two nucleons - the final state spin for spin 0+ projectiles. The sensitivity of these now-accessible observables to specific details of the (correlated) two-nucleon wave functions is of importance. We clarify that it is the total orbital angular momentum L of the two nucleons that is the primary factor in determining the shapes and widths of the calculated momentum distributions. It follows that, with accurate measurements, this dependence upon the L make-up of the two-nucleon wave functions could be used to assess the accuracy of (shell- or many-body) model predictions of these two-nucleon configurations. By use of several tailored examples, with specific combinations of active two-nucleon orbitals, we demonstrate that more subtle structure aspects may be observed, allowing such reactions to probe and/or confirm the details of theoretical model wave functions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary factor in determining the shapes and widths of the calculated momentum distributions in direct two-nucleon removal reactions, according to the shell-model predictions?\n\nA) The total spin of the projectile nucleus\nB) The total orbital angular momentum L of the two nucleons\nC) The total energy of the residual nucleus\nD) The total momentum of the projectile nucleus\n\nCorrect Answer: B) The total orbital angular momentum L of the two nucleons\n\nExplanation: According to the documentation, the shell-model predictions suggest that the total orbital angular momentum L of the two nucleons is the primary factor in determining the shapes and widths of the calculated momentum distributions. This is stated in the sentence: \"It follows that, with accurate measurements, this dependence upon the L make-up of the two-nucleon wave functions could be used to assess the accuracy of (shell- or many-body) model predictions of these two-nucleon configurations.\"\n\nThe other options are incorrect because:\n\n* A) The total spin of the projectile nucleus is not mentioned as a factor in determining the momentum distributions.\n* C) The total energy of the residual nucleus is not mentioned as a factor in determining the momentum distributions.\n* D) The total momentum of the projectile nucleus is not mentioned as a factor in determining the momentum distributions."}, "7": {"documentation": {"title": "Rapid Circadian Entrainment in Models of Circadian Genes Regulation", "source": "Jiawei Yin, Agung Julius, John T. Wen", "docs_id": "1902.09059", "section": ["q-bio.MN", "math.OC", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Rapid Circadian Entrainment in Models of Circadian Genes Regulation. The light-based minimum-time circadian entrainment problem for mammals, Neurospora, and Drosophila is studied based on the mathematical models of their circadian gene regulation. These models contain high order nonlinear differential equations. Two model simplification methods are applied to these high-order models: the phase response curves (PRC) and the Principal Orthogonal Decomposition (POD). The variational calculus and a gradient descent algorithm are applied for solving the optimal light input in the high-order models. As the results of the gradient descent algorithm rely heavily on the initial guesses, we use the optimal control of the PRC and the simplified model to initialize the gradient descent algorithm. In this paper, we present: (1) the application of PRC and direct shooting algorithm on high-order nonlinear models; (2) a general process for solving the minimum-time optimal control problem on high-order models; (3) the impacts of minimum-time optimal light on circadian gene transcription and protein synthesis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the study presented in the paper \"Rapid Circadian Entrainment in Models of Circadian Genes Regulation\"?\n\nA) To investigate the effects of light on circadian gene regulation in Drosophila\nB) To develop a mathematical model of circadian gene regulation in mammals\nC) To apply optimal control methods to high-order nonlinear models of circadian gene regulation\nD) To analyze the impact of light on circadian gene transcription and protein synthesis\n\n**Correct Answer:** C) To apply optimal control methods to high-order nonlinear models of circadian gene regulation\n\n**Explanation:** The study aims to develop a mathematical framework for solving the minimum-time circadian entrainment problem in high-order nonlinear models of circadian gene regulation. The authors apply various methods, including phase response curves (PRC) and Principal Orthogonal Decomposition (POD), to simplify the models and solve the optimal control problem. The primary goal is to apply optimal control methods to these high-order nonlinear models, making option C the correct answer.\n\n**Additional explanations:**\n\n* Option A is incorrect because while the study does involve Drosophila, it is not the primary focus of the study.\n* Option B is incorrect because the study builds upon existing mathematical models of circadian gene regulation, rather than developing a new model from scratch.\n* Option D is incorrect because while the study does investigate the impact of light on circadian gene transcription and protein synthesis, this is not the primary goal of the study."}, "8": {"documentation": {"title": "The Emergency of Pico-Kelvin Physics", "source": "Xuzong Chen and Bo Fan", "docs_id": "2005.01304", "section": ["cond-mat.quant-gas"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Emergency of Pico-Kelvin Physics. The frontier of low-temperature physics has advanced to the mid pico-Kelvin (pK) regime but progress has come to a halt because of the problem of gravity. Ultra cold atoms must be confined in some type of potential energy well: if the depth of the well is less than the energy an atom gains by falling through it, the atom escapes. This article reviews ultra cold atom research, emphasizing the advances that carried the low temperature frontier to 450 pico-Kelvin. We review micro gravity methods for overcoming the gravitation limit to achieve further low temperature using free fall techniques such as a drop tower, sounding rocket, parabolic flight plane and the Space Station. We describe two techniques that give promise of further advance--an atom chip and an all-optical trap--and present recent experimental results. Basic research in new regimes of observation has generally led to scientific discoveries and new technologies that benefit society. We expect this to be the case as the low temperature frontier advances and we propose some new opportunities for research."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary challenge in achieving further low-temperature advances in ultra-cold atom research, and how do researchers propose to overcome this limitation?\n\nA) The difficulty in cooling atoms to extremely low temperatures is due to the limitations of current trapping technologies, which are unable to confine atoms in a potential energy well deep enough to prevent escape.\n\nB) The primary challenge is the problem of gravity, which prevents ultra-cold atoms from being confined in a potential energy well, and researchers propose to overcome this limitation using microgravity methods such as free fall techniques.\n\nC) The main obstacle is the lack of sufficient energy to cool atoms to the desired low temperatures, and researchers are exploring new energy sources to overcome this limitation.\n\nD) The primary challenge is the difficulty in observing and measuring the behavior of ultra-cold atoms, and researchers propose to overcome this limitation using advanced observational techniques.\n\nCorrect Answer: B) The primary challenge is the problem of gravity, which prevents ultra-cold atoms from being confined in a potential energy well, and researchers propose to overcome this limitation using microgravity methods such as free fall techniques.\n\nExplanation: The correct answer is B) because the documentation states that the problem of gravity is the reason why progress in ultra-cold atom research has come to a halt, and that microgravity methods such as free fall techniques are being explored to overcome this limitation. The other options are incorrect because they do not accurately reflect the information provided in the documentation. Option A is incorrect because it states that the difficulty is due to limitations in trapping technologies, which is not mentioned in the documentation. Option C is incorrect because it states that the main obstacle is the lack of sufficient energy, which is not mentioned in the documentation. Option D is incorrect because it states that the primary challenge is the difficulty in observing and measuring the behavior of ultra-cold atoms, which is not mentioned in the documentation."}, "9": {"documentation": {"title": "Spin-current probe for phase transition in an insulator", "source": "Zhiyong Qiu, Jia Li, Dazhi Hou, Elke Arenholz, Alpha T. NDiaye, Ali\n  Tan, Ken-ichi Uchida, K. Sato, Satoshi Okamoto, Yaroslav Tserkovnyak, Z. Q.\n  Qiu, Eiji Saitoh", "docs_id": "1505.03926", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spin-current probe for phase transition in an insulator. Spin fluctuation and transition have always been one of central topics of magnetism and condensed matter science. Experimentally, the spin fluctuation is found transcribed onto scattering intensity in the neutron scattering process, which is represented by dynamical magnetic susceptibility and maximized at phase transitions. Importantly, a neutron carries spin without electric charge, and it can bring spin into a sample without being disturbed by electric energy, although large facilities such as a nuclear reactor is necessary. Here we show that spin pumping, frequently used in nanoscale spintronic devices, provides a desktop micro probe for spin transition; spin current is a flux of spin without an electric charge and its transport reflects spin excitation. We demonstrate detection of antiferromagnetic transition in ultra-thin CoO films via frequency dependent spin-current transmission measurements, which provides a versatile probe for phase transition in an electric manner in minute devices."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using spin pumping as a probe for phase transitions in insulators, as demonstrated in the study \"Spin-current probe for phase transition in an insulator\"?\n\nA) It allows for the detection of phase transitions in materials with high electric conductivity.\nB) It enables the measurement of spin fluctuations without disturbing the sample's electric energy.\nC) It provides a non-invasive method for probing phase transitions in materials without the need for large facilities.\nD) It enables the detection of phase transitions in materials with high magnetic susceptibility.\n\nCorrect Answer: B) It enables the measurement of spin fluctuations without disturbing the sample's electric energy.\n\nExplanation: The correct answer is B) It enables the measurement of spin fluctuations without disturbing the sample's electric energy. This is because neutron scattering is a technique that can detect spin fluctuations in materials, but it requires large facilities such as a nuclear reactor. In contrast, spin pumping allows for the detection of spin fluctuations without disturbing the sample's electric energy, making it a more versatile and practical probe for phase transitions."}, "10": {"documentation": {"title": "On a logarithmic sum related to a natural quadratic sieve", "source": "Sebastian Zuniga Alterman", "docs_id": "2005.04280", "section": ["math.NT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On a logarithmic sum related to a natural quadratic sieve. We study the sum $\\Sigma_q(U)=\\sum_{\\substack{d,e\\leq U\\\\(de,q)=1}}\\frac{\\mu(d)\\mu(e)}{[d,e]}\\log\\left(\\frac{U}{d}\\right)\\log\\left(\\frac{U}{e}\\right)$, $U>1$, so that a continuous, monotonic and explicit version of Selberg's sieve can be stated. Thanks to Barban-Vehov (1968), Motohashi (1974) and Graham (1978), it has been long known, but never explicitly, that $\\Sigma_1(U)$ is asymptotic to $\\log(U)$. In this article, we discover not only that $\\Sigma_q(U)\\sim\\frac{q}{\\varphi(q)}\\log(U)$ for all $q\\in\\mathbb{Z}_{>0}$, but also we find a closed-form expression for its secondary order term of $\\Sigma_q(U)$, a constant $\\mathfrak{s}_q$, which we are able to estimate explicitly when $q=v\\in\\{1,2\\}$. We thus have $\\Sigma_v(U)= \\frac{v}{\\varphi(v)}\\log(U)-\\mathfrak{s}_v+O_v^*\\left(\\frac{K_v}{\\log(U)}\\right)$, for some explicit constant $K_v > 0$, where $\\mathfrak{s}_1=0.60731\\ldots$ and $\\mathfrak{s}_2=1.4728\\ldots$. As an application, we show how our result gives an explicit version of the Brun-Titchmarsh theorem within a range."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let $\\Sigma_q(U)$ be the sum defined in the article, and let $\\mathfrak{s}_q$ be the constant term in the asymptotic expansion of $\\Sigma_q(U)$. Prove that for $q=2$, the constant $\\mathfrak{s}_2$ is equal to $1.4728\\ldots$, and explain why this result is significant in the context of the Brun-Titchmarsh theorem.\n\nA) The constant $\\mathfrak{s}_2$ is equal to $1.4728\\ldots$ because the authors of the article used a specific numerical method to estimate the constant term in the asymptotic expansion of $\\Sigma_2(U)$.\n\nB) The constant $\\mathfrak{s}_2$ is equal to $1.4728\\ldots$ because the authors of the article used a mathematical induction argument to prove that the asymptotic expansion of $\\Sigma_2(U)$ has a specific form.\n\nC) The constant $\\mathfrak{s}_2$ is equal to $1.4728\\ldots$ because the authors of the article used a computer simulation to estimate the constant term in the asymptotic expansion of $\\Sigma_2(U)$.\n\nD) The constant $\\mathfrak{s}_2$ is equal to $1.4728\\ldots$ because the authors of the article used a deep mathematical theorem, such as the Prime Number Theorem, to prove the asymptotic expansion of $\\Sigma_2(U)$.\n\nCorrect Answer: D) The constant $\\mathfrak{s}_2$ is equal to $1.4728\\ldots$ because the authors of the article used a deep mathematical theorem, such as the Prime Number Theorem, to prove the asymptotic expansion of $\\Sigma_2(U)$. This result is significant in the context of the Brun-Titchmarsh theorem because it provides an explicit estimate of the error term in the theorem, which is a crucial component in many applications of the theorem."}, "11": {"documentation": {"title": "An entropy stable spectral vanishing viscosity for discontinuous\n  Galerkin schemes: application to shock capturing and LES models", "source": "Andr\\'es Mateo-Gab\\'in, Juan Manzanero, Eusebio Valero", "docs_id": "2109.06653", "section": ["math.NA", "cs.NA", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An entropy stable spectral vanishing viscosity for discontinuous\n  Galerkin schemes: application to shock capturing and LES models. We present a stable spectral vanishing viscosity for discontinuous Galerkin schemes, with applications to turbulent and supersonic flows. The idea behind the SVV is to spatially filter the dissipative fluxes, such that it concentrates in higher wavenumbers, where the flow is typically under-resolved, leaving low wavenumbers dissipation-free. Moreover, we derive a stable approximation of the Guermond-Popov fluxes with the Bassi-Rebay 1 scheme, used to introduce density regularization in shock capturing simulations. This filtering uses a Cholesky decomposition of the fluxes that ensures the entropy stability of the scheme, which also includes a stable approximation of boundary conditions for adiabatic walls. For turbulent flows, we test the method with the three-dimensional Taylor-Green vortex and show that energy is correctly dissipated, and the scheme is stable when a kinetic energy preserving split-form is used in combination with a low dissipation Riemann solver. Finally, we test the shock capturing capabilities of our method with the Shu-Osher and the supersonic forward facing step cases, obtaining good results without spurious oscillations even with coarse meshes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the proposed spectral vanishing viscosity (SVV) in discontinuous Galerkin schemes, and how does it address the issue of under-resolved flows?\n\nA) To introduce a global viscosity that dampens all wavenumbers, thereby reducing numerical diffusion.\nB) To spatially filter dissipative fluxes, concentrating them in higher wavenumbers where the flow is typically under-resolved, while leaving low wavenumbers dissipation-free.\nC) To derive a stable approximation of the Guermond-Popov fluxes with the Bassi-Rebay 1 scheme, used to introduce density regularization in shock capturing simulations.\nD) To develop a low-dissipation Riemann solver that preserves kinetic energy in turbulent flows.\n\nCorrect Answer: B) To spatially filter dissipative fluxes, concentrating them in higher wavenumbers where the flow is typically under-resolved, while leaving low wavenumbers dissipation-free.\n\nExplanation: The correct answer is B) because the SVV aims to concentrate dissipative fluxes in higher wavenumbers, where the flow is typically under-resolved, thereby reducing numerical diffusion and preserving the accuracy of the simulation. This approach is entropy-stable and ensures that the scheme is stable even with coarse meshes. The other options are incorrect because they either describe a different approach (A), a related but distinct application (C), or a separate aspect of the method (D)."}, "12": {"documentation": {"title": "Bayesian Estimation of Economic Simulation Models using Neural Networks", "source": "Donovan Platt", "docs_id": "1906.04522", "section": ["econ.GN", "q-fin.CP", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian Estimation of Economic Simulation Models using Neural Networks. Recent advances in computing power and the potential to make more realistic assumptions due to increased flexibility have led to the increased prevalence of simulation models in economics. While models of this class, and particularly agent-based models, are able to replicate a number of empirically-observed stylised facts not easily recovered by more traditional alternatives, such models remain notoriously difficult to estimate due to their lack of tractable likelihood functions. While the estimation literature continues to grow, existing attempts have approached the problem primarily from a frequentist perspective, with the Bayesian estimation literature remaining comparatively less developed. For this reason, we introduce a Bayesian estimation protocol that makes use of deep neural networks to construct an approximation to the likelihood, which we then benchmark against a prominent alternative from the existing literature. Overall, we find that our proposed methodology consistently results in more accurate estimates in a variety of settings, including the estimation of financial heterogeneous agent models and the identification of changes in dynamics occurring in models incorporating structural breaks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of existing Bayesian estimation methods for economic simulation models, and how does the proposed methodology in the paper address this limitation?\n\nA) The primary limitation is the lack of computational power, and the proposed methodology addresses this by using more efficient algorithms. \nB) The primary limitation is the lack of tractable likelihood functions, and the proposed methodology addresses this by using deep neural networks to construct an approximation to the likelihood. \nC) The primary limitation is the lack of flexibility in assumptions, and the proposed methodology addresses this by allowing for more realistic assumptions. \nD) The primary limitation is the lack of empirical data, and the proposed methodology addresses this by using more data-intensive methods.\n\n**Correct Answer:** B) The primary limitation is the lack of tractable likelihood functions, and the proposed methodology addresses this by using deep neural networks to construct an approximation to the likelihood.\n\n**Explanation:** The paper states that existing Bayesian estimation methods for economic simulation models are limited by the lack of tractable likelihood functions, which makes it difficult to estimate these models. The proposed methodology addresses this limitation by using deep neural networks to construct an approximation to the likelihood, allowing for more accurate estimates. This is a key contribution of the paper and is the correct answer."}, "13": {"documentation": {"title": "Beyond the Standard Model Searches with the Majorana Demonstrator\n  Experiment", "source": "Wenqin Xu", "docs_id": "2110.11173", "section": ["hep-ex", "physics.ins-det"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Beyond the Standard Model Searches with the Majorana Demonstrator\n  Experiment. The Majorana Demonstrator experiment operated two modular arrays of p-type point contact high purity germanium (HPGe) detectors, of which 30 kg is enriched to 88% in Ge-76, to search for neutrinoless double beta decay. The data-taking campaign for double beta decay with enriched detectors was successfully concluded in March 2021, and data-taking with natural detectors is still ongoing. The Demonstrator has achieved excellent energy performance in a wide dynamic range covering 1 keV to 10 MeV. The extra-low background level and excellent energy performance achieved by the Demonstrator makes it competitive in various searches of physics beyond the Standard Model. If there is an axion-photon coupling, axions can be produced by the Primakoff conversion of photons in the Sun. Solar axions can inversely generate photon signals in germanium crystals, which can be coherently enhanced when the Bragg condition is satisfied. The Demonstrator is searching for solar axions with a novel method to correlate and leverage its high number of HPGe detectors. In this talk, we will discuss the status and results of recent searches for new physics with the Demonstrator, including the first reporting of a solar axion search."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary method used by the Majorana Demonstrator experiment to search for solar axions, and how does it leverage the high number of HPGe detectors?\n\nA) The Demonstrator uses a traditional method of counting individual axion signals, which is limited by the number of detectors.\nB) The Demonstrator employs a novel method to correlate and leverage the high number of HPGe detectors to enhance the signal-to-noise ratio, allowing for the detection of solar axions.\nC) The Demonstrator relies on the Primakoff conversion of photons in the Sun to produce axions, which are then detected by the HPGe detectors.\nD) The Demonstrator uses a machine learning algorithm to analyze the data from the HPGe detectors and identify patterns indicative of axion production.\n\nCorrect Answer: B) The Demonstrator employs a novel method to correlate and leverage the high number of HPGe detectors to enhance the signal-to-noise ratio, allowing for the detection of solar axions.\n\nExplanation: The correct answer is B) because the text states that the Demonstrator is searching for solar axions with a \"novel method to correlate and leverage its high number of HPGe detectors\". This suggests that the experiment is using a sophisticated technique to enhance the signal-to-noise ratio, allowing for the detection of solar axions. The other options are incorrect because they do not accurately reflect the method used by the Demonstrator to search for solar axions. Option A is incorrect because the text states that the Demonstrator has achieved excellent energy performance, suggesting that it is capable of detecting individual axion signals. Option C is incorrect because the Primakoff conversion is mentioned as a possible mechanism for axion production, but it is not the primary method used by the Demonstrator. Option D is incorrect because there is no mention of a machine learning algorithm being used to analyze the data from the HPGe detectors."}, "14": {"documentation": {"title": "Speckle decorrelation in fundamental and second-harmonic light scattered\n  from nonlinear disorder", "source": "Rabisankar Samanta, Romain Pierrat, R\\'emi Carminati and Sushil\n  Mujumdar", "docs_id": "2107.08736", "section": ["physics.optics", "cond-mat.dis-nn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Speckle decorrelation in fundamental and second-harmonic light scattered\n  from nonlinear disorder. Speckle patterns generated in a disordered medium carry a lot of information despite the complete randomness in the intensity pattern. When the medium possesses $\\chi^{(2)}$ nonlinearity, the speckle is sensitive to the phase of the incident fundamental light, as well as the light generated within. Here, we examine the speckle decorrelation in the fundamental and second-harmonic transmitted light as a function of varying power in the fundamental beam. At low powers, the speckle exhibits strong spatial correlations, which decrease with increasing incident power. We measure the statistical distributions of the correlation coefficients, which transform from sharp-peaked distributions at low power, to wide flat distributions at higher power. The average correlation in the second-harmonic speckle decays faster than in the fundamental speckle. Next, we construct a theoretical model, backed up by numerical computations, to obtain deeper physical insights on the faster decorrelations in the second-harmonic light. Whilst providing excellent qualitative agreement with the experiments, the model sheds important light on the contribution of two effects in the correlations, namely, the generation of second-harmonic light, and the propagation thereof."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the speckle decorrelation in fundamental and second-harmonic light scattered from nonlinear disorder in a medium. What is the primary factor that contributes to the faster decorrelation in the second-harmonic light, and how does it differ from the decorrelation in the fundamental light?\n\n**A)** The primary factor contributing to the faster decorrelation in the second-harmonic light is the generation of second-harmonic light, which is more susceptible to phase fluctuations. This is in contrast to the fundamental light, where the decorrelation is primarily influenced by the propagation of the fundamental beam.\n\n**B)** The primary factor contributing to the faster decorrelation in the second-harmonic light is the propagation of the second-harmonic light, which is more prone to phase fluctuations due to the nonlinearity of the medium. This is in contrast to the fundamental light, where the decorrelation is primarily influenced by the generation of second-harmonic light.\n\n**C)** The primary factor contributing to the faster decorrelation in the second-harmonic light is the spatial correlations in the fundamental light, which decay faster with increasing incident power. This is in contrast to the fundamental light, where the decorrelation is primarily influenced by the generation of second-harmonic light.\n\n**D)** The primary factor contributing to the faster decorrelation in the second-harmonic light is the statistical distribution of the correlation coefficients, which transforms from sharp-peaked distributions at low power to wide flat distributions at higher power. This is in contrast to the fundamental light, where the decorrelation is primarily influenced by the propagation of the fundamental beam.\n\n**Correct Answer:** A) The primary factor contributing to the faster decorrelation in the second-harmonic light is the generation of second-harmonic light, which is more susceptible to phase fluctuations. This is in contrast to the fundamental light, where the decorrelation is primarily influenced by the propagation of the fundamental beam.\n\n**Explanation:** The correct answer is A) because the documentation states that the average correlation in the second-harmonic speckle decays faster than in the fundamental speckle. This suggests that the generation of second-harmonic light is a primary factor contributing to the faster decorrelation in the second-harmonic light. Additionally, the documentation mentions that the speckle patterns generated in a disordered medium carry a lot of information despite the complete randomness in the intensity pattern, and that the speckle is sensitive to the phase of the incident fundamental light, as well as the light generated within. This implies that the generation of second-harmonic light is more susceptible to phase fluctuations, leading to faster decorrelation."}, "15": {"documentation": {"title": "Lagrangian single particle turbulent statistics through the\n  Hilbert-Huang Transform", "source": "Yongxiang Huang, Luca Biferale, Enrico Calzavarini, Chao Sun and\n  Federico Toschi", "docs_id": "1212.5741", "section": ["physics.flu-dyn", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lagrangian single particle turbulent statistics through the\n  Hilbert-Huang Transform. The Hilbert-Huang transform is applied to analyze single particle Lagrangian velocity data from numerical simulations of hydrodynamic turbulence. The velocity trajectory is described in terms of a set of intrinsic mode functions, C_{i}(t), and of their instantaneous frequency, \\omega_{i}(t). On the basis of this decomposition we define the \\omega-conditioned statistical moments of the C_{i} modes, named q-order Hilbert Spectra (HS). We show that such new quantities have enhanced scaling properties as compared to traditional Fourier transform- or correlation-based (Structure Functions) statistical indicators, thus providing better insights into the turbulent energy transfer process. We present a clear empirical evidence that the energy-like quantity, i.e. the second-order HS, displays a linear scaling in time in the inertial range, as expected from dimensional analysis and never observed before. We also measure high order moment scaling exponents in a direct way, without resorting the Extended Self Similarity (ESS) procedure. This leads to a new estimate of the Lagrangian structure functions exponents which are consistent with the multifractal prediction in the Lagrangian frame as proposed in [Biferale et al., Phys. Rev. Lett. vol. 93, 064502 (2004)]."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The Hilbert-Huang Transform is applied to analyze single particle Lagrangian velocity data from numerical simulations of hydrodynamic turbulence. According to the paper, what is the primary advantage of using the Hilbert-Huang Transform over traditional Fourier transform- or correlation-based statistical indicators, such as Structure Functions?\n\n**A)** The Hilbert-Huang Transform provides a more accurate representation of the turbulent energy transfer process.\n**B)** The Hilbert-Huang Transform offers enhanced scaling properties, allowing for better insights into the turbulent energy transfer process.\n**C)** The Hilbert-Huang Transform is more computationally efficient than traditional methods.\n**D)** The Hilbert-Huang Transform is only applicable to specific types of turbulence.\n\n**Correct Answer:** B) The Hilbert-Huang Transform offers enhanced scaling properties, allowing for better insights into the turbulent energy transfer process.\n\n**Explanation:** The correct answer is B) because the paper states that the new quantities defined by the Hilbert-Huang Transform, namely the q-order Hilbert Spectra (HS), have \"enhanced scaling properties as compared to traditional Fourier transform- or correlation-based (Structure Functions) statistical indicators\". This suggests that the Hilbert-Huang Transform provides a more nuanced understanding of the turbulent energy transfer process, which is the primary advantage of using this method."}, "16": {"documentation": {"title": "On the limit of English conversational speech recognition", "source": "Zolt\\'an T\\\"uske, George Saon, Brian Kingsbury", "docs_id": "2105.00982", "section": ["cs.CL", "cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the limit of English conversational speech recognition. In our previous work we demonstrated that a single headed attention encoder-decoder model is able to reach state-of-the-art results in conversational speech recognition. In this paper, we further improve the results for both Switchboard 300 and 2000. Through use of an improved optimizer, speaker vector embeddings, and alternative speech representations we reduce the recognition errors of our LSTM system on Switchboard-300 by 4% relative. Compensation of the decoder model with the probability ratio approach allows more efficient integration of an external language model, and we report 5.9% and 11.5% WER on the SWB and CHM parts of Hub5'00 with very simple LSTM models. Our study also considers the recently proposed conformer, and more advanced self-attention based language models. Overall, the conformer shows similar performance to the LSTM; nevertheless, their combination and decoding with an improved LM reaches a new record on Switchboard-300, 5.0% and 10.0% WER on SWB and CHM. Our findings are also confirmed on Switchboard-2000, and a new state of the art is reported, practically reaching the limit of the benchmark."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary contribution of the proposed study on the limit of English conversational speech recognition, and how does it improve upon previous results?\n\n**A)** The study introduces a new attention mechanism that reduces the recognition errors of the LSTM system on Switchboard-300 by 4% relative, but does not provide any further improvements.\n\n**B)** The study demonstrates that the use of an improved optimizer, speaker vector embeddings, and alternative speech representations can lead to significant reductions in recognition errors, with a new state-of-the-art result reported on Switchboard-300.\n\n**C)** The study proposes a new language model that combines the conformer with an improved decoder, resulting in a new record on Switchboard-300, but does not provide any further analysis on the impact of these changes.\n\n**D)** The study only reports results on the SWB part of Hub5'00, and does not provide any analysis on the performance of the proposed system on other datasets.\n\n**Correct Answer:** B) The study demonstrates that the use of an improved optimizer, speaker vector embeddings, and alternative speech representations can lead to significant reductions in recognition errors, with a new state-of-the-art result reported on Switchboard-300.\n\n**Explanation:** The correct answer is B) because the study highlights the use of an improved optimizer, speaker vector embeddings, and alternative speech representations as key contributions that lead to significant reductions in recognition errors. Additionally, the study reports a new state-of-the-art result on Switchboard-300, which further supports this answer. The other options are incorrect because they either omit important details (A), focus on a single aspect of the study (C), or are limited to a specific part of the dataset (D)."}, "17": {"documentation": {"title": "Asymptotic control of FWER under Gaussian assumption: application to\n  correlation tests", "source": "Sophie Achard (LMC - IMAG), Pierre Borgnat (Phys-ENS), Ir\\`ene Gannaz\n  (PSPM, ICJ)", "docs_id": "2007.00909", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymptotic control of FWER under Gaussian assumption: application to\n  correlation tests. In many applications, hypothesis testing is based on an asymptotic distribution of statistics. The aim of this paper is to clarify and extend multiple correction procedures when the statistics are asymptotically Gaussian. We propose a unified framework to prove their asymptotic behavior which is valid in the case of highly correlated tests. We focus on correlation tests where several test statistics are proposed. All these multiple testing procedures on correlations are shown to control FWER. An extensive simulation study on correlation-based graph estimation highlights finite sample behavior, independence on the sparsity of graphs and dependence on the values of correlations. Empirical evaluation of power provides comparisons of the proposed methods. Finally validation of our procedures is proposed on real dataset of rats brain connectivity measured by fMRI. We confirm our theoretical findings by applying our procedures on a full null hypotheses with data from dead rats. Data on alive rats show the performance of the proposed procedures to correctly identify brain connectivity graphs with controlled errors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed framework in the paper \"Asymptotic control of FWER under Gaussian assumption: application to correlation tests\"?\n\nA) To develop a new method for estimating the correlation between two variables\nB) To prove the asymptotic behavior of multiple correction procedures for correlation tests\nC) To investigate the finite sample behavior of correlation-based graph estimation\nD) To validate the proposed procedures on real-world datasets\n\n**Correct Answer:** B) To prove the asymptotic behavior of multiple correction procedures for correlation tests\n\n**Explanation:** The paper aims to clarify and extend multiple correction procedures for correlation tests under the assumption of an asymptotic Gaussian distribution. The proposed framework is designed to prove the asymptotic behavior of these procedures, which is essential for controlling the Family-Wise Error Rate (FWER) in multiple testing scenarios. The correct answer, B, reflects this primary goal of the paper.\n\n**Candidate A:** This option is incorrect because the paper does not focus on estimating the correlation between two variables. While correlation is a key concept in the paper, the primary goal is not to develop a new method for correlation estimation.\n\n**Candidate B:** This option is correct, as explained above. The paper aims to prove the asymptotic behavior of multiple correction procedures for correlation tests.\n\n**Candidate C:** This option is incorrect because while the paper does investigate the finite sample behavior of correlation-based graph estimation, this is not the primary goal of the paper. The paper's focus is on the asymptotic behavior of multiple correction procedures.\n\n**Candidate D:** This option is incorrect because the paper does not aim to validate the proposed procedures on real-world datasets. The paper does provide empirical evaluations of the proposed methods, but the primary goal is to prove their asymptotic behavior."}, "18": {"documentation": {"title": "Total Error and Variability Measures for the Quarterly Workforce\n  Indicators and LEHD Origin-Destination Employment Statistics in OnTheMap", "source": "Kevin L. McKinney and Andrew S. Green and Lars Vilhuber and John M.\n  Abowd", "docs_id": "2007.13275", "section": ["econ.EM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Total Error and Variability Measures for the Quarterly Workforce\n  Indicators and LEHD Origin-Destination Employment Statistics in OnTheMap. We report results from the first comprehensive total quality evaluation of five major indicators in the U.S. Census Bureau's Longitudinal Employer-Household Dynamics (LEHD) Program Quarterly Workforce Indicators (QWI): total flow-employment, beginning-of-quarter employment, full-quarter employment, average monthly earnings of full-quarter employees, and total quarterly payroll. Beginning-of-quarter employment is also the main tabulation variable in the LEHD Origin-Destination Employment Statistics (LODES) workplace reports as displayed in OnTheMap (OTM), including OnTheMap for Emergency Management. We account for errors due to coverage; record-level non-response; edit and imputation of item missing data; and statistical disclosure limitation. The analysis reveals that the five publication variables under study are estimated very accurately for tabulations involving at least 10 jobs. Tabulations involving three to nine jobs are a transition zone, where cells may be fit for use with caution. Tabulations involving one or two jobs, which are generally suppressed on fitness-for-use criteria in the QWI and synthesized in LODES, have substantial total variability but can still be used to estimate statistics for untabulated aggregates as long as the job count in the aggregate is more than 10."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** According to the study on Total Error and Variability Measures for the Quarterly Workforce, what is the minimum number of jobs required to estimate the five major indicators in the U.S. Census Bureau's LEHD Program Quarterly Workforce Indicators (QWI) with high accuracy?\n\n**A)** 3-9 jobs\n**B)** 10 jobs\n**C)** 20 jobs\n**D)** 50 jobs\n\n**Correct Answer:** B) 10 jobs\n\n**Explanation:** The study states that the five publication variables under study are estimated very accurately for tabulations involving at least 10 jobs. This suggests that a minimum of 10 jobs is required to estimate these indicators with high accuracy. Options A and C are incorrect because they represent a transition zone where cells may be fit for use with caution, while option D is an overestimation of the minimum required number of jobs."}, "19": {"documentation": {"title": "The unresolved mystery of the great divergence is solved", "source": "Ron W Nielsen", "docs_id": "1603.08344", "section": ["q-fin.EC", "q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The unresolved mystery of the great divergence is solved. The so-called great divergence in the income per capita is described in the Unified Growth Theory as the mind-boggling and unresolved mystery about the growth process. This mystery has now been solved: the great divergence never happened. It was created by the manipulation of data. Economic growth in various regions is at different levels of development but it follows similar, non-divergent trajectories. Unified Growth Theory is shown yet again to be incorrect and scientifically unacceptable. It promotes incorrect and even potentially dangerous concepts. The distorted presentation of data supporting the concept of the great divergence shows that economic growth is now developing along moderately-increasing trajectories but mathematical analysis of the same data and even their undistorted presentation shows that these trajectories are now increasing approximately vertically with time. So, while the distorted presentation of data used in the Unified Growth Theory suggests generally sustainable and secure economic growth, the undistorted presentation of data demonstrates that the growth is unsustainable and insecure. The concept of takeoffs from stagnation to the sustained-growth regime promoted in the Unified Growth Theory is also dangerously misleading because it suggests a sustainable and prosperous future while the mathematical analysis of data shows that the current economic growth is insecure and unsustainable."}, "answer": "Here is a difficult multi-form question based on the provided text:\n\nWhat is the primary conclusion drawn by the authors regarding the Unified Growth Theory, and how does it relate to the concept of economic growth?\n\nA) The Unified Growth Theory accurately describes the great divergence in income per capita, and its mathematical analysis supports the concept of sustainable economic growth.\nB) The great divergence in income per capita is a real phenomenon that has been solved by the Unified Growth Theory, which promotes a concept of takeoffs from stagnation to sustained-growth regimes.\nC) The Unified Growth Theory is incorrect and scientifically unacceptable, as it distorts the presentation of data to suggest generally sustainable and secure economic growth, but mathematical analysis reveals that the growth is unsustainable and insecure.\nD) The great divergence in income per capita is a myth created by the manipulation of data, and the Unified Growth Theory's mathematical analysis supports the concept of moderately-increasing trajectories of economic growth.\n\nCorrect Answer: C) The Unified Growth Theory is incorrect and scientifically unacceptable, as it distorts the presentation of data to suggest generally sustainable and secure economic growth, but mathematical analysis reveals that the growth is unsustainable and insecure."}, "20": {"documentation": {"title": "Developing classification indices for Chinese pulse diagnosis", "source": "Jian-Jun Shu and Yuguang Sun", "docs_id": "1402.6649", "section": ["physics.med-ph", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Developing classification indices for Chinese pulse diagnosis. Aim: To develop classification criteria for Chinese pulse diagnosis and to objectify the ancient diagnostic technique. Methods: Chinese pulse curves are treated as wave signals. Multidimensional variable analysis is performed to provide the best curve fit between the recorded Chinese pulse waveforms and the collective Gamma density functions. Results: Chinese pulses can be recognized quantitatively by the newly-developed four classification indices, that is, the wave length, the relative phase difference, the rate parameter, and the peak ratio. The new quantitative classification not only reduces the dependency of pulse diagnosis on Chinese physician's experience, but also is able to interpret pathological wrist-pulse waveforms more precisely. Conclusions: Traditionally, Chinese physicians use fingertips to feel the wrist-pulses of patients in order to determine their health conditions. The qualitative theory of the Chinese pulse diagnosis is based on the experience of Chinese physicians for thousands of years. However, there are no quantitative theories to relate these different wrist-pulse waveforms to the health conditions of patients. In this paper, new quantified classification indices have been introduced to interpret the Chinese pulse waveform patterns objectively."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the study \"Developing classification indices for Chinese pulse diagnosis\" in terms of the traditional qualitative theory of Chinese pulse diagnosis?\n\n**A)** To develop a new qualitative theory based on experience and intuition.\n**B)** To introduce a new quantitative theory to relate wrist-pulse waveforms to health conditions.\n**C)** To reduce the dependency of pulse diagnosis on Chinese physicians' experience.\n**D)** To improve the accuracy of pulse diagnosis through the use of mathematical models.\n\n**Correct Answer:** C) To reduce the dependency of pulse diagnosis on Chinese physicians' experience.\n\n**Explanation:** The study aims to develop classification criteria for Chinese pulse diagnosis, which would reduce the reliance on Chinese physicians' experience and provide a more objective method for interpreting wrist-pulse waveforms. This is evident from the statement \"The new quantitative classification not only reduces the dependency of pulse diagnosis on Chinese physician's experience, but also is able to interpret pathological wrist-pulse waveforms more precisely.\""}, "21": {"documentation": {"title": "First Results on the Search for Chameleons with the KWISP Detector at\n  CAST", "source": "S. Arguedas Cuendis, J. Baier, K. Barth, S. Baum, A. Bayirli, A.\n  Belov, H. Br\\\"auninger, G. Cantatore, J. M. Carmona, J. F. Castel, S. A.\n  Cetin, T. Dafni, M. Davenport, A. Dermenev, K. Desch, B. D\\\"obrich, H.\n  Fischer, W. Funk, J. A. Garc\\'ia, A. Gardikiotis, J. G. Garza, S. Gninenko,\n  M. D. Hasinoff, D. H. H. Hoffmann, F. J. Iguaz, I. G. Irastorza, K.\n  Jakov\\v{c}i\\'c, J. Kaminski, M. Karuza, C. Krieger, B. Laki\\'c, J. M.\n  Laurent, G. Luz\\'on, M. Maroudas, L. Miceli, S. Neff, I. Ortega, A. Ozbey, M.\n  J. Pivovaroff, M. Rosu, J. Ruz, E. Ruiz Ch\\'oliz, S. Schmidt, M. Schumann, Y.\n  K. Semertzidis, S. K. Solanki, L. Stewart, I. Tsagris, T. Vafeiadis, J. K.\n  Vogel, M. Vretenar, S. C. Yildiz, K. Zioutas", "docs_id": "1906.01084", "section": ["hep-ex", "astro-ph.CO", "astro-ph.SR", "physics.ins-det"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "First Results on the Search for Chameleons with the KWISP Detector at\n  CAST. We report on a first measurement with a sensitive opto-mechanical force sensor designed for the direct detection of coupling of real chameleons to matter. These dark energy candidates could be produced in the Sun and stream unimpeded to Earth. The KWISP detector installed on the CAST axion search experiment at CERN looks for tiny displacements of a thin membrane caused by the mechanical effect of solar chameleons. The displacements are detected by a Michelson interferometer with a homodyne readout scheme. The sensor benefits from the focusing action of the ABRIXAS X-ray telescope installed at CAST, which increases the chameleon flux on the membrane. A mechanical chopper placed between the telescope output and the detector modulates the incoming chameleon stream. We present the results of the solar chameleon measurements taken at CAST in July 2017, setting an upper bound on the force acting on the membrane of $80$~pN at 95\\% confidence level. The detector is sensitive for direct coupling to matter $10^4 \\leq\\beta_m \\leq 10^8$, where the coupling to photons is locally bound to $\\beta_\\gamma \\leq 10^{11}$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The KWISP detector at CAST is designed to detect the mechanical effect of solar chameleons on a thin membrane. What is the significance of the ABRIXAS X-ray telescope installed at CAST, and how does it impact the chameleon flux on the membrane?\n\nA) The ABRIXAS X-ray telescope is used to modulate the incoming chameleon stream, which is necessary for the detector's operation.\nB) The ABRIXAS X-ray telescope is used to focus the incoming chameleon flux on the membrane, increasing the sensitivity of the detector.\nC) The ABRIXAS X-ray telescope is used to detect the chameleons themselves, rather than their mechanical effect on the membrane.\nD) The ABRIXAS X-ray telescope is not used at all, and its absence would not affect the detector's operation.\n\n**Correct answer:** B) The ABRIXAS X-ray telescope is used to focus the incoming chameleon flux on the membrane, increasing the sensitivity of the detector.\n\n**Explanation:** The ABRIXAS X-ray telescope is installed at CAST to increase the chameleon flux on the membrane, which is essential for the detector's operation. By focusing the incoming chameleon flux, the detector can detect even smaller displacements of the membrane, allowing for more sensitive measurements. This is a critical aspect of the KWISP detector's design, as it enables the detection of dark energy candidates such as chameleons.\n\n**Additional question:**\n\n**Question:** The detector is sensitive for direct coupling to matter in the range $10^4 \\leq\\beta_m \\leq 10^8$. What is the significance of the upper bound on the force acting on the membrane of $80$~pN at 95\\% confidence level?\n\nA) It indicates that the detector is sensitive to forces as small as $80$~pN, but cannot detect forces larger than this value.\nB) It indicates that the detector is sensitive to forces in the range $80$~pN to $10^8$~pN, but the upper bound is not well constrained.\nC) It indicates that the detector is sensitive to forces in the range $10^4$~pN to $10^8$~pN, and the upper bound of $80$~pN is a conservative estimate.\nD) It indicates that the detector is not sensitive to forces at all, and the upper bound is a meaningless value.\n\n**Correct answer:** C) It indicates that the detector is sensitive to forces in the range $10^4$~pN to $10^8$~pN, and the upper bound of $80$~pN is a conservative estimate.\n\n**Explanation:** The upper bound on the force acting on the membrane of $80$~pN at 95\\% confidence level indicates that the detector is sensitive to forces in the range $10^4$~pN to $10^8$~pN. The fact that the upper bound is $80$~pN is a conservative estimate, meaning that the detector is likely to be able to detect forces smaller than this value. This is an important consideration in the design of the detector, as it allows for the detection of dark energy candidates such as chameleons."}, "22": {"documentation": {"title": "Consistent Inter-Model Specification for Time-Homogeneous SPX Stochastic\n  Volatility and VIX Market Models", "source": "Andrew Papanicolaou", "docs_id": "1812.05859", "section": ["q-fin.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Consistent Inter-Model Specification for Time-Homogeneous SPX Stochastic\n  Volatility and VIX Market Models. This paper shows how to recover stochastic volatility models (SVMs) from market models for the VIX futures term structure. Market models have more flexibility for fitting of curves than do SVMs, and therefore they are better-suited for pricing VIX futures and derivatives. But the VIX itself is a derivative of the S&P500 (SPX) and it is common practice to price SPX derivatives using an SVM. Hence, a consistent model for both SPX and VIX derivatives would be one where the SVM is obtained by inverting the market model. This paper's main result is a method for the recovery of a stochastic volatility function as the output of an inverse problem, with the inputs given by a VIX futures market model. Analysis will show that some conditions need to be met in order for there to not be any inter-model arbitrage or mis-priced derivatives. Given these conditions the inverse problem can be solved. Several models are analyzed and explored numerically to gain a better understanding of the theory and its limitations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a market model for the VIX futures term structure, which is a type of stochastic volatility model (SVM). The market model is given by the following equation:\n\ndVIX_t = \u03bcVIX_t dt + \u03c3VIX_t dW_t\n\nwhere VIX_t is the VIX index at time t, \u03bc is the drift term, \u03c3 is the volatility term, and dW_t is a Brownian motion.\n\nA) The market model can be used to price VIX futures and derivatives, but it is not suitable for pricing SPX derivatives.\n\nB) The market model can be used to price both VIX futures and derivatives, as well as SPX derivatives, as long as the stochastic volatility function is properly specified.\n\nC) The market model is a type of Black-Scholes model, and it can only be used to price European options.\n\nD) The market model is a type of Cox-Ingersoll-Ross model, and it can only be used to price American options.\n\n**Correct Answer:** B) The market model can be used to price both VIX futures and derivatives, as well as SPX derivatives, as long as the stochastic volatility function is properly specified.\n\n**Explanation:** The market model is a type of stochastic volatility model that can be used to price both VIX futures and derivatives, as well as SPX derivatives. However, the model requires a properly specified stochastic volatility function to ensure that the prices are consistent across different models. The paper \"Consistent Inter-Model Specification for Time-Homogeneous SPX Stochastic Volatility and VIX Market Models\" provides a method for recovering a stochastic volatility function from a VIX futures market model, which can be used to price SPX derivatives.\n\n**Note:** The other options are incorrect because:\n\n* A) The market model can be used to price both VIX futures and derivatives, as well as SPX derivatives, but it requires a properly specified stochastic volatility function.\n* C) The market model is not a type of Black-Scholes model, and it can be used to price both European and American options.\n* D) The market model is not a type of Cox-Ingersoll-Ross model, and it can be used to price both European and American options."}, "23": {"documentation": {"title": "Efficient nonmyopic active search with applications in drug and\n  materials discovery", "source": "Shali Jiang, Gustavo Malkomes, Benjamin Moseley, Roman Garnett", "docs_id": "1811.08871", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient nonmyopic active search with applications in drug and\n  materials discovery. Active search is a learning paradigm for actively identifying as many members of a given class as possible. A critical target scenario is high-throughput screening for scientific discovery, such as drug or materials discovery. In this paper, we approach this problem in Bayesian decision framework. We first derive the Bayesian optimal policy under a natural utility, and establish a theoretical hardness of active search, proving that the optimal policy can not be approximated for any constant ratio. We also study the batch setting for the first time, where a batch of $b>1$ points can be queried at each iteration. We give an asymptotic lower bound, linear in batch size, on the adaptivity gap: how much we could lose if we query $b$ points at a time for $t$ iterations, instead of one point at a time for $bt$ iterations. We then introduce a novel approach to nonmyopic approximations of the optimal policy that admits efficient computation. Our proposed policy can automatically trade off exploration and exploitation, without relying on any tuning parameters. We also generalize our policy to batch setting, and propose two approaches to tackle the combinatorial search challenge. We evaluate our proposed policies on a large database of drug discovery and materials science. Results demonstrate the superior performance of our proposed policy in both sequential and batch setting; the nonmyopic behavior is also illustrated in various aspects."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of efficient nonmyopic active search for drug and materials discovery, what is the theoretical hardness of active search, and what is the implication of this hardness on the approximation of the optimal policy?\n\n**A)** The theoretical hardness of active search implies that the optimal policy can be approximated using a constant ratio, and this approximation can be achieved with a fixed number of iterations.\n\n**B)** The theoretical hardness of active search implies that the optimal policy cannot be approximated for any constant ratio, and this hardness is linear in the batch size.\n\n**C)** The theoretical hardness of active search implies that the optimal policy can be approximated using a non-constant ratio, and this approximation can be achieved with a fixed number of iterations.\n\n**D)** The theoretical hardness of active search implies that the optimal policy can be approximated using a constant ratio, and this approximation can be achieved with a batch size of 1.\n\n**Correct Answer:** B) The theoretical hardness of active search implies that the optimal policy cannot be approximated for any constant ratio, and this hardness is linear in the batch size.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"We establish a theoretical hardness of active search, proving that the optimal policy can not be approximated for any constant ratio.\" This implies that the optimal policy cannot be approximated using a constant ratio, and the hardness is linear in the batch size, as stated in option B."}, "24": {"documentation": {"title": "Optimal Torque Control of Permanent Magnet Synchronous Motors Using\n  Adaptive Dynamic Programming", "source": "Ataollah Gogani Khiabani, Ali Heydari", "docs_id": "1911.03534", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Torque Control of Permanent Magnet Synchronous Motors Using\n  Adaptive Dynamic Programming. In this study, a new approach based on adaptive dynamic programming (ADP) is proposed to control permanent magnet synchronous motors (PMSMs). The objective of this paper is to control the torque and consequently the speed of a PMSM when an unknown load torque is applied to it. The proposed controller achieves a fast transient response, low ripples and small steady-state error. The control algorithm uses two neural networks, called critic and actor. The former is utilized to evaluate the cost and the latter is used to generate control signals. The training is done once offline and the calculated optimal weights of actor network are used in online control to achieve fast and accurate torque control of PMSMs. This algorithm is compared with field oriented control (FOC) and direct torque control based on space vector modulation (DTC-SVM). Simulations and experimental results show that the proposed algorithm provides desirable results under both accurate and uncertain modeled dynamics. Although the performance of FOC method is comparable with ADP under nominal conditions, the torque and speed response of ADP is better than FOC under realistic scenarios, that is, when parameter uncertainties exist."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare the performance of the proposed Adaptive Dynamic Programming (ADP) controller with the Field Oriented Control (FOC) and Direct Torque Control based on Space Vector Modulation (DTC-SVM) methods in terms of torque and speed response under nominal and realistic scenarios.\n\n**A)** The proposed ADP controller outperforms FOC and DTC-SVM methods in terms of torque and speed response under both nominal and realistic scenarios.\n\n**B)** The performance of FOC and DTC-SVM methods is comparable to the proposed ADP controller under nominal conditions, but ADP provides better results under realistic scenarios.\n\n**C)** The proposed ADP controller is only suitable for PMSMs with known load torque, while FOC and DTC-SVM methods can handle unknown load torque.\n\n**D)** The ADP controller requires more computational resources and training time compared to FOC and DTC-SVM methods.\n\n**Correct Answer:** B) The performance of FOC and DTC-SVM methods is comparable to the proposed ADP controller under nominal conditions, but ADP provides better results under realistic scenarios.\n\n**Explanation:**\n\nThe correct answer is B) because the documentation states that \"Although the performance of FOC method is comparable with ADP under nominal conditions, the torque and speed response of ADP is better than FOC under realistic scenarios, that is, when parameter uncertainties exist.\" This indicates that under nominal conditions, FOC and ADP perform similarly, but under realistic scenarios with parameter uncertainties, ADP outperforms FOC.\n\nThe other options are incorrect because:\n\n* A) is incorrect because the documentation does not state that ADP outperforms FOC and DTC-SVM under both nominal and realistic scenarios.\n* C) is incorrect because the documentation does not mention the suitability of ADP for PMSMs with known load torque.\n* D) is incorrect because the documentation does not mention any differences in computational resources or training time between ADP and FOC/DTC-SVM methods."}, "25": {"documentation": {"title": "On the growth and dissemination laws in a mathematical model of\n  metastatic growth", "source": "S\\'ebastien Benzekry (IMB, MONC), J. M. L. Ebos", "docs_id": "1511.01810", "section": ["q-bio.TO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the growth and dissemination laws in a mathematical model of\n  metastatic growth. Metastasis represents one of the main clinical challenge in cancer treatment since it is associated with the majority of deaths. Recent technological advances allow quantification of the dynamics of the process by means of noninvasive techniques such as longitudinal tracking of bioluminescent cells. The metastatic process was simplified here into two essential components -- dissemination and colonization -- which were mathematically formalized in terms of simple quantitative laws. The resulting mathematical model was confronted to in vivo experimental data of spontaneous metastasis after primary tumor resection. We discuss how much information can be inferred from confrontation of theories to the data with emphasis on identifiability issues. It is shown that two mutually exclusive assumptions for the secondary growth law (namely same or different from the primary tumor growth law) could fit equally well the data. Similarly, the fractal dimension coefficient in the dissemination law could not be uniquely determined from data on total metastatic burden only. Together, these results delimitate the range of information that can be recovered from fitting data of metastatic growth to already simplified mathematical models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the mathematical model of metastatic growth, the dissemination law is described by the equation:\n\nD(t) = \u03b1 \\* (D0 \\* t)^\u03b2 \\* (1 - (D0 \\* t)^(\u03b3+1))\n\nwhere D(t) is the dissemination at time t, D0 is the initial dissemination, \u03b1 is a constant, and \u03b2 and \u03b3 are parameters to be estimated. The model is confronted to in vivo experimental data of spontaneous metastasis after primary tumor resection.\n\n**A)** The value of \u03b2 can be uniquely determined from the data on total metastatic burden only.\n\n**B)** The value of \u03b3 can be uniquely determined from the data on the growth rate of the primary tumor only.\n\n**C)** The parameters \u03b2 and \u03b3 cannot be uniquely determined from the data on total metastatic burden and the growth rate of the primary tumor.\n\n**D)** The value of \u03b1 can be uniquely determined from the data on the dissemination at time t = 0 only.\n\n**Correct Answer:** C) The parameters \u03b2 and \u03b3 cannot be uniquely determined from the data on total metastatic burden and the growth rate of the primary tumor.\n\n**Explanation:** According to the documentation, the fractal dimension coefficient in the dissemination law could not be uniquely determined from data on total metastatic burden only, and similarly, the secondary growth law could fit equally well the data under two mutually exclusive assumptions. This implies that the parameters \u03b2 and \u03b3 cannot be uniquely determined from the data on total metastatic burden and the growth rate of the primary tumor."}, "26": {"documentation": {"title": "VLA Observations of the Gravitational Lens System Q2237+0305", "source": "E.E.Falco, J.Lehar (CfA), R.A.Perley (NRAO), J.Wambsganss (AIP\n  Potsdam), M.V.Gorenstein (Waters Corp.)", "docs_id": "astro-ph/9606048", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "VLA Observations of the Gravitational Lens System Q2237+0305. We report observations of the four-image gravitational lens system Q2237+0305 with the VLA at 20 cm and 3.6 cm. The quasar was detected at both frequencies (\\approx 0.7 mJy) with a flat spectrum. All four lensed images are clearly resolved at 3.6 cm, and the agreement of the radio and optical image positions is excellent. No radio emission is detected from the lensing galaxy, and any fifth lensed quasar image must be fainter than \\sim 20% of the A image flux density. Since the optical quasar images are variable and susceptible to extinction, radio flux ratios provide the best measurement of the macrolensing magnification ratios. The radio B/A and C/A image flux ratios are consistent with the observed range of optical variations, but the D/A ratio is consistently higher in the radio than in the optical. The radio ratios are consistent with magnification ratios predicted by lens models, and weaken alternative interpretations for Q2237+0305. More accurate radio ratios can distinguish between the models, as well as improve our understanding of both microlensing and extinction in this system."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: What can be inferred about the lensing galaxy in the Q2237+0305 system based on the radio observations?\n\nA) The lensing galaxy is actively accreting material, causing it to emit radio waves.\nB) The lensing galaxy is a faint, distant galaxy that is not contributing significantly to the lensing effect.\nC) The lensing galaxy is a massive, luminous galaxy that is causing the microlensing of the quasar.\nD) The lensing galaxy is a dark matter halo that is not emitting any radiation.\n\nCorrect Answer: B) The lensing galaxy is a faint, distant galaxy that is not contributing significantly to the lensing effect.\n\nExplanation: The text states that \"No radio emission is detected from the lensing galaxy\", which suggests that the lensing galaxy is not actively accreting material or emitting significant amounts of radio radiation. This implies that the lensing galaxy is a faint, distant galaxy that is not contributing significantly to the lensing effect."}, "27": {"documentation": {"title": "Micro-Estimates of Wealth for all Low- and Middle-Income Countries", "source": "Guanghua Chi, Han Fang, Sourav Chatterjee, Joshua E. Blumenstock", "docs_id": "2104.07761", "section": ["econ.GN", "cs.CY", "cs.LG", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Micro-Estimates of Wealth for all Low- and Middle-Income Countries. Many critical policy decisions, from strategic investments to the allocation of humanitarian aid, rely on data about the geographic distribution of wealth and poverty. Yet many poverty maps are out of date or exist only at very coarse levels of granularity. Here we develop the first micro-estimates of wealth and poverty that cover the populated surface of all 135 low and middle-income countries (LMICs) at 2.4km resolution. The estimates are built by applying machine learning algorithms to vast and heterogeneous data from satellites, mobile phone networks, topographic maps, as well as aggregated and de-identified connectivity data from Facebook. We train and calibrate the estimates using nationally-representative household survey data from 56 LMICs, then validate their accuracy using four independent sources of household survey data from 18 countries. We also provide confidence intervals for each micro-estimate to facilitate responsible downstream use. These estimates are provided free for public use in the hope that they enable targeted policy response to the COVID-19 pandemic, provide the foundation for new insights into the causes and consequences of economic development and growth, and promote responsible policymaking in support of the Sustainable Development Goals."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study on micro-estimates of wealth for low- and middle-income countries (LMICs) used a combination of data sources to develop their estimates. Which of the following data sources were used, and how did they contribute to the accuracy of the estimates?\n\nA) Only satellite imagery and mobile phone networks were used to develop the estimates.\nB) Topographic maps and aggregated and de-identified connectivity data from Facebook were used to develop the estimates, in addition to satellite imagery and mobile phone networks.\nC) Only nationally-representative household survey data from 56 LMICs were used to develop the estimates.\nD) Only independent sources of household survey data from 18 countries were used to validate the accuracy of the estimates.\n\n**Correct Answer:** B) Topographic maps and aggregated and de-identified connectivity data from Facebook were used to develop the estimates, in addition to satellite imagery and mobile phone networks.\n\n**Explanation:** The correct answer is B) because the documentation states that the authors used \"vast and heterogeneous data from satellites, mobile phone networks, topographic maps, as well as aggregated and de-identified connectivity data from Facebook\" to develop their estimates. This indicates that all four data sources were used, and that they contributed to the accuracy of the estimates. The other options are incorrect because they either omit one or more of the data sources used, or focus only on validation rather than development of the estimates."}, "28": {"documentation": {"title": "Modelling the downhill of the Sars-Cov-2 in Italy and a universal\n  forecast of the epidemic in the world", "source": "Gabriele Martelloni and Gianluca Martelloni", "docs_id": "2005.03455", "section": ["q-bio.PE", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modelling the downhill of the Sars-Cov-2 in Italy and a universal\n  forecast of the epidemic in the world. In a previous article [1] we have described the temporal evolution of the Sars- Cov-2 in Italy in the time window February 24-April 1. As we can see in [1] a generalized logistic equation captures both the peaks of the total infected and the deaths. In this article our goal is to study the missing peak, i.e. the currently infected one (or total currently positive). After the April 7 the large increase in the number of swabs meant that the logistical behavior of the infected curve no longer worked. So we decided to generalize the model, introducing new parameters. Moreover, we adopt a similar approach used in [1] (for the estimation of deaths) in order to evaluate the recoveries. In this way, introducing a simple conservation law, we define a model with 4 populations: total infected, currently positives, recoveries and deaths. Therefore, we propose an alternative method to a classical SIRD model for the evaluation of the Sars-Cov-2 epidemic. However, the method is general and thus applicable to other diseases. Finally we study the behavior of the ratio infected over swabs for Italy, Germany and USA, and we show as studying this parameter we recover the generalized Logistic model used in [1] for these three countries. We think that this trend could be useful for a future epidemic of this coronavirus."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the article propose a model to study the Sars-Cov-2 epidemic in Italy, which includes four populations: total infected, currently positives, recoveries, and deaths. They introduce a simple conservation law to estimate recoveries and generalize the logistic equation to capture the behavior of the currently infected population. What is the primary advantage of this approach over classical SIRD models?\n\n**A)** It allows for the estimation of recoveries using a simple conservation law.\n**B)** It generalizes the logistic equation to capture the behavior of the currently infected population.\n**C)** It provides a more accurate representation of the epidemic dynamics by including the currently infected population.\n**D)** It enables the estimation of the ratio of infected individuals to swabs, which can be useful for future epidemics.\n\n**Correct Answer:** C) It provides a more accurate representation of the epidemic dynamics by including the currently infected population.\n\n**Explanation:** The correct answer is C) because the authors' approach provides a more comprehensive representation of the epidemic dynamics by including the currently infected population, which is not accounted for in classical SIRD models. This allows for a more accurate estimation of the epidemic's behavior and can be useful for future epidemics. The other options are incorrect because while they are related to the model's features, they do not capture the primary advantage of this approach. Option A is incorrect because the conservation law is used to estimate recoveries, not the primary advantage of the approach. Option B is incorrect because the generalized logistic equation is used to capture the behavior of the currently infected population, but it is not the primary advantage of the approach. Option D is incorrect because the estimation of the ratio of infected individuals to swabs is not the primary advantage of the approach, although it is a related feature."}, "29": {"documentation": {"title": "Secure Authentication of ADS-B Aircraft Communications using Retroactive\n  Key Publication", "source": "Pavana Prakash, Ahmed Abdelhadi, Miao Pan", "docs_id": "1907.04909", "section": ["eess.SP", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Secure Authentication of ADS-B Aircraft Communications using Retroactive\n  Key Publication. Automatic Dependent Surveillance-Broadcast(ADSB), is the next generation Air Traffic management system to monitor the airspace for air traffic communication and traffic information. While the ADS-B empowers aircraft to broadcast their location information automatically and provide situational awareness, it is susceptible to attacks and security issues. In this paper, we introduce a method to secure the ADS-B protocol in aircraft communication using Retroactive Key Publication where senders publish their keys retroactively, which is different from the traditional asymmetric cryptography. The deduced solution does not rely on a connection or two-way packets exchange to establish security. It compensates for the loss of packets owing to huge air traffic, yet preserving the open and broadcast nature of ADS-B. Our proposed protocol uses the existing ADS-B system and same hardware with no modifications but still adds security. Our secure system has low impact on current operations and retains the operational efficiency of the current aircraft system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the proposed Retroactive Key Publication method in securing ADS-B aircraft communications, and how does it differ from traditional asymmetric cryptography?\n\nA) It provides a two-way packet exchange to establish security, which is not possible in ADS-B due to its open and broadcast nature.\nB) It compensates for the loss of packets due to huge air traffic, preserving the operational efficiency of the current aircraft system.\nC) It relies on a connection to establish security, which is not a characteristic of the proposed method.\nD) It uses a traditional asymmetric cryptography approach, which is not the case in the proposed method.\n\nCorrect Answer: B) It compensates for the loss of packets due to huge air traffic, preserving the operational efficiency of the current aircraft system.\n\nExplanation: The proposed method uses Retroactive Key Publication, which allows senders to publish their keys retroactively, without relying on a connection or two-way packet exchange to establish security. This approach compensates for the loss of packets due to huge air traffic, preserving the operational efficiency of the current aircraft system. This is a key advantage of the proposed method, as it allows for secure communication in a system where packet loss is a significant concern."}, "30": {"documentation": {"title": "Transient Stability Analysis with Physics-Informed Neural Networks", "source": "Jochen Stiasny, Georgios S. Misyris, Spyros Chatzivasileiadis", "docs_id": "2106.13638", "section": ["cs.LG", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transient Stability Analysis with Physics-Informed Neural Networks. We explore the possibility to use physics-informed neural networks to drastically accelerate the solution of ordinary differential-algebraic equations that govern the power system dynamics. When it comes to transient stability assessment, the traditionally applied methods either carry a significant computational burden, require model simplifications, or use overly conservative surrogate models. Conventional neural networks can circumvent these limitations but are faced with high demand of high-quality training datasets, while they ignore the underlying governing equations. Physics-informed neural networks are different: they incorporate the power system differential algebraic equations directly into the neural network training and drastically reduce the need for training data. This paper takes a deep dive into the performance of physics-informed neural networks for power system transient stability assessment. Introducing a new neural network training procedure to facilitate a thorough comparison, we explore how physics-informed neural networks compare with conventional differential-algebraic solvers and classical neural networks in terms of computation time, requirements in data, and prediction accuracy. We illustrate the findings on the Kundur two-area system, and assess the opportunities and challenges of physics-informed neural networks to serve as a transient stability analysis tool, highlighting possible pathways to further develop this method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using Physics-Informed Neural Networks (PINNs) for transient stability assessment in power systems, and how do they differ from conventional neural networks?\n\n**A)** PINNs can handle complex power system dynamics with minimal training data, whereas conventional neural networks require large datasets to achieve accurate results.\n\n**B)** PINNs incorporate the power system differential-algebraic equations directly into the neural network training, reducing the need for high-quality training data, whereas conventional neural networks ignore these equations.\n\n**C)** PINNs are limited to solving specific types of power system models, whereas conventional neural networks can be applied to a wide range of power system scenarios.\n\n**D)** PINNs are computationally more expensive than conventional neural networks, requiring more computational resources to achieve accurate results.\n\n**Correct Answer:** B) PINNs incorporate the power system differential-algebraic equations directly into the neural network training, reducing the need for high-quality training data, whereas conventional neural networks ignore these equations.\n\n**Explanation:** The correct answer is B) because the documentation states that Physics-Informed Neural Networks (PINNs) incorporate the power system differential-algebraic equations directly into the neural network training, which drastically reduces the need for high-quality training data. This is in contrast to conventional neural networks, which ignore the underlying governing equations. This key difference is the primary advantage of using PINNs for transient stability assessment in power systems."}, "31": {"documentation": {"title": "A breakthrough in Speech emotion recognition using Deep Retinal\n  Convolution Neural Networks", "source": "Yafeng Niu, Dongsheng Zou, Yadong Niu, Zhongshi He, Hua Tan", "docs_id": "1707.09917", "section": ["cs.SD", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A breakthrough in Speech emotion recognition using Deep Retinal\n  Convolution Neural Networks. Speech emotion recognition (SER) is to study the formation and change of speaker's emotional state from the speech signal perspective, so as to make the interaction between human and computer more intelligent. SER is a challenging task that has encountered the problem of less training data and low prediction accuracy. Here we propose a data augmentation algorithm based on the imaging principle of the retina and convex lens, to acquire the different sizes of spectrogram and increase the amount of training data by changing the distance between the spectrogram and the convex lens. Meanwhile, with the help of deep learning to get the high-level features, we propose the Deep Retinal Convolution Neural Networks (DRCNNs) for SER and achieve the average accuracy over 99%. The experimental results indicate that DRCNNs outperforms the previous studies in terms of both the number of emotions and the accuracy of recognition. Predictably, our results will dramatically improve human-computer interaction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the proposed data augmentation algorithm in the study on Speech Emotion Recognition using Deep Retinal Convolution Neural Networks?\n\n**A)** To improve the robustness of the model to different acoustic environments\n**B)** To increase the amount of training data for the model\n**C)** To enhance the model's ability to recognize emotions in speech signals\n**D)** To reduce the computational complexity of the model\n\n**Correct Answer:** B) To increase the amount of training data for the model\n\n**Explanation:** The study proposes a data augmentation algorithm to acquire different sizes of spectrogram and increase the amount of training data. This is done to address the problem of less training data and low prediction accuracy in Speech Emotion Recognition. The algorithm uses the imaging principle of the retina and convex lens to change the distance between the spectrogram and the convex lens, effectively increasing the amount of training data."}, "32": {"documentation": {"title": "Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup", "source": "Jang-Hyun Kim, Wonho Choo, Hyun Oh Song", "docs_id": "2009.06962", "section": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup. While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed. However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets. The source code is available at https://github.com/snu-mllab/PuzzleMix."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation behind the development of Puzzle Mix, a mixup method for augmenting deep neural networks?\n\nA) To create previously unseen virtual examples that can be used to fine-tune pre-trained models.\nB) To explicitly utilize the saliency information and underlying statistics of natural examples to improve generalization and adversarial robustness.\nC) To provide a simple and efficient way to generate mixup examples without requiring any additional computational resources.\nD) To develop a method that can be used to train deep neural networks on large datasets with limited computational resources.\n\nCorrect Answer: B) To explicitly utilize the saliency information and underlying statistics of natural examples to improve generalization and adversarial robustness.\n\nExplanation: The correct answer is B) because the documentation states that Puzzle Mix is a mixup method that \"explicitly utilizes the saliency information and the underlying statistics of the natural examples\" to improve generalization and adversarial robustness. This is the primary motivation behind the development of Puzzle Mix, as stated in the abstract of the paper. The other options are incorrect because they do not accurately reflect the motivation behind Puzzle Mix. Option A is incorrect because Puzzle Mix is not primarily focused on creating virtual examples for fine-tuning pre-trained models. Option C is incorrect because Puzzle Mix is a more complex method that requires additional computational resources to generate mixup examples. Option D is incorrect because Puzzle Mix is designed to be used on large datasets, not limited datasets."}, "33": {"documentation": {"title": "Perfect fluidity of a dissipative system: Analytical solution for the\n  Boltzmann equation in $\\mathrm{AdS}_{2}\\otimes \\mathrm{S}_{2}$", "source": "Jorge Noronha and Gabriel S. Denicol", "docs_id": "1502.05892", "section": ["hep-ph", "hep-th", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Perfect fluidity of a dissipative system: Analytical solution for the\n  Boltzmann equation in $\\mathrm{AdS}_{2}\\otimes \\mathrm{S}_{2}$. In this paper we obtain an analytical solution of the relativistic Boltzmann equation under the relaxation time approximation that describes the out-of-equilibrium dynamics of a radially expanding massless gas. This solution is found by mapping this expanding system in flat spacetime to a static flow in the curved spacetime $\\mathrm{AdS}_{2}\\otimes \\mathrm{S}_{2}$. We further derive explicit analytic expressions for the momentum dependence of the single particle distribution function as well as for the spatial dependence of its moments. We find that this dissipative system has the ability to flow as a perfect fluid even though its entropy density does not match the equilibrium form. The non-equilibrium contribution to the entropy density is shown to be due to higher order scalar moments (which possess no hydrodynamical interpretation) of the Boltzmann equation that can remain out of equilibrium but do not couple to the energy-momentum tensor of the system. Thus, in this system the slowly moving hydrodynamic degrees of freedom can exhibit true perfect fluidity while being totally decoupled from the fast moving, non-hydrodynamical microscopic degrees of freedom that lead to entropy production."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the relativistic Boltzmann equation in $\\mathrm{AdS}_{2}\\otimes \\mathrm{S}_{2}$, what is the relationship between the non-equilibrium contribution to the entropy density and the hydrodynamic degrees of freedom of the system?\n\nA) The non-equilibrium contribution to the entropy density is solely due to the energy-momentum tensor of the system, and the hydrodynamic degrees of freedom are in equilibrium with the microscopic degrees of freedom.\n\nB) The non-equilibrium contribution to the entropy density is due to higher order scalar moments of the Boltzmann equation that couple to the energy-momentum tensor of the system, and the hydrodynamic degrees of freedom are in equilibrium with the microscopic degrees of freedom.\n\nC) The non-equilibrium contribution to the entropy density is due to higher order scalar moments of the Boltzmann equation that possess no hydrodynamical interpretation, and the hydrodynamic degrees of freedom are decoupled from the microscopic degrees of freedom.\n\nD) The non-equilibrium contribution to the entropy density is solely due to the energy-momentum tensor of the system, and the hydrodynamic degrees of freedom are in equilibrium with the microscopic degrees of freedom, but the system exhibits perfect fluidity due to the presence of higher order scalar moments.\n\nCorrect Answer: C) The non-equilibrium contribution to the entropy density is due to higher order scalar moments of the Boltzmann equation that possess no hydrodynamical interpretation, and the hydrodynamic degrees of freedom are decoupled from the microscopic degrees of freedom.\n\nExplanation: The correct answer is C) because the documentation states that the non-equilibrium contribution to the entropy density is due to higher order scalar moments of the Boltzmann equation that possess no hydrodynamical interpretation, and that these moments do not couple to the energy-momentum tensor of the system. This means that the hydrodynamic degrees of freedom are decoupled from the microscopic degrees of freedom, allowing the system to exhibit perfect fluidity despite the non-equilibrium contribution to the entropy density."}, "34": {"documentation": {"title": "Existence, Local uniqueness and periodicity of bubbling solutions for a\n  critical nonlinear elliptic equation", "source": "Chunhua Wang, Qingfang Wang and Jing Yang", "docs_id": "2108.12206", "section": ["math.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Existence, Local uniqueness and periodicity of bubbling solutions for a\n  critical nonlinear elliptic equation. We revisit the following nonlinear critical elliptic equation \\begin{equation*} -\\Delta u+Q(y)u=u^{\\frac{N+2}{N-2}},\\;\\;\\; u>0\\;\\;\\;\\hbox{ in } \\mathbb{R}^N, \\end{equation*} where $N\\geq 5.$ Although there are some existence results of bubbling solutions for problem above, there are no results about the periodicity of bubbling solutions. Here we investigate some related problems. Assuming that $Q(y)$ is periodic in $y_1$ with period 1 and has a local minimum at 0 satisfying $Q(0)=0,$ we prove the existence and local uniqueness of infinitely many bubbling solutions of the problem above. This local uniqueness result implies that some bubbling solutions preserve the symmetry of the potential function $Q(y),$ i.e. the bubbling solution whose blow-up set is $\\{(jL,0,...,0):j=0,1,2,...,m\\}$ must be periodic in $y_{1}$ provided that $L$ is large enough, where $m$ is the number of the bubbles which is large enough but independent of $L.$ Moreover, we also show a non-existence of this bubbling solutions for the problem above if the local minimum of $Q(y)$ does not equal to zero."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the nonlinear critical elliptic equation\n\n$$-\\Delta u+Q(y)u=u^{\\frac{N+2}{N-2}},\\;\\;\\; u>0\\;\\;\\;\\hbox{ in } \\mathbb{R}^N,$$\n\nwhere $N\\geq 5.$ Suppose that $Q(y)$ is periodic in $y_1$ with period 1 and has a local minimum at 0 satisfying $Q(0)=0.$\n\nA) If the local minimum of $Q(y)$ is non-degenerate, then there exists a bubbling solution that is periodic in $y_1$.\n\nB) If the local minimum of $Q(y)$ is degenerate, then there exists a bubbling solution that is periodic in $y_1$.\n\nC) If the local minimum of $Q(y)$ is non-degenerate, then there exists a bubbling solution that is periodic in $y_1$ and the number of bubbles is independent of the blow-up radius $L$.\n\nD) If the local minimum of $Q(y)$ is degenerate, then there exists a bubbling solution that is periodic in $y_1$ and the number of bubbles is independent of the blow-up radius $L$.\n\nCorrect Answer: C\n\nExplanation: The proof of the local uniqueness result in the original paper implies that if the local minimum of $Q(y)$ is non-degenerate, then there exists a bubbling solution that is periodic in $y_1$ and the number of bubbles is independent of the blow-up radius $L$. This is because the bubbling solution preserves the symmetry of the potential function $Q(y)$, and the local minimum of $Q(y)$ is non-degenerate, which implies that the number of bubbles is independent of the blow-up radius $L$."}, "35": {"documentation": {"title": "Designing spontaneous behavioral switching via chaotic itinerancy", "source": "Katsuma Inoue, Kohei Nakajima, and Yasuo Kuniyoshi", "docs_id": "2002.08332", "section": ["cs.RO", "math.DS", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Designing spontaneous behavioral switching via chaotic itinerancy. Chaotic itinerancy is a frequently observed phenomenon in high-dimensional and nonlinear dynamical systems, and it is characterized by the random transitions among multiple quasi-attractors. Several studies have revealed that chaotic itinerancy has been observed in brain activity, and it is considered to play a critical role in the spontaneous, stable behavior generation of animals. Thus, chaotic itinerancy is a topic of great interest, particularly for neurorobotics researchers who wish to understand and implement autonomous behavioral controls for agents. However, it is generally difficult to gain control over high-dimensional nonlinear dynamical systems. Hence, the implementation of chaotic itinerancy has mainly been accomplished heuristically. In this study, we propose a novel way of implementing chaotic itinerancy reproducibly and at will in a generic high-dimensional chaotic system. In particular, we demonstrate that our method enables us to easily design both the trajectories of quasi-attractors and the transition rules among them simply by adjusting the limited number of system parameters and by utilizing the intrinsic high-dimensional chaos. Finally, we quantitatively discuss the validity and scope of application through the results of several numerical experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Designing spontaneous behavioral switching via chaotic itinerancy is a topic of great interest in neurorobotics research. What are the main challenges in implementing chaotic itinerancy in high-dimensional nonlinear dynamical systems, and how does the proposed method in the study address these challenges?\n\n**A)** The main challenges in implementing chaotic itinerancy are the difficulty in gaining control over the system and the need for a more systematic approach to designing quasi-attractors and transition rules. The proposed method addresses these challenges by allowing for the adjustment of a limited number of system parameters and the utilization of intrinsic high-dimensional chaos.\n\n**B)** The main challenges in implementing chaotic itinerancy are the need for a more sophisticated understanding of the underlying nonlinear dynamics and the requirement for a more precise control over the system's parameters. The proposed method addresses these challenges by providing a novel way of implementing chaotic itinerancy reproducibly and at will.\n\n**C)** The main challenges in implementing chaotic itinerancy are the difficulty in quantifying the validity and scope of application of the method, and the need for further research on the underlying theoretical foundations. The proposed method addresses these challenges by providing a quantitative discussion of the results of several numerical experiments.\n\n**D)** The main challenges in implementing chaotic itinerancy are the need for a more efficient algorithm for designing quasi-attractors and transition rules, and the requirement for a more robust method for controlling the system's behavior. The proposed method addresses these challenges by providing a novel way of implementing chaotic itinerancy reproducibly and at will.\n\n**Correct Answer:** A) The main challenges in implementing chaotic itinerancy are the difficulty in gaining control over the system and the need for a more systematic approach to designing quasi-attractors and transition rules. The proposed method addresses these challenges by allowing for the adjustment of a limited number of system parameters and the utilization of intrinsic high-dimensional chaos.\n\n**Explanation:** The correct answer is A) because it accurately summarizes the main challenges in implementing chaotic itinerancy in high-dimensional nonlinear dynamical systems, as mentioned in the documentation. The proposed method in the study addresses these challenges by providing a novel way of implementing chaotic itinerancy reproducibly and at will, which is achieved by allowing for the adjustment of a limited number of system parameters and the utilization of intrinsic high-dimensional chaos."}, "36": {"documentation": {"title": "Spin-dependent scattering induced negative magnetoresistance in\n  topological insulator Bi2Te3 nanowires", "source": "Biplab Bhattacharyya, Bahadur Singh, R. P. Aloysius, Reena Yadav,\n  Chenliang Su, Hsin Lin, S. Auluck, Anurag Gupta, T. D. Senguttuvan, and\n  Sudhir Husale", "docs_id": "1905.10219", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spin-dependent scattering induced negative magnetoresistance in\n  topological insulator Bi2Te3 nanowires. Studies of negative magnetoresistance in novel materials have recently been in the forefront of spintronic research. Here, we report an experimental observation of the temperature dependent negative magnetoresistance in Bi2Te3 topological insulator (TI) nanowires at ultralow temperatures (20 mK). We find a crossover from negative to positive magnetoresistance while increasing temperature under longitudinal magnetic field. We observe a large negative magnetoresistance which reaches -22% at 8T. The interplay between negative and positive magnetoresistance can be understood in terms of the competition between dephasing and spin-orbit scattering time scales. Based on the first-principles calculations within a density functional theory framework, we demonstrate that disorder (substitutional) by Ga+ ion milling process, which is used to fabricate nanowires, induces local magnetic moments in Bi2Te3 crystal that can lead to spin-dependent scattering of surface and bulk electrons. These experimental findings show a significant advance in the nanoscale spintronics applications based on longitudinal magnetoresistance in TIs. Our experimental results of large negative longitudinal magnetoresistance in 3D TIs further indicate that axial anomaly is a universal phenomenon in generic 3D metals."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the underlying mechanism that explains the crossover from negative to positive magnetoresistance in Bi2Te3 topological insulator (TI) nanowires as the temperature increases under a longitudinal magnetic field?\n\nA) The competition between dephasing and spin-orbit scattering time scales leads to a decrease in the negative magnetoresistance as the temperature increases.\nB) The introduction of disorder (substitutional) by Ga+ ion milling process induces local magnetic moments in Bi2Te3 crystal that can lead to spin-dependent scattering of surface and bulk electrons, resulting in a crossover from negative to positive magnetoresistance.\nC) The axial anomaly is a universal phenomenon in generic 3D metals, and its presence in Bi2Te3 TI nanowires is responsible for the observed negative magnetoresistance.\nD) The longitudinal magnetic field induces a phase transition in the Bi2Te3 crystal, leading to a change in the spin-dependent scattering mechanism and resulting in a crossover from negative to positive magnetoresistance.\n\nCorrect Answer: B) The introduction of disorder (substitutional) by Ga+ ion milling process induces local magnetic moments in Bi2Te3 crystal that can lead to spin-dependent scattering of surface and bulk electrons, resulting in a crossover from negative to positive magnetoresistance.\n\nExplanation: The correct answer is based on the explanation provided in the documentation, which states that the disorder (substitutional) by Ga+ ion milling process induces local magnetic moments in Bi2Te3 crystal that can lead to spin-dependent scattering of surface and bulk electrons. This is the underlying mechanism that explains the crossover from negative to positive magnetoresistance in Bi2Te3 TI nanowires as the temperature increases under a longitudinal magnetic field."}, "37": {"documentation": {"title": "Hybrid Dynamic Density Functional Theory for Polymer Melts and Blends", "source": "Takashi Honda, Toshihiro Kawakatsu", "docs_id": "cond-mat/0609081", "section": ["cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hybrid Dynamic Density Functional Theory for Polymer Melts and Blends. We propose a high-speed and accurate hybrid dynamic density functional theory for the computer simulations of the phase separation processes of polymer melts and blends. The proposed theory is a combination of the dynamic self-consistent field (SCF) theory and a time-dependent Ginzburg-Landau type theory with the random phase approximation (GRPA). The SCF theory is known to be accurate in evaluating the free energy of the polymer systems in both weak and strong segregation regions although it has a disadvantage of the requirement of a considerable amount of computational cost. On the other hand, the GRPA theory has an advantage of much smaller amount of required computational cost than the SCF theory while its applicability is limited to the weak segregation region. To make the accuracy of the SCF theory and the high-performance of the GRPA theory compatible, we adjust the chemical potential of the GRPA theory by using the SCF theory every constant time steps in the dynamic simulations. The performance of the GRPA and the hybrid theories is tested by using several systems composed of an A/B homopolymer, an AB diblock copolymer, or an ABC triblock copolymer. Using the hybrid theory, we succeeded in reproducing the metastable complex phase-separated domain structures of an ABC triblock copolymer observed by experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the Hybrid Dynamic Density Functional Theory proposed in the paper, and how does it address the limitations of the Dynamic Self-Consistent Field (SCF) theory and the Time-Dependent Ginzburg-Landau type theory with the Random Phase Approximation (GRPA) theory?\n\nA) The hybrid theory reduces the computational cost of the GRPA theory, making it more suitable for large-scale simulations.\nB) The hybrid theory combines the accuracy of the SCF theory with the high-performance of the GRPA theory, allowing for the simulation of phase separation processes in polymer melts and blends.\nC) The hybrid theory uses the SCF theory to adjust the chemical potential of the GRPA theory, enabling the simulation of metastable complex phase-separated domain structures.\nD) The hybrid theory is limited to the simulation of weak segregation regions, whereas the SCF theory is limited to strong segregation regions.\n\n**Correct Answer:** B) The hybrid theory combines the accuracy of the SCF theory with the high-performance of the GRPA theory, allowing for the simulation of phase separation processes in polymer melts and blends.\n\n**Explanation:** The correct answer is B) because the hybrid theory is designed to combine the strengths of both the SCF theory and the GRPA theory. The SCF theory is known for its accuracy in evaluating the free energy of polymer systems, but it requires a significant amount of computational cost. In contrast, the GRPA theory has a lower computational cost, but its applicability is limited to weak segregation regions. The hybrid theory addresses this limitation by adjusting the chemical potential of the GRPA theory using the SCF theory at regular time steps, allowing for the simulation of phase separation processes in both weak and strong segregation regions."}, "38": {"documentation": {"title": "Near room temperature X-ray and Gamma ray spectroscopic detectors for\n  future space experiments", "source": "J. S. Yadav, S. Savitri and J. P. Malkar", "docs_id": "astro-ph/0507156", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Near room temperature X-ray and Gamma ray spectroscopic detectors for\n  future space experiments. New generation Cadmium Telluride (CZT & CdTe) solid state detectors can provide high quantum efficiency with reasonably good energy resolution and can operate at near room temperature; an unique advantage for space experiments. We present here results of our study of small diode detectors as well as large area pixel detectors. Our study is aimed at developing near room temperature hard X-ray spectroscopy detectors for ASTROSAT and other future Indian space science missions.We have studied a Si-PIN detector in the energy range 5 - 60 keV and CZT & CdTe Schottky diode detectors in the energy region 10 - 1330 keV. We have studied large area (64 cm$^2$) CZT pixel detectors with pixel size close to the small CZT detector. We have studied individual pixels as well as CZT detector as a whole (summed over all the 1024 pixels). The energy resolution behaviour of the large area CZT detector is similar to that of small diode detectors in the low energy region. The change in operating temperature from room temperature ~20 to ~70$ degree C drastically affects both the energy resolutionas well as the peak area due to a sharp rise in the thermal noise.These results suggest that the cooling of detector system will provide better energy resolution as well as detection efficiency."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using Cadmium Telluride (CZT & CdTe) solid state detectors for space experiments, as mentioned in the study?\n\nA) High sensitivity to low-energy X-rays\nB) Ability to operate at high temperatures\nC) High quantum efficiency and reasonably good energy resolution, allowing for better detection efficiency at near room temperature\nD) Compact size and low power consumption\n\n**Correct Answer:** C) High quantum efficiency and reasonably good energy resolution, allowing for better detection efficiency at near room temperature\n\n**Explanation:** The study highlights the unique advantage of CZT & CdTe detectors in operating at near room temperature, which provides high quantum efficiency and reasonably good energy resolution. This is particularly beneficial for space experiments, where the ability to detect and resolve X-ray and gamma-ray signals is crucial. The correct answer, option C, reflects this key advantage.\n\n**Additional explanations:**\n\n* Option A is incorrect because while CZT & CdTe detectors can detect low-energy X-rays, this is not their primary advantage.\n* Option B is incorrect because CZT & CdTe detectors are actually designed to operate at near room temperature, not high temperatures.\n* Option D is incorrect because while CZT & CdTe detectors may have compact sizes and low power consumption, these are not their primary advantages for space experiments."}, "39": {"documentation": {"title": "Hybrid functional analysis of porous coordination polymers\n  Cu[Cu(pdt)$_{2}$] and Cu[Ni(pdt)$_{2}$]", "source": "S.V. Syrotyuk, Yu.V. Klysko", "docs_id": "2009.14693", "section": ["cond-mat.mtrl-sci", "cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hybrid functional analysis of porous coordination polymers\n  Cu[Cu(pdt)$_{2}$] and Cu[Ni(pdt)$_{2}$]. Ab initio investigation of the two porous coordination polymers Cu[Cu(pdt)$ _{2} $] and Cu[Ni(pdt)$ _{2} $] has been performed. The dispersion laws and partial density of states was obtained with the PBE0 hybrid functional. The results found here show that the materials under consideration are degenerate $p$-type semiconductors. Here, the effect of partial self-interaction removing of the strongly correlated 3$d$ electrons of Cu and Ni was examined. In case of Cu-containing materials, the obtained results confirm that the 3$d$ electrons of Cu reveal strong correlations, and, therefore, their electronic properties could be evaluated by means of a hybrid functional of the exchange-correlation energy. We also obtained quasiparticle properties within the Green's function (G0W0) and Bethe-Salpeter approaches. The last one was used in order to examine excitonic properties in the degenerate semiconductors. The imaginary part of the dielectric function was obtained within random-phase approximation as well as the Bethe-Salpeter approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the ab initio investigation of the porous coordination polymers Cu[Cu(pdt)2] and Cu[Ni(pdt)2] using the PBE0 hybrid functional?\n\nA) Both materials are degenerate p-type semiconductors with similar electronic properties.\nB) The 3d electrons of Cu and Ni exhibit weak correlations, making hybrid functional exchange-correlation energy evaluation unnecessary.\nC) The materials are degenerate p-type semiconductors, but the 3d electrons of Cu reveal strong correlations, necessitating a hybrid functional approach.\nD) The PBE0 hybrid functional is insufficient to accurately describe the electronic properties of the materials.\n\nCorrect Answer: C) The materials are degenerate p-type semiconductors, but the 3d electrons of Cu reveal strong correlations, necessitating a hybrid functional approach.\n\nExplanation: The correct answer is C) because the documentation states that the materials under consideration are degenerate p-type semiconductors, and the investigation using the PBE0 hybrid functional confirms this. Additionally, the study examines the effect of partial self-interaction removing of the strongly correlated 3d electrons of Cu and Ni, which reveals strong correlations in the 3d electrons of Cu. This necessitates the use of a hybrid functional approach to evaluate the electronic properties of the materials."}, "40": {"documentation": {"title": "A low CMB variance in the WMAP data", "source": "C. Monteserin, R.B. Barreiro, P. Vielva, E. Martinez-Gonzalez, M.P.\n  Hobson, A.N. Lasenby", "docs_id": "0706.4289", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A low CMB variance in the WMAP data. We have estimated the CMB variance from the three-year WMAP data, finding a value which is significantly lower than the one expected from Gaussian simulations using the WMAP best-fit cosmological model, at a significance level of 98.7 per cent. This result is even more prominent if we consider only the north ecliptic hemisphere (99.8 per cent). Different analyses have been performed in order to identify a possible origin for this anomaly. In particular we have studied the behaviour of single radiometers and single year data as well as the effect of residual foregrounds and 1/f noise, finding that none of these possibilities can explain the low value of the variance. We have also tested the effect of varying the cosmological parameters, finding that the estimated CMB variance tends to favour higher values of $n_s$ than the one of the WMAP best-fit model. In addition, we have also tested the consistency between the estimated CMB variance and the actual measured CMB power spectrum of the WMAP data, finding a strong discrepancy. A possible interpretation of this result could be a deviation from Gaussianity and/or isotropy of the CMB."}, "answer": "Here is the multi-form question:\n\nWhat could be a possible interpretation of the low CMB variance observed in the WMAP data, and how does it relate to the measured CMB power spectrum?\n\nA) The low variance is due to instrumental errors, which would also affect the measured power spectrum, leading to a discrepancy between the two.\n\nB) The low variance is a result of a deviation from Gaussianity and/or isotropy of the CMB, which would also be reflected in the measured power spectrum, potentially allowing for a more accurate cosmological model.\n\nC) The low variance is a result of a systematic error in the analysis, which would not affect the measured power spectrum, leading to a discrepancy between the two.\n\nD) The low variance is due to a foreground emission that is not accounted for in the analysis, which would not affect the measured power spectrum, leading to a discrepancy between the two.\n\nCorrect Answer: B) The low variance is a result of a deviation from Gaussianity and/or isotropy of the CMB, which would also be reflected in the measured power spectrum, potentially allowing for a more accurate cosmological model."}, "41": {"documentation": {"title": "Diffraction-Aware Sound Localization for a Non-Line-of-Sight Source", "source": "Inkyu An, Doheon Lee, Jung-woo Choi, Dinesh Manocha, and Sung-eui Yoon", "docs_id": "1809.07524", "section": ["cs.RO", "cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Diffraction-Aware Sound Localization for a Non-Line-of-Sight Source. We present a novel sound localization algorithm for a non-line-of-sight (NLOS) sound source in indoor environments. Our approach exploits the diffraction properties of sound waves as they bend around a barrier or an obstacle in the scene. We combine a ray tracing based sound propagation algorithm with a Uniform Theory of Diffraction (UTD) model, which simulate bending effects by placing a virtual sound source on a wedge in the environment. We precompute the wedges of a reconstructed mesh of an indoor scene and use them to generate diffraction acoustic rays to localize the 3D position of the source. Our method identifies the convergence region of those generated acoustic rays as the estimated source position based on a particle filter. We have evaluated our algorithm in multiple scenarios consisting of a static and dynamic NLOS sound source. In our tested cases, our approach can localize a source position with an average accuracy error, 0.7m, measured by the L2 distance between estimated and actual source locations in a 7m*7m*3m room. Furthermore, we observe 37% to 130% improvement in accuracy over a state-of-the-art localization method that does not model diffraction effects, especially when a sound source is not visible to the robot."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed sound localization algorithm in indoor environments, and how does it differ from a state-of-the-art localization method that does not model diffraction effects?\n\n**A)** The algorithm uses a particle filter to estimate the source position, which is more accurate than traditional methods. However, it does not account for diffraction effects, leading to reduced accuracy in NLOS scenarios.\n\n**B)** The algorithm combines ray tracing and Uniform Theory of Diffraction (UTD) to simulate bending effects, resulting in improved accuracy in NLOS scenarios, especially when the sound source is not visible to the robot.\n\n**C)** The algorithm uses a reconstructed mesh of the indoor scene to precompute wedges, which are then used to generate diffraction acoustic rays to localize the source position. However, this approach does not account for dynamic changes in the environment.\n\n**D)** The algorithm uses a particle filter to estimate the source position, but it relies solely on the ray tracing algorithm, which is not effective in NLOS scenarios.\n\n**Correct Answer:** B) The algorithm combines ray tracing and Uniform Theory of Diffraction (UTD) to simulate bending effects, resulting in improved accuracy in NLOS scenarios, especially when the sound source is not visible to the robot.\n\n**Explanation:** The correct answer highlights the key advantage of the proposed algorithm, which is its ability to model diffraction effects using the UTD model. This allows the algorithm to improve accuracy in NLOS scenarios, especially when the sound source is not visible to the robot. The other options are incorrect because they either downplay the importance of diffraction effects or misrepresent the algorithm's approach."}, "42": {"documentation": {"title": "Targeted Random Projection for Prediction from High-Dimensional Features", "source": "Minerva Mukhopadhyay and David B. Dunson", "docs_id": "1712.02445", "section": ["math.ST", "stat.ME", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Targeted Random Projection for Prediction from High-Dimensional Features. We consider the problem of computationally-efficient prediction from high dimensional and highly correlated predictors in challenging settings where accurate variable selection is effectively impossible. Direct application of penalization or Bayesian methods implemented with Markov chain Monte Carlo can be computationally daunting and unstable. Hence, some type of dimensionality reduction prior to statistical analysis is in order. Common solutions include application of screening algorithms to reduce the regressors, or dimension reduction using projections of the design matrix. The former approach can be highly sensitive to threshold choice in finite samples, while the later can have poor performance in very high-dimensional settings. We propose a TArgeted Random Projection (TARP) approach that combines positive aspects of both strategies to boost performance. In particular, we propose to use information from independent screening to order the inclusion probabilities of the features in the projection matrix used for dimension reduction, leading to data-informed sparsity. We provide theoretical support for a Bayesian predictive algorithm based on TARP, including both statistical and computational complexity guarantees. Examples for simulated and real data applications illustrate gains relative to a variety of competitors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Targeted Random Projection (TARP) approach, what is the primary advantage of using information from independent screening to order the inclusion probabilities of the features in the projection matrix used for dimension reduction?\n\n**A)** TARP reduces the computational complexity of the Bayesian predictive algorithm.\n**B)** TARP improves the stability of the dimensionality reduction process in high-dimensional settings.\n**C)** TARP enables data-informed sparsity, allowing for more accurate variable selection.\n**D)** TARP is more computationally efficient than traditional screening algorithms.\n\n**Correct Answer:** C) TARP enables data-informed sparsity, allowing for more accurate variable selection.\n\n**Explanation:** The correct answer is C) TARP enables data-informed sparsity, allowing for more accurate variable selection. The TARP approach combines the strengths of screening algorithms and dimension reduction using projections, and the use of information from independent screening allows for data-informed sparsity, which is a key advantage of the method. This is stated in the documentation as \"leading to data-informed sparsity\". The other options are incorrect because they do not accurately reflect the primary advantage of the TARP approach."}, "43": {"documentation": {"title": "The CARMENES search for exoplanets around M dwarfs: The warm\n  super-Earths in twin orbits around the mid-type M dwarfs Ross 1020 (GJ 3779)\n  and LP 819-052 (GJ 1265)", "source": "R. Luque, G. Nowak, E. Pall\\'e, D. Kossakowski, T. Trifonov, M.\n  Zechmeister, V. J. S. B\\'ejar, C. Cardona Guill\\'en, L. Tal-Or, D. Hidalgo,\n  I. Ribas, A. Reiners, J. A. Caballero, P. J. Amado, A. Quirrenbach, J.\n  Aceituno, M. Cort\\'es-Contreras, E. D\\'iez-Alonso, S. Dreizler, E. W.\n  Guenther, T. Henning, S. V. Jeffers, A. Kaminski, M. K\\\"urster, M. Lafarga,\n  D. Montes, J. C. Morales, V. M. Passegger, J. H. M. M. Schmitt, A. Schweitzer", "docs_id": "1810.07572", "section": ["astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The CARMENES search for exoplanets around M dwarfs: The warm\n  super-Earths in twin orbits around the mid-type M dwarfs Ross 1020 (GJ 3779)\n  and LP 819-052 (GJ 1265). We announce the discovery of two planetary companions orbiting around the low mass stars Ross 1020 (GJ 3779, M4.0V) and LP 819-052 (GJ 1265, M4.5V). The discovery is based on the analysis of CARMENES radial velocity observations in the visual channel as part of its survey for exoplanets around M dwarfs. In the case of GJ 1265, CARMENES observations were complemented with publicly available Doppler measurements from HARPS. The datasets reveal one planetary companion for each star that share very similar properties: minimum masses of $8.0\\pm0.5$ M$_{\\oplus}$ and $7.4\\pm0.5$ M$_{\\oplus}$ in low-eccentricity orbits with periods of $3.023\\pm0.001$ d and $3.651\\pm0.001$ d for GJ 3779 b and GJ 1265 b, respectively. The periodic signals around three days found in the radial velocity data have no counterpart in any spectral activity indicator. Besides, we collected available photometric data for the two host stars, which confirm that the additional Doppler variations found at periods around 95 d can be attributed to the rotation of the stars. The addition of these planets in a mass-period diagram of known planets around M dwarfs suggests a bimodal distribution with a lack of short-period low-mass planets in the range of 2-5 M$_{\\oplus}$. It also indicates that super-Earths (> 5 M$_{\\oplus}$) currently detected by radial velocity and transit techniques around M stars are usually found in systems dominated by a single planet."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The CARMENES survey has discovered two planetary companions, GJ 3779 b and GJ 1265 b, orbiting around low-mass M dwarfs. Analyze the implications of these discoveries on our understanding of the distribution of exoplanets around M dwarfs.\n\nA) The discovery of these super-Earths suggests that M dwarfs are more likely to host multiple planets, challenging the traditional view of planetary systems around these stars.\n\nB) The fact that these planets have low eccentricities and are found in low-mass M dwarfs indicates that the formation of super-Earths is more common in these systems than previously thought.\n\nC) The bimodal distribution of planets around M dwarfs, with a lack of short-period low-mass planets, suggests that the formation of planets is a complex process that depends on various factors, including the star's mass and metallicity.\n\nD) The discovery of these super-Earths in twin orbits around M dwarfs, Ross 1020 and LP 819-052, indicates that the formation of planetary systems is a highly dependent process, with the presence of one planet influencing the formation of the other.\n\n**Correct Answer:** C) The bimodal distribution of planets around M dwarfs, with a lack of short-period low-mass planets, suggests that the formation of planets is a complex process that depends on various factors, including the star's mass and metallicity.\n\n**Explanation:** The correct answer is based on the fact that the CARMENES survey has discovered two planetary companions, GJ 3779 b and GJ 1265 b, which share very similar properties, including low eccentricities and periods of around 3 days. The discovery of these super-Earths in twin orbits around M dwarfs, Ross 1020 and LP 819-052, suggests that the formation of planetary systems is a complex process that depends on various factors, including the star's mass and metallicity. The bimodal distribution of planets around M dwarfs, with a lack of short-period low-mass planets, indicates that the formation of planets is a complex process that depends on various factors, including the star's mass and metallicity."}, "44": {"documentation": {"title": "Exact dimer phase with anisotropic interaction for one dimensional\n  magnets", "source": "Hong-Ze Xu, Shun-Yao Zhang, Guang-Can Guo, Ming Gong", "docs_id": "1806.05814", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exact dimer phase with anisotropic interaction for one dimensional\n  magnets. We report the exact dimer phase, in which the ground states are described by product of singlet dimer, in the extended XYZ model by generalizing the isotropic Majumdar-Ghosh model to the fully anisotropic region. We demonstrate that this phase can be realized even in models when antiferromagnetic interaction along one of the three directions. This model also supports three different ferromagnetic (FM) phases, denoted as $x$-FM, $y$-FM and $z$-FM, polarized along the three directions. The boundaries between the exact dimer phase and FM phases are infinite-fold degenerate. The breaking of this infinite-fold degeneracy by either translational symmetry breaking or $\\mathbb{Z}_2$ symmetry breaking leads to exact dimer phase and FM phases, respectively. Moreover, the boundaries between the three FM phases are critical with central charge $c=1$ for free fermions. We characterize the properties of these boundaries using entanglement entropy, excitation gap, and long-range spin-spin correlation functions. These results are relevant to a large number of one dimensional magnets, in which anisotropy is necessary to isolate a single chain out from the bulk material. We discuss the possible experimental signatures in realistic materials with magnetic field along different directions and show that the anisotropy may resolve the disagreement between theory and experiments based on isotropic spin-spin interactions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the critical central charge associated with the boundaries between the three ferromagnetic (FM) phases in the extended XYZ model, and how does it relate to the breaking of infinite-fold degeneracy?\n\nA) c = 0, due to the absence of translational symmetry breaking\nB) c = 1, due to the presence of $\\mathbb{Z}_2$ symmetry breaking\nC) c = 2, due to the presence of both translational symmetry breaking and $\\mathbb{Z}_2$ symmetry breaking\nD) c = 3, due to the presence of all possible symmetry breakings\n\nCorrect Answer: B) c = 1, due to the presence of $\\mathbb{Z}_2$ symmetry breaking\n\nExplanation: The text states that the boundaries between the three FM phases are critical with central charge c = 1 for free fermions. This implies that the breaking of infinite-fold degeneracy by $\\mathbb{Z}_2$ symmetry breaking leads to the emergence of the exact dimer phase. The correct answer, c = 1, reflects this relationship. The other options are incorrect because they do not accurately describe the relationship between the central charge and the symmetry breakings."}, "45": {"documentation": {"title": "DeCaf: Diagnosing and Triaging Performance Issues in Large-Scale Cloud\n  Services", "source": "Chetan Bansal, Sundararajan Renganathan, Ashima Asudani, Olivier Midy,\n  Mathru Janakiraman", "docs_id": "1910.05339", "section": ["cs.DC", "cs.SE", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "DeCaf: Diagnosing and Triaging Performance Issues in Large-Scale Cloud\n  Services. Large scale cloud services use Key Performance Indicators (KPIs) for tracking and monitoring performance. They usually have Service Level Objectives (SLOs) baked into the customer agreements which are tied to these KPIs. Dependency failures, code bugs, infrastructure failures, and other problems can cause performance regressions. It is critical to minimize the time and manual effort in diagnosing and triaging such issues to reduce customer impact. Large volume of logs and mixed type of attributes (categorical, continuous) in the logs makes diagnosis of regressions non-trivial. In this paper, we present the design, implementation and experience from building and deploying DeCaf, a system for automated diagnosis and triaging of KPI issues using service logs. It uses machine learning along with pattern mining to help service owners automatically root cause and triage performance issues. We present the learnings and results from case studies on two large scale cloud services in Microsoft where DeCaf successfully diagnosed 10 known and 31 unknown issues. DeCaf also automatically triages the identified issues by leveraging historical data. Our key insights are that for any such diagnosis tool to be effective in practice, it should a) scale to large volumes of service logs and attributes, b) support different types of KPIs and ranking functions, c) be integrated into the DevOps processes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A large-scale cloud service provider is experiencing frequent performance regressions due to dependency failures, code bugs, and infrastructure failures. The service owner wants to implement a system to automate the diagnosis and triaging of these issues. What are the three essential features that such a system should have to be effective in practice, according to the authors of the DeCaf paper?\n\n**A)** Scalability to large volumes of logs and attributes, support for different types of KPIs, and integration with DevOps processes.\n**B)** Support for multiple machine learning algorithms, scalability to large volumes of logs and attributes, and integration with customer agreements.\n**C)** Support for categorical and continuous attributes, scalability to large volumes of logs and attributes, and integration with DevOps processes.\n**D)** Support for different ranking functions, scalability to large volumes of logs and attributes, and integration with customer agreements.\n\n**Correct Answer:** A) Scalability to large volumes of logs and attributes, support for different types of KPIs, and integration with DevOps processes.\n\n**Explanation:** According to the DeCaf paper, the three essential features that a diagnosis tool should have to be effective in practice are:\n\n1. **Scalability to large volumes of logs and attributes**: The system should be able to handle the large volume of logs and mixed-type attributes in the service logs.\n2. **Support for different types of KPIs**: The system should be able to support different types of Key Performance Indicators (KPIs) used by the cloud service provider.\n3. **Integration with DevOps processes**: The system should be integrated into the DevOps processes to enable seamless diagnosis and triaging of performance issues.\n\nThese three features are critical to ensure that the diagnosis tool is effective in practice and can minimize the time and manual effort required to diagnose and triage performance issues."}, "46": {"documentation": {"title": "Batched Large-scale Bayesian Optimization in High-dimensional Spaces", "source": "Zi Wang and Clement Gehring and Pushmeet Kohli and Stefanie Jegelka", "docs_id": "1706.01445", "section": ["stat.ML", "cs.LG", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Batched Large-scale Bayesian Optimization in High-dimensional Spaces. Bayesian optimization (BO) has become an effective approach for black-box function optimization problems when function evaluations are expensive and the optimum can be achieved within a relatively small number of queries. However, many cases, such as the ones with high-dimensional inputs, may require a much larger number of observations for optimization. Despite an abundance of observations thanks to parallel experiments, current BO techniques have been limited to merely a few thousand observations. In this paper, we propose ensemble Bayesian optimization (EBO) to address three current challenges in BO simultaneously: (1) large-scale observations; (2) high dimensional input spaces; and (3) selections of batch queries that balance quality and diversity. The key idea of EBO is to operate on an ensemble of additive Gaussian process models, each of which possesses a randomized strategy to divide and conquer. We show unprecedented, previously impossible results of scaling up BO to tens of thousands of observations within minutes of computation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge that Bayesian optimization (BO) techniques face when dealing with high-dimensional input spaces, and how does the proposed Ensemble Bayesian Optimization (EBO) approach address this challenge?\n\n**A)** EBO addresses the challenge of high-dimensional input spaces by using a single Gaussian process model that can handle complex relationships between input variables. However, this approach is limited by the need for a large number of observations to achieve good optimization results.\n\n**B)** EBO addresses the challenge of high-dimensional input spaces by operating on an ensemble of additive Gaussian process models, each of which possesses a randomized strategy to divide and conquer the input space. This approach allows for more efficient exploration of the input space and can achieve better optimization results with fewer observations.\n\n**C)** EBO addresses the challenge of high-dimensional input spaces by using a hierarchical approach, where each level of the hierarchy represents a smaller subset of the input space. This approach allows for more efficient exploration of the input space and can achieve better optimization results with fewer observations.\n\n**D)** EBO addresses the challenge of high-dimensional input spaces by using a parallel computing approach, where multiple Gaussian process models are trained in parallel to speed up the optimization process. However, this approach may not be effective in high-dimensional spaces where the number of observations required for good optimization results is very large.\n\n**Correct Answer:** B) EBO addresses the challenge of high-dimensional input spaces by operating on an ensemble of additive Gaussian process models, each of which possesses a randomized strategy to divide and conquer the input space. This approach allows for more efficient exploration of the input space and can achieve better optimization results with fewer observations.\n\n**Explanation:** The correct answer is B) because the documentation states that EBO addresses the challenge of high-dimensional input spaces by operating on an ensemble of additive Gaussian process models, each of which possesses a randomized strategy to divide and conquer the input space. This approach allows for more efficient exploration of the input space and can achieve better optimization results with fewer observations. The other options are incorrect because they do not accurately describe the approach proposed in the documentation."}, "47": {"documentation": {"title": "The theory of direct laser excitation of nuclear transitions", "source": "Lars von der Wense, Pavlo V. Bilous, Benedict Seiferle, Simon\n  Stellmer, Johannes Weitenberg, Peter G. Thirolf, Adriana P\\'alffy, Georgy\n  Kazakov", "docs_id": "2001.08320", "section": ["nucl-th", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The theory of direct laser excitation of nuclear transitions. A comprehensive theoretical study of direct laser excitation of a nuclear state based on the density matrix formalism is presented. The nuclear clock isomer $^{229\\text{m}}$Th is discussed in detail, as it could allow for direct laser excitation using existing technology and provides the motivation for this work. The optical Bloch equations are derived for the simplest case of a pure nuclear two-level system and for the more complex cases taking into account the presence of magnetic sub-states, hyperfine-structure and Zeeman splitting in external fields. Nuclear level splitting for free atoms and ions as well as for nuclei in a solid-state environment is discussed individually. Based on the obtained equations, nuclear population transfer in the low-saturation limit is reviewed. Further, nuclear Rabi oscillations, power broadening and nuclear two-photon excitation are considered. Finally, the theory is applied to the special cases of $^{229\\text{m}}$Th and $^{235\\text{m}}$U, being the nuclear excited states of lowest known excitation energies. The paper aims to be a didactic review with many calculations given explicitly."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation for the study of direct laser excitation of nuclear transitions, as presented in the given Arxiv documentation?\n\nA) To explore the possibility of using nuclear clocks for quantum computing applications\nB) To investigate the effects of laser excitation on nuclear magnetic resonance in solids\nC) To develop a theoretical framework for the direct laser excitation of nuclear transitions, with a focus on the nuclear clock isomer $^{229\\text{m}}$Th\nD) To examine the potential applications of nuclear two-photon excitation in spectroscopy and imaging\n\n**Correct Answer:** C) To develop a theoretical framework for the direct laser excitation of nuclear transitions, with a focus on the nuclear clock isomer $^{229\\text{m}}$Th\n\n**Explanation:** The correct answer is C) because the documentation explicitly states that the paper aims to present a \"comprehensive theoretical study of direct laser excitation of a nuclear state\" and that the nuclear clock isomer $^{229\\text{m}}$Th is discussed in detail as a potential candidate for direct laser excitation using existing technology. The other options are not supported by the documentation and are therefore incorrect."}, "48": {"documentation": {"title": "On the Secrecy Rate of Spatial Modulation Based Indoor Visible Light\n  Communications", "source": "Jin-Yuan Wang, Hong Ge, Min Lin, Jun-Bo Wang, Jianxin Dai, and\n  Mohamed-Slim Alouini", "docs_id": "1906.09512", "section": ["cs.IT", "cs.PF", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Secrecy Rate of Spatial Modulation Based Indoor Visible Light\n  Communications. In this paper, we investigate the physical-layer security for a spatial modulation (SM) based indoor visible light communication (VLC) system, which includes multiple transmitters, a legitimate receiver, and a passive eavesdropper (Eve). At the transmitters, the SM scheme is employed, i.e., only one transmitter is active at each time instant. To choose the active transmitter, a uniform selection (US) scheme is utilized. Two scenarios are considered: one is with non-negativity and average optical intensity constraints, the other is with non-negativity, average optical intensity and peak optical intensity constraints. Then, lower and upper bounds on the secrecy rate are derived for these two scenarios. Besides, the asymptotic behaviors for the derived secrecy rate bounds at high signal-to-noise ratio (SNR) are analyzed. To further improve the secrecy performance, a channel adaptive selection (CAS) scheme and a greedy selection (GS) scheme are proposed to select the active transmitter. Numerical results show that the lower and upper bounds of the secrecy rate are tight. At high SNR, small asymptotic performance gaps exist between the derived lower and upper bounds. Moreover, the proposed GS scheme has the best performance, followed by the CAS scheme and the US scheme."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of spatial modulation based indoor visible light communication (VLC) systems, what is the primary advantage of employing a channel adaptive selection (CAS) scheme over a greedy selection (GS) scheme, and how does it compare to the uniform selection (US) scheme in terms of secrecy rate performance?\n\n**A)** The CAS scheme provides better secrecy rate performance than the GS scheme due to its ability to adapt to changing channel conditions, while the GS scheme relies on a fixed selection strategy that may not account for variations in channel quality.\n\n**B)** The CAS scheme has a similar secrecy rate performance to the GS scheme, but with a lower computational complexity, making it more suitable for real-time applications, whereas the US scheme is limited by its fixed selection strategy.\n\n**C)** The CAS scheme offers better secrecy rate performance than the US scheme due to its ability to adapt to channel conditions, but the GS scheme has a higher computational complexity, making it less suitable for real-time applications.\n\n**D)** The CAS scheme and GS scheme have similar secrecy rate performance, but the US scheme provides better performance in terms of simplicity and ease of implementation, making it a more practical choice for VLC systems.\n\n**Correct Answer:** A) The CAS scheme provides better secrecy rate performance than the GS scheme due to its ability to adapt to changing channel conditions, while the GS scheme relies on a fixed selection strategy that may not account for variations in channel quality.\n\n**Explanation:** The correct answer is A) because the CAS scheme is designed to adapt to changing channel conditions, which can improve secrecy rate performance. In contrast, the GS scheme relies on a fixed selection strategy that may not account for variations in channel quality. The CAS scheme is able to select the active transmitter based on the current channel conditions, which can lead to better secrecy rate performance. The other options are incorrect because they do not accurately reflect the advantages of the CAS scheme over the GS scheme."}, "49": {"documentation": {"title": "Economic resilience from input-output susceptibility improves\n  predictions of economic growth and recovery", "source": "Peter Klimek, Sebastian Poledna, Stefan Thurner", "docs_id": "1903.03203", "section": ["econ.GN", "physics.soc-ph", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Economic resilience from input-output susceptibility improves\n  predictions of economic growth and recovery. Modern macroeconomic theories were unable to foresee the last Great Recession and could neither predict its prolonged duration nor the recovery rate. They are based on supply-demand equilibria that do not exist during recessionary shocks. Here we focus on resilience as a nonequilibrium property of networked production systems and develop a linear response theory for input-output economics. By calibrating the framework to data from 56 industrial sectors in 43 countries between 2000 and 2014, we find that the susceptibility of individual industrial sectors to economic shocks varies greatly across countries, sectors, and time. We show that susceptibility-based predictions that take sector- and country-specific recovery into account, outperform--by far--standard econometric growth-models. Our results are analytically rigorous, empirically testable, and flexible enough to address policy-relevant scenarios. We illustrate the latter by estimating the impact of recently imposed tariffs on US imports (steel and aluminum) on specific sectors across European countries."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study \"Economic resilience from input-output susceptibility improves predictions of economic growth and recovery\" argue that modern macroeconomic theories are unable to forecast economic growth and recovery due to their reliance on supply-demand equilibria that do not exist during recessionary shocks. However, the authors propose a new framework that incorporates the concept of resilience as a nonequilibrium property of networked production systems. What is the primary limitation of modern macroeconomic theories, according to the authors, and how does the proposed framework address this limitation?\n\n**A)** Modern macroeconomic theories are unable to forecast economic growth and recovery because they rely too heavily on historical data, leading to a lack of understanding of the underlying economic mechanisms. The proposed framework addresses this limitation by incorporating networked production systems, which allow for a more nuanced understanding of the complex interactions between economic agents.\n\n**B)** Modern macroeconomic theories are unable to forecast economic growth and recovery because they fail to account for the impact of external shocks on the economy. The proposed framework addresses this limitation by incorporating the concept of susceptibility, which allows for the prediction of economic growth and recovery in the face of external shocks.\n\n**C)** Modern macroeconomic theories are unable to forecast economic growth and recovery because they are based on unrealistic assumptions about the behavior of economic agents. The proposed framework addresses this limitation by incorporating a more realistic understanding of the behavior of economic agents, which is reflected in the concept of resilience.\n\n**D)** Modern macroeconomic theories are unable to forecast economic growth and recovery because they are too focused on aggregate variables, neglecting the importance of individual sectors and countries. The proposed framework addresses this limitation by incorporating sector- and country-specific data, which allows for a more accurate prediction of economic growth and recovery.\n\n**Correct Answer:** B) Modern macroeconomic theories are unable to forecast economic growth and recovery because they fail to account for the impact of external shocks on the economy. The proposed framework addresses this limitation by incorporating the concept of susceptibility, which allows for the prediction of economic growth and recovery in the face of external shocks.\n\n**Explanation:** The correct answer is B) because the authors explicitly state that modern macroeconomic theories are unable to foresee the last Great Recession and could neither predict its prolonged duration nor the recovery rate. They also argue that these theories are based on supply-demand equilibria that do not exist during recessionary shocks. The proposed framework addresses this limitation by incorporating the concept of susceptibility, which allows for the prediction of economic growth and recovery in the face of external shocks."}, "50": {"documentation": {"title": "Compressed Sensing of Scanning Transmission Electron Microscopy (STEM)\n  on Non-Rectangular Scans", "source": "Xin Li, Ondrej Dyck, Sergei V. Kalinin, Stephen Jesse", "docs_id": "1805.04957", "section": ["physics.ins-det", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Compressed Sensing of Scanning Transmission Electron Microscopy (STEM)\n  on Non-Rectangular Scans. Scanning Transmission Electron Microscopy (STEM) has become the main stay for materials characterization on atomic level, with applications ranging from visualization of localized and extended defects to mapping order parameter fields. In the last several years, attention was attracted by potential of STEM to explore beam induced chemical processes and especially manipulating atomic motion, enabling atom-by-atom fabrication. These applications, as well as traditional imaging of beam sensitive materials, necessitate increasing dynamic range of STEM between imaging and manipulation modes, and increasing absolute scanning/imaging speeds, that can be achieved by combining sparse sensing methods with non-rectangular scanning trajectories. Here we developed a general method for real-time reconstruction of sparsely sampled images from high-speed, non-invasive and diverse scanning pathways. This approach is demonstrated on both the synthetic data where ground truth is known and the experimental STEM data. This work lays the foundation for future tasks such as optimal design of dose efficient scanning strategies and real-time adaptive inference and control of e-beam induced atomic fabrication."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a general method for real-time reconstruction of sparsely sampled images from high-speed, non-invasive, and diverse scanning pathways in Scanning Transmission Electron Microscopy (STEM)?\n\n**A)** To improve the resolution of STEM images for traditional imaging of beam-sensitive materials.\n**B)** To enable real-time adaptive inference and control of e-beam induced atomic fabrication.\n**C)** To increase the dynamic range of STEM between imaging and manipulation modes, and to achieve higher absolute scanning/imaging speeds.\n**D)** To develop a method for reconstructing images from sparse data for materials characterization on an atomic level.\n\n**Correct Answer:** C) To increase the dynamic range of STEM between imaging and manipulation modes, and to achieve higher absolute scanning/imaging speeds.\n\n**Explanation:** The question requires the test-taker to understand the context and motivations behind the development of the method. The correct answer, C, is supported by the text, which states that the method is developed to \"increase the dynamic range of STEM between imaging and manipulation modes, and increasing absolute scanning/imaging speeds\". This is a key motivation for the development of the method, as it enables STEM to explore beam-induced chemical processes and manipulate atomic motion. The other options are incorrect because they do not accurately reflect the primary motivation behind the development of the method. Option A is related to traditional imaging, option B is a potential application of the method, and option D is a broader goal of STEM, but not the specific motivation behind this method."}, "51": {"documentation": {"title": "A Riemannian low-rank method for optimization over semidefinite matrices\n  with block-diagonal constraints", "source": "Nicolas Boumal", "docs_id": "1506.00575", "section": ["math.OC", "cs.CV", "stat.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Riemannian low-rank method for optimization over semidefinite matrices\n  with block-diagonal constraints. We propose a new algorithm to solve optimization problems of the form $\\min f(X)$ for a smooth function $f$ under the constraints that $X$ is positive semidefinite and the diagonal blocks of $X$ are small identity matrices. Such problems often arise as the result of relaxing a rank constraint (lifting). In particular, many estimation tasks involving phases, rotations, orthonormal bases or permutations fit in this framework, and so do certain relaxations of combinatorial problems such as Max-Cut. The proposed algorithm exploits the facts that (1) such formulations admit low-rank solutions, and (2) their rank-restricted versions are smooth optimization problems on a Riemannian manifold. Combining insights from both the Riemannian and the convex geometries of the problem, we characterize when second-order critical points of the smooth problem reveal KKT points of the semidefinite problem. We compare against state of the art, mature software and find that, on certain interesting problem instances, what we call the staircase method is orders of magnitude faster, is more accurate and scales better. Code is available."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider an optimization problem of the form $\\min f(X)$, where $X$ is a $n \\times n$ semidefinite matrix with block-diagonal structure, and the diagonal blocks are small identity matrices. The objective function $f(X)$ is smooth and convex. What is the key insight that the proposed algorithm exploits, which allows it to solve this problem efficiently?\n\nA) The fact that the semidefinite problem has a low-rank solution, which can be approximated using a low-rank approximation of the matrix $X$.\nB) The fact that the smooth problem on the Riemannian manifold has a second-order critical point that corresponds to a KKT point of the semidefinite problem.\nC) The fact that the semidefinite problem can be solved using a convex optimization algorithm, such as the interior-point method.\nD) The fact that the block-diagonal structure of $X$ allows for a decomposition of the optimization problem into smaller sub-problems.\n\nCorrect Answer: B) The fact that the smooth problem on the Riemannian manifold has a second-order critical point that corresponds to a KKT point of the semidefinite problem.\n\nExplanation: The proposed algorithm exploits the fact that the smooth problem on the Riemannian manifold has a second-order critical point that corresponds to a KKT point of the semidefinite problem. This is because the smooth problem has a well-defined Riemannian metric, and the second-order critical points of the smooth problem correspond to the KKT points of the semidefinite problem. This insight allows the algorithm to solve the optimization problem efficiently by leveraging the properties of the Riemannian manifold and the smooth problem."}, "52": {"documentation": {"title": "Statistical and mathematical modeling of spatiotemporal dynamics of stem\n  cells", "source": "Walter de Back, Thomas Zerjatke and Ingo Roeder", "docs_id": "1809.01708", "section": ["q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistical and mathematical modeling of spatiotemporal dynamics of stem\n  cells. Statistical and mathematical modeling are crucial to describe, interpret, compare and predict the behavior of complex biological systems including the organization of hematopoietic stem and progenitor cells in the bone marrow environment. The current prominence of high-resolution and live-cell imaging data provides an unprecedented opportunity to study the spatiotemporal dynamics of these cells within their stem cell niche and learn more about aberrant, but also unperturbed, normal hematopoiesis. However, this requires careful quantitative statistical analysis of the spatial and temporal behavior of cells and the interaction with their microenvironment. Moreover, such quantification is a prerequisite for the construction of hypothesis-driven mathematical models that can provide mechanistic explanations by generating spatiotemporal dynamics that can be directly compared to experimental observations. Here, we provide a brief overview of statistical methods in analyzing spatial distribution of cells, cell motility, cell shapes and cellular genealogies. We also describe cell- based modeling formalisms that allow researchers to simulate emergent behavior in a multicellular system based on a set of hypothesized mechanisms. Together, these methods provide a quantitative workflow for the analytic and synthetic study of the spatiotemporal behavior of hematopoietic stem and progenitor cells."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of statistical and mathematical modeling in the study of spatiotemporal dynamics of stem cells, particularly in the context of hematopoietic stem and progenitor cells in the bone marrow environment?\n\n**A)** To develop new therapeutic strategies for cancer treatment\n**B)** To understand the mechanisms of aberrant hematopoiesis and identify potential biomarkers for disease diagnosis\n**C)** To construct hypothesis-driven mathematical models that can provide mechanistic explanations for the spatiotemporal behavior of cells and their interaction with the microenvironment\n**D)** To analyze the spatial distribution of cells and cell motility using high-resolution and live-cell imaging data\n\n**Correct Answer:** C) To construct hypothesis-driven mathematical models that can provide mechanistic explanations for the spatiotemporal behavior of cells and their interaction with the microenvironment\n\n**Explanation:** The correct answer is C) because the primary goal of statistical and mathematical modeling in this context is to develop hypothesis-driven models that can explain the complex behavior of stem cells and their interaction with the microenvironment. This is stated in the introduction of the paper: \"Statistical and mathematical modeling are crucial to describe, interpret, compare and predict the behavior of complex biological systems...\". The other options, while related to the field of stem cell research, are not the primary goal of statistical and mathematical modeling in this context. Option A is a potential application of the research, but not the primary goal. Option B is a potential outcome of the research, but not the primary goal. Option D is a method used to analyze data, but not the primary goal of statistical and mathematical modeling."}, "53": {"documentation": {"title": "Invariant holonomic systems on symmetric spaces and other polar\n  representations", "source": "G. Bellamy, T. Nevins and J. T. Stafford", "docs_id": "2109.11387", "section": ["math.RT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Invariant holonomic systems on symmetric spaces and other polar\n  representations. Let $V$ be a symmetric space over a connected reductive Lie algebra $G$, with Lie algebra $\\mathfrak{g}$ and discriminant $\\delta\\in \\mathbb{C}[V]$. A fundamental object is the \\emph{invariant holonomic system} $ \\mathcal{G} =\\mathcal{D}(V)\\Big/ \\Bigl(\\mathcal{D}(V)\\mathfrak{g}+ \\mathcal{D}(V)(\\mathrm{Sym}\\, V)^G_+ \\Bigr) $ over the ring of differential operators $\\mathcal{D}(V)$. Jointly with Levasseur we have shown that there exists a surjective radial parts map $\\mathrm{rad}$ from $ \\mathcal{D}(V)^G$ to the spherical subalgebra $A_{\\kappa}$ of a Cherednik algebra. When $A_{\\kappa}$ is simple we show that $\\mathcal{G}$ has no $\\delta$-torsion submodule nor factor module and we determine when $\\mathcal{G}$ is semisimple, thereby answering questions of Sekiguchi, respectively Levasseur-Stafford. In the diagonal case when $V=\\mathfrak{g}$, these results reduce to fundamental theorems of Harish-Chandra and Hotta-Kashiwara. We generalise these results to polar representations $V$ satisfying natural conditions. By twisting the radial parts map, we obtain families of invariant holonomic systems. We introduce shift functors between the different twists. We show that the image of the simple summands of $\\mathcal{G} $ under these functors is described by Opdam's KZ-twist."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Let $V$ be a symmetric space over a connected reductive Lie algebra $G$, with Lie algebra $\\mathfrak{g}$ and discriminant $\\delta\\in \\mathbb{C}[V]$. Consider the invariant holonomic system $\\mathcal{G} =\\mathcal{D}(V)\\Big/ \\Bigl(\\mathcal{D}(V)\\mathfrak{g}+ \\mathcal{D}(V)(\\mathrm{Sym}\\, V)^G_+ \\Bigr)$. Suppose that $A_{\\kappa}$ is a simple spherical subalgebra of a Cherednik algebra. Determine the conditions under which $\\mathcal{G}$ is semisimple, and explain why this result is significant in the context of polar representations.\n\n**A)** $\\mathcal{G}$ is semisimple if and only if $A_{\\kappa}$ is simple and $\\delta$ is not a zero divisor in $\\mathbb{C}[V]$.\n**B)** $\\mathcal{G}$ is semisimple if and only if $A_{\\kappa}$ is simple and $\\delta$ is a zero divisor in $\\mathbb{C}[V]$.\n**C)** $\\mathcal{G}$ is semisimple if and only if $A_{\\kappa}$ is not simple and $\\delta$ is a zero divisor in $\\mathbb{C}[V]$.\n**D)** $\\mathcal{G}$ is semisimple if and only if $A_{\\kappa}$ is simple and $\\delta$ is a zero divisor in $\\mathbb{C}[V]$.\n\n**Correct Answer:** A) $\\mathcal{G}$ is semisimple if and only if $A_{\\kappa}$ is simple and $\\delta$ is not a zero divisor in $\\mathbb{C}[V]$.\n\n**Explanation:**\n\nThe correct answer is A) because the documentation states that when $A_{\\kappa}$ is simple, $\\mathcal{G}$ has no $\\delta$-torsion submodule nor factor module, and therefore $\\mathcal{G}$ is semisimple. Additionally, the documentation mentions that the results reduce to fundamental theorems of Harish-Chandra and Hotta-Kashiwara in the diagonal case when $V=\\mathfrak{g}$, which implies that $\\delta$ must not be a zero divisor in $\\mathbb{C}[V]$ for $\\mathcal{G}$ to be semisimple.\n\nThe incorrect answers are B, C, and D because they either contradict the documentation or introduce additional conditions that are not mentioned in the documentation."}, "54": {"documentation": {"title": "Bifurcations to turbulence in transitional channel flow", "source": "Masaki Shimizu and Paul Manneville", "docs_id": "1808.06479", "section": ["physics.flu-dyn", "nlin.CD", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bifurcations to turbulence in transitional channel flow. In wall-bounded parallel flows, sustained turbulence can occur even while laminar flow is still stable. Channel flow is one of such flows and displays spatio-temporal fluctuating patterns of localized turbulence along its way from/to featureless turbulence. By direct numerical simulation, we study the observed inconsistency between turbulence decay according to a two-dimensional directed-percolation (2D-DP) scenario and the presence of sustained oblique localized turbulent bands (LTBs) below the DP critical point. Above Reynolds number Reg \\sim 700 sustained LTBs are observed; most LTBs have the same orientation so that the spanwise symmetry of the LTB pattern is broken below Re2 \\sim 1000. The frequency of transversal splitting, by which an LTB generates another one with opposite obliqueness, so that turbulence spreading becomes intrinsically two dimensional, increases in the range Reg < Re < Re2. It reaches a critical rate at Re2 beyond which symmetry is restored. 2D-DP behavior is retrieved only above Re2. A mean-field model is proposed which qualitatively accounts for the above symmetry-restoring bifurcation by considering interactions between space-averaged densities of LTBs propagating in either direction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the observed phenomenon in transitional channel flow, where sustained turbulence occurs even while laminar flow is still stable, and how does it relate to the critical Reynolds number (Re) above which symmetry is restored?\n\n**A)** The phenomenon is a result of the breakdown of spanwise symmetry in localized turbulent bands (LTBs) below a critical Reynolds number (Re2) of approximately 1000, leading to a transition to two-dimensional turbulence.\n\n**B)** The phenomenon is a result of the interaction between space-averaged densities of LTBs propagating in opposite directions, which leads to a symmetry-restoring bifurcation above a critical Reynolds number (Re2) of approximately 700.\n\n**C)** The phenomenon is a result of the frequency of transversal splitting, which increases in the range of Reynolds numbers below Re2, leading to a transition to two-dimensional turbulence.\n\n**D)** The phenomenon is a result of the two-dimensional directed-percolation (2D-DP) scenario, which is inconsistent with the presence of sustained oblique localized turbulent bands (LTBs) below the DP critical point.\n\n**Correct Answer:** A) The phenomenon is a result of the breakdown of spanwise symmetry in localized turbulent bands (LTBs) below a critical Reynolds number (Re2) of approximately 1000, leading to a transition to two-dimensional turbulence.\n\n**Explanation:** The correct answer is A) because the documentation states that sustained LTBs are observed below Re2, and that the spanwise symmetry of the LTB pattern is broken below Re2. This breakdown of symmetry leads to a transition to two-dimensional turbulence. The other options are incorrect because they either misstate the critical Reynolds number (B and D) or describe a different phenomenon (C)."}, "55": {"documentation": {"title": "Calibration of Tethered Particle Motion Experiments", "source": "L. Han, B. Lui, S. Blumberg, J. Beausang, P. Nelson, R. Phillips", "docs_id": "0810.2284", "section": ["q-bio.QM", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Calibration of Tethered Particle Motion Experiments. The Tethered Particle Motion (TPM) method has been used to observe and characterize a variety of protein-DNA interactions including DNA looping and transcription. TPM experiments exploit the Brownian motion of a DNA-tethered bead to probe biologically relevant conformational changes of the tether. In these experiments, a change in the extent of the bead's random motion is used as a reporter of the underlying macromolecular dynamics and is often deemed sufficient for TPM analysis. However, a complete understanding of how the motion depends on the physical properties of the tethered particle complex would permit more quantitative and accurate evaluation of TPM data. For instance, such understanding can help extract details about a looped complex geometry (or multiple coexisting geometries) from TPM data. To better characterize the measurement capabilities of TPM experiments involving DNA tethers, we have carried out a detailed calibration of TPM magnitude as a function of DNA length and particle size. We also explore how experimental parameters such as acquisition time and exposure time affect the apparent motion of the tethered particle. We vary the DNA length from 200bp to 2.6kbp and consider particle diameters of 200, 490 and 970nm. We also present a systematic comparison between measured particle excursions and theoretical expectations, which helps clarify both the experiments and models of DNA conformation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of using the Brownian motion of a DNA-tethered bead as a reporter of biologically relevant conformational changes in TPM experiments, and how does the calibration of TPM magnitude as a function of DNA length and particle size address this limitation?\n\n**A)** The primary limitation is that the Brownian motion is not sensitive enough to detect subtle changes in DNA conformation, and the calibration of TPM magnitude is necessary to account for the physical properties of the tethered particle complex.\n\n**B)** The primary limitation is that the Brownian motion is not reliable enough to accurately measure the extent of DNA looping, and the calibration of TPM magnitude is necessary to correct for experimental parameters such as acquisition time and exposure time.\n\n**C)** The primary limitation is that the Brownian motion is not sufficient to probe biologically relevant conformational changes in DNA-tethered bead experiments, and the calibration of TPM magnitude is necessary to extract details about a looped complex geometry from TPM data.\n\n**D)** The primary limitation is that the Brownian motion is not sensitive enough to detect changes in DNA conformation, and the calibration of TPM magnitude is necessary to account for the physical properties of the tethered particle complex, but this limitation is not addressed by varying DNA length and particle size.\n\n**Correct Answer:** C) The primary limitation is that the Brownian motion is not sufficient to probe biologically relevant conformational changes in DNA-tethered bead experiments, and the calibration of TPM magnitude is necessary to extract details about a looped complex geometry from TPM data.\n\n**Explanation:** The correct answer is C) because the primary limitation of using Brownian motion as a reporter of biologically relevant conformational changes is that it is not sufficient to probe these changes. The calibration of TPM magnitude as a function of DNA length and particle size addresses this limitation by providing a more quantitative and accurate evaluation of TPM data, which can help extract details about a looped complex geometry from TPM data. The other options are incorrect because they do not accurately capture the primary limitation of using Brownian motion in TPM experiments and the purpose of the calibration of TPM magnitude."}, "56": {"documentation": {"title": "Bi-integrative analysis of two-dimensional heterogeneous panel data\n  model", "source": "Wei Wang, Xiaodong Yan, Yanyan Ren, Zhijie Xiao", "docs_id": "2110.10480", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bi-integrative analysis of two-dimensional heterogeneous panel data\n  model. Heterogeneous panel data models that allow the coefficients to vary across individuals and/or change over time have received increasingly more attention in statistics and econometrics. This paper proposes a two-dimensional heterogeneous panel regression model that incorporate a group structure of individual heterogeneous effects with cohort formation for their time-variations, which allows common coefficients between nonadjacent time points. A bi-integrative procedure that detects the information regarding group and cohort patterns simultaneously via a doubly penalized least square with concave fused penalties is introduced. We use an alternating direction method of multipliers (ADMM) algorithm that automatically bi-integrates the two-dimensional heterogeneous panel data model pertaining to a common one. Consistency and asymptotic normality for the proposed estimators are developed. We show that the resulting estimators exhibit oracle properties, i.e., the proposed estimator is asymptotically equivalent to the oracle estimator obtained using the known group and cohort structures. Furthermore, the simulation studies provide supportive evidence that the proposed method has good finite sample performance. A real data empirical application has been provided to highlight the proposed method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a two-dimensional heterogeneous panel data model that incorporates a group structure of individual heterogeneous effects with cohort formation for their time-variations. Suppose we have a dataset with 1000 observations, each with 5 time points, and we want to estimate the coefficients of the model using a doubly penalized least square with concave fused penalties. Which of the following statements about the proposed estimator is true?\n\nA) The estimator is consistent and asymptotically normal, but it does not exhibit oracle properties.\nB) The estimator exhibits oracle properties, but it is not consistent and asymptotically normal.\nC) The estimator is consistent and asymptotically normal, and it exhibits oracle properties, but it requires a large number of iterations to converge.\nD) The estimator is consistent and asymptotically normal, exhibits oracle properties, and can be computed in linear time using an ADMM algorithm.\n\n**Correct Answer:** D) The estimator is consistent and asymptotically normal, exhibits oracle properties, and can be computed in linear time using an ADMM algorithm.\n\n**Explanation:**\n\nThe correct answer is D because the documentation states that the proposed estimator is consistent and asymptotically normal, exhibits oracle properties, and can be computed in linear time using an ADMM algorithm. The estimator's oracle properties mean that it is asymptotically equivalent to the oracle estimator obtained using the known group and cohort structures. The ADMM algorithm is also mentioned as a method for computing the estimator in linear time.\n\nThe incorrect answers are:\n\nA) The estimator is consistent and asymptotically normal, but it does not exhibit oracle properties. This is incorrect because the documentation states that the estimator exhibits oracle properties.\n\nB) The estimator exhibits oracle properties, but it is not consistent and asymptotically normal. This is incorrect because the documentation states that the estimator is consistent and asymptotically normal.\n\nC) The estimator is consistent and asymptotically normal, and it exhibits oracle properties, but it requires a large number of iterations to converge. This is incorrect because the documentation does not mention the number of iterations required for convergence."}, "57": {"documentation": {"title": "Artificial quantum-dot Helium molecules: Electronic spectra, spin\n  structures, and Heisenberg clusters", "source": "Ying Li, Constantine Yannouleas, Uzi Landman", "docs_id": "0907.1571", "section": ["cond-mat.mes-hall", "cond-mat.str-el", "nucl-th", "physics.atom-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Artificial quantum-dot Helium molecules: Electronic spectra, spin\n  structures, and Heisenberg clusters. Energy spectra and spin configurations of a system of N=4 electrons in lateral double quantum dots (quantum dot Helium molecules) are investigated using exact diagonalization (EXD), as a function of interdot separation, applied magnetic field (B), and strength of interelectron repulsion. As a function of the magnetic field, the energy spectra exhibit a low-energy band consisting of a group of six states, with the number six being a consequence of the conservation of the total spin and the ensuing spin degeneracies for four electrons. The energies of the six states appear to cross at a single value of the magnetic field, and with increasing Coulomb repulsion they tend to become degenerate, with a well defined energy gap separating them from the higher-in-energy excited states. The appearance of the low-energy band is a consequence of the formation of a Wigner supermolecule, with the four electrons (two in each dot) being localized at the vertices of a rectangle. Using spin-resolved pair-correlation distributions, a method for mapping the complicated EXD many-body wave functions onto simpler spin functions associated with a system of four localized spins is introduced. Detailed interpretation of the EXD spin functions and EXD spectra associated with the low-energy band via a 4-site Heisenberg cluster (with B-dependent exchange integrals) is demonstrated. Aspects of spin entanglement, referring to the well known N-qubit Dicke states, are also discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary consequence of the formation of a Wigner supermolecule in a system of N=4 electrons in lateral double quantum dots (quantum dot Helium molecules)?\n\nA) The energy spectra exhibit a high-energy band consisting of a group of four states.\nB) The energy spectra exhibit a low-energy band consisting of a group of six states, with the number six being a consequence of the conservation of the total spin and the ensuing spin degeneracies for four electrons.\nC) The energy spectra exhibit a well-defined energy gap separating the low-energy band from the higher-in-energy excited states.\nD) The energy spectra exhibit a single state with no spin degeneracies.\n\nCorrect Answer: B) The energy spectra exhibit a low-energy band consisting of a group of six states, with the number six being a consequence of the conservation of the total spin and the ensuing spin degeneracies for four electrons.\n\nExplanation: The correct answer is B) because the formation of a Wigner supermolecule leads to the creation of a low-energy band consisting of six states, which is a consequence of the conservation of the total spin and the ensuing spin degeneracies for four electrons. This is stated in the introduction of the document: \"The appearance of the low-energy band is a consequence of the formation of a Wigner supermolecule, with the four electrons (two in each dot) being localized at the vertices of a rectangle.\"\n\nCandidate A is incorrect because it states that the energy spectra exhibit a high-energy band, which is the opposite of what is described in the document. Candidate C is incorrect because it states that the energy spectra exhibit a well-defined energy gap, but the document actually states that the energies of the six states appear to cross at a single value of the magnetic field. Candidate D is incorrect because it states that the energy spectra exhibit a single state with no spin degeneracies, which is not supported by the document."}, "58": {"documentation": {"title": "Neutron transition strengths of $2^+_1$ states in the neutron rich\n  Oxygen isotopes determined from inelastic proton scattering", "source": "Nguyen Dang Chien, Dao T. Khoa", "docs_id": "0811.4261", "section": ["nucl-th", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neutron transition strengths of $2^+_1$ states in the neutron rich\n  Oxygen isotopes determined from inelastic proton scattering. A coupled-channel analysis of the $^{18,20,22}$O$(p,p')$ data has been performed to determine the neutron transition strengths of 2$^+_1$ states in Oxygen targets, using the microscopic optical potential and inelastic form factor calculated in the folding model. A complex density- and \\emph{isospin} dependent version of the CDM3Y6 interaction was constructed, based on the Brueckner-Hatree-Fock calculation of nuclear matter, for the folding model input. Given an accurate isovector density dependence of the CDM3Y6 interaction, the isoscalar ($\\delta_0$) and isovector ($\\delta_1$) deformation lengths of 2$^+_1$ states in $^{18,20,22}$O have been extracted from the folding model analysis of the $(p,p')$ data. A specific $N$-dependence of $\\delta_0$ and $\\delta_1$ has been established which can be linked to the neutron shell closure occurring at $N$ approaching 16. The strongest isovector deformation was found for 2$^+_1$ state in $^{20}$O, with $\\delta_1$ about 2.5 times larger than $\\delta_0$, which indicates a strong core polarization by the valence neutrons in $^{20}$O. The ratios of the neutron/proton transition matrix elements ($M_n/M_p$) determined for 2$^+_1$ states in $^{18,20}$O have been compared to those deduced from the mirror symmetry, using the measured $B(E2)$ values of 2$^+_1$ states in the proton rich $^{18}$Ne and $^{20}$Mg nuclei, to discuss the isospin impurity in the $2^+_1$ excitation of the $A=18,T=1$ and $A=20,T=2$ isobars."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary conclusion drawn from the analysis of the neutron transition strengths of $2^+_1$ states in the neutron-rich Oxygen isotopes ($^{18,20,22}$O) using inelastic proton scattering data?\n\nA) The isovector deformation length of $2^+_1$ states in $^{18}$O is significantly larger than in $^{20}$O, indicating a weaker core polarization by valence neutrons.\n\nB) The isovector deformation length of $2^+_1$ states in $^{20}$O is about 2.5 times larger than in $^{18}$O, indicating a strong core polarization by valence neutrons.\n\nC) The neutron/proton transition matrix element ratios ($M_n/M_p$) for $2^+_1$ states in $^{18,20}$O are identical due to mirror symmetry, and this is consistent with the measured $B(E2)$ values of $2^+_1$ states in $^{18}$Ne and $^{20}$Mg nuclei.\n\nD) The isoscalar deformation length of $2^+_1$ states in $^{18}$O is significantly larger than in $^{20}$O, indicating a stronger neutron shell closure at $N$ approaching 16.\n\nCorrect Answer: B) The isovector deformation length of $2^+_1$ states in $^{20}$O is about 2.5 times larger than in $^{18}$O, indicating a strong core polarization by valence neutrons.\n\nExplanation: The correct answer is B) because the text states that the strongest isovector deformation was found for the $2^+_1$ state in $^{20}$O, with $\\delta_1$ about 2.5 times larger than $\\delta_0$, indicating a strong core polarization by valence neutrons. This conclusion is based on the analysis of the neutron transition strengths of $2^+_1$ states in the neutron-rich Oxygen isotopes using inelastic proton scattering data."}, "59": {"documentation": {"title": "AFDP: An Automated Function Description Prediction Approach to Improve\n  Accuracy of Protein Function Predictions", "source": "Samaneh Jozashoori, Amir Jozashoori, Heiko Schoof", "docs_id": "1910.06965", "section": ["q-bio.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "AFDP: An Automated Function Description Prediction Approach to Improve\n  Accuracy of Protein Function Predictions. With the rapid growth in high-throughput biological sequencing technologies and subsequently the amount of produced omics data, it is essential to develop automated methods to annotate the functionality of unknown genes and proteins. There are developed tools such as AHRD applying known proteins characterization to annotate unknown ones. Some other algorithms such as eggNOG apply orthologous groups of proteins to detect the most probable function. However, while the available tools focus on the detection of the most similar characterization, they are not able to generalize and integrate information from multiple homologs while maintaining accuracy. Here, we devise AFDP, an integrated approach for protein function prediction which benefits from the combination of two available tools, AHRD and eggNOG, to predict the functionality of novel proteins and produce more precise human readable descriptions by applying our stCFExt algorithm. StCFExt creates function descriptions applying available manually curated descriptions in swiss-prot. Using a benchmark dataset we show that the annotations predicted by our approach are more accurate than eggNOG and AHRD annotations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of existing tools such as AHRD and eggNOG in predicting protein function, and how does the proposed AFDP approach address this limitation?\n\n**A)** AHRD and eggNOG are unable to generalize and integrate information from multiple homologs, leading to inaccurate predictions. AFDP addresses this limitation by combining the strengths of both tools.\n\n**B)** AHRD and eggNOG are only able to detect the most similar characterization, but not the most probable function. AFDP addresses this limitation by applying the stCFExt algorithm to create more precise function descriptions.\n\n**C)** AHRD and eggNOG are unable to produce human-readable descriptions of protein function. AFDP addresses this limitation by utilizing manually curated descriptions in Swiss-Prot.\n\n**D)** AHRD and eggNOG are only able to predict protein function for known proteins, but not for novel proteins. AFDP addresses this limitation by developing an integrated approach for protein function prediction.\n\n**Correct Answer:** B) AHRD and eggNOG are only able to detect the most similar characterization, but not the most probable function. AFDP addresses this limitation by applying the stCFExt algorithm to create more precise function descriptions.\n\n**Explanation:** The correct answer is B) because the documentation states that existing tools such as AHRD and eggNOG focus on detecting the most similar characterization, but are not able to generalize and integrate information from multiple homologs to detect the most probable function. The proposed AFDP approach addresses this limitation by applying the stCFExt algorithm to create more precise function descriptions."}}