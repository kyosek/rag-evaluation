{"0": {"documentation": {"title": "Uncovering disassortativity in large scale-free networks", "source": "Nelly Litvak and Remco van der Hofstad", "docs_id": "1204.0266", "section": ["physics.soc-ph", "cond-mat.stat-mech", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Uncovering disassortativity in large scale-free networks. Mixing patterns in large self-organizing networks, such as the Internet, the World Wide Web, social and biological networks are often characterized by degree-degree dependencies between neighbouring nodes. In this paper we propose a new way of measuring degree-degree dependencies. One of the problems with the commonly used assortativity coefficient is that in disassortative networks its magnitude decreases with the network size. We mathematically explain this phenomenon and validate the results on synthetic graphs and real-world network data. As an alternative, we suggest to use rank correlation measures such as Spearman's rho. Our experiments convincingly show that Spearman's rho produces consistent values in graphs of different sizes but similar structure, and it is able to reveal strong (positive or negative) dependencies in large graphs. In particular, we discover much stronger negative degree-degree dependencies} in Web graphs than was previously thought. {Rank correlations allow us to compare the assortativity of networks of different sizes, which is impossible with the assortativity coefficient due to its genuine dependence on the network size. We conclude that rank correlations provide a suitable and informative method for uncovering network mixing patterns."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary limitation of using the assortativity coefficient to measure degree-degree dependencies in large-scale-free networks, and how does the proposed alternative method using rank correlation measures address this issue?\n\n**A)** The assortativity coefficient is sensitive to network size, but the proposed alternative method is not. However, the alternative method is more computationally intensive.\n**B)** The assortativity coefficient is not sensitive to network size, but the proposed alternative method is not suitable for large graphs. The alternative method is more prone to noise.\n**C)** The assortativity coefficient decreases with network size, making it difficult to compare networks of different sizes. The proposed alternative method, Spearman's rho, is able to reveal strong degree-degree dependencies in large graphs, regardless of network size.\n**D)** The assortativity coefficient is not sensitive to degree-degree dependencies, but the proposed alternative method is not able to capture the underlying structure of the network. The alternative method is more focused on the distribution of degrees.\n\n**Correct Answer:** C) The assortativity coefficient decreases with network size, making it difficult to compare networks of different sizes. The proposed alternative method, Spearman's rho, is able to reveal strong degree-degree dependencies in large graphs, regardless of network size.\n\n**Explanation:** The correct answer, C, highlights the primary limitation of the assortativity coefficient mentioned in the Arxiv documentation. The coefficient decreases with network size, making it difficult to compare networks of different sizes. The proposed alternative method, Spearman's rho, addresses this issue by providing a rank correlation measure that is independent of network size, allowing for the comparison of networks of different sizes. This method is able to reveal strong degree-degree dependencies in large graphs, which is a key finding of the paper."}, "1": {"documentation": {"title": "Mesoporous bioactive glass/e-polycaprolactone scaffolds promote bone\n  regeneration in osteoporotic sheep", "source": "N. Gomez-Cerezo, L. Casarrubios, M. Saiz-Pardo, L. Ortega, D. de\n  Pablo, I. Diaz-Guemes, B. Fernandez-Tome, S. Enciso, F.M. Sanchez-Margallo,\n  M.T. Portoles, D. Arcos, M. Vallet-Regi", "docs_id": "2103.13114", "section": ["q-bio.TO", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mesoporous bioactive glass/e-polycaprolactone scaffolds promote bone\n  regeneration in osteoporotic sheep. Macroporous scaffolds made of a SiO2-CaO-P2O5 mesoporous bioactive glass (MBG) and epolycaprolactone (PCL) have been prepared by robocasting. These scaffolds showed an excellent in vitro biocompatibility in contact with osteoblast like cells (Saos 2) and osteoclasts derived from RAW 264.7 macrophages. In vivo studies were carried out by implantation into cavitary defects drilled in osteoporotic sheep. The scaffolds evidenced excellent bone regeneration properties, promoting new bone formation at both the peripheral and the inner parts of the scaffolds, thick trabeculae, high vascularization and high presence of osteoblasts and osteoclasts. In order to evaluate the effects of the local release of an antiosteoporotic drug, 1% (%wt) of zoledronic acid was incorporated to the scaffolds. The scaffolds loaded with zoledronic acid induced apoptosis in Saos 2 cells, impeded osteoclast differentiation in a time dependent manner and inhibited bone healing, promoting an intense inflammatory response in osteoporotic sheep."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the effect of incorporating 1% (w/w) zoledronic acid into the mesoporous bioactive glass/epolycaprolactone scaffolds on bone regeneration in osteoporotic sheep?\n\nA) It promotes new bone formation and inhibits osteoclast differentiation.\nB) It impedes osteoclast differentiation and inhibits bone healing, promoting an intense inflammatory response.\nC) It has no effect on osteoclast differentiation or bone healing.\nD) It induces apoptosis in osteoblast-like cells and promotes bone resorption.\n\nCorrect Answer: B) It impedes osteoclast differentiation and inhibits bone healing, promoting an intense inflammatory response.\n\nExplanation: The correct answer is B) It impedes osteoclast differentiation and inhibits bone healing, promoting an intense inflammatory response. This is because the incorporation of 1% (w/w) zoledronic acid into the scaffolds induced apoptosis in osteoblast-like cells, impeded osteoclast differentiation in a time-dependent manner, and inhibited bone healing, leading to an intense inflammatory response in osteoporotic sheep. The other options are incorrect because they do not accurately reflect the effects of incorporating zoledronic acid into the scaffolds."}, "2": {"documentation": {"title": "Carrier Lifetimes in a III-V-N Intermediate Band Semiconductor", "source": "J. N. Heyman, A. M. Schwartzberg, K. M. Yu, A.V. Luce, O. D. Dubon, Y.\n  J. Kuang, C. W. Tu, W. Walukiewicz", "docs_id": "1608.05375", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Carrier Lifetimes in a III-V-N Intermediate Band Semiconductor. We have used transient absorption spectroscopy to measure carrier lifetimes in the multiband band semiconductor GaPAsN. These measurements probe the electron populations in the conduction band, intermediate band and valance band as a function of time after an excitation pulse. Following photoexcitation of GaP0.32As0.67N0.01 we find that the electron population in the conduction band decays exponentially with a time constant 23ps. The electron population in the intermediate band exhibits bimolecular recombination with recombination constant r = 2 10^-8 cm-3/s. In our experiment an optical pump pulse excited electrons from the valance band to the intermediate and conduction bands, and the change in interband absorption due to absorption saturation and induced absorption was probed with a delayed white light pulse. We modeled the optical properties of our samples using the band anti-crossing model to extract carrier densities as a function of time. These results indicate that the minority carrier lifetimes are too short for efficient solar power conversion and that improvements in material quality will be required for practical applications of GaPAsN based intermediate band solar cells."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the minority carrier lifetimes in GaPAsN intermediate band semiconductor based on the transient absorption spectroscopy measurements?\n\nA) The minority carrier lifetimes are too long for efficient solar power conversion.\nB) The minority carrier lifetimes are too short for efficient solar power conversion, and improvements in material quality are required.\nC) The minority carrier lifetimes are independent of the intermediate band and only depend on the conduction band.\nD) The minority carrier lifetimes are identical to the electron population in the conduction band.\n\nCorrect Answer: B) The minority carrier lifetimes are too short for efficient solar power conversion, and improvements in material quality are required.\n\nExplanation: The transient absorption spectroscopy measurements show that the electron population in the conduction band decays exponentially with a time constant of 23 ps, indicating a short minority carrier lifetime. Additionally, the electron population in the intermediate band exhibits bimolecular recombination with a recombination constant r = 2 \u00d7 10^(-8) cm^(-3)/s, further supporting the conclusion that the minority carrier lifetimes are too short for efficient solar power conversion. The correct answer requires the test-taker to analyze the data and draw a logical conclusion about the minority carrier lifetimes."}, "3": {"documentation": {"title": "How the Weak Variance of Momentum Can Turn Out to be Negative", "source": "M. R. Feyereisen", "docs_id": "1503.07309", "section": ["quant-ph", "physics.data-an"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "How the Weak Variance of Momentum Can Turn Out to be Negative. Weak values are average quantities,therefore investigating their associated variance is crucial in understanding their place in quantum mechanics. We develop the concept of a position-postselected weak variance of momentum as cohesively as possible, building primarily on material from Moyal (Mathematical Proceedings of the Cambridge Philosophical Society, Cambridge University Press, Cambridge, 1949) and Sonego (Found Phys 21(10):1135, 1991) . The weak variance is defined in terms of the Wigner function, using a standard construction from probability theory. We show this corresponds to a measurable quantity, which is not itself a weak value. It also leads naturally to a connection between the imaginary part of the weak value of momentum and the quantum potential. We study how the negativity of the Wigner function causes negative weak variances, and the implications this has on a class of `subquantum' theories. We also discuss the role of weak variances in studying determinism, deriving the classical limit from a variational principle."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the position-postselected weak variance of momentum, what is the relationship between the negativity of the Wigner function and the weak variances, and how does this relate to the concept of \"subquantum\" theories?\n\n**A)** The negativity of the Wigner function causes the weak variances to be positive, and this is a necessary condition for the existence of a classical limit.\n\n**B)** The negativity of the Wigner function leads to negative weak variances, which is a characteristic of \"subquantum\" theories that attempt to reconcile quantum mechanics with classical determinism.\n\n**C)** The weak variances are independent of the negativity of the Wigner function, and the relationship between the two is merely a mathematical construct.\n\n**D)** The negativity of the Wigner function is a sufficient condition for the existence of a classical limit, but it does not necessarily lead to negative weak variances.\n\n**Correct Answer:** B) The negativity of the Wigner function leads to negative weak variances, which is a characteristic of \"subquantum\" theories that attempt to reconcile quantum mechanics with classical determinism.\n\n**Explanation:** The correct answer is based on the idea that the negativity of the Wigner function causes negative weak variances, as discussed in the provided documentation. This is a key concept in the study of \"subquantum\" theories, which aim to reconcile the probabilistic nature of quantum mechanics with the deterministic nature of classical physics. The correct answer requires an understanding of the relationship between the Wigner function and the weak variances, as well as the implications of this relationship for our understanding of quantum mechanics and its relationship to classical determinism."}, "4": {"documentation": {"title": "Signatures of Chaos in Time Series Generated by Many-Spin Systems at\n  High Temperatures", "source": "Tarek A. Elsayed, Benjamin Hess, and Boris V. Fine", "docs_id": "1105.4575", "section": ["cond-mat.stat-mech", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Signatures of Chaos in Time Series Generated by Many-Spin Systems at\n  High Temperatures. Extracting reliable indicators of chaos from a single experimental time series is a challenging task, in particular, for systems with many degrees of freedom. The techniques available for this purpose often require unachievably long time series. In this paper, we explore a new method of discriminating chaotic from multi-periodic integrable motion in many-particle systems. The applicability of this method is supported by our numerical simulations of the dynamics of classical spin lattices at high temperatures. We compared chaotic and nonchaotic regimes of these lattices and investigated the transition between the two. The method is based on analyzing higher-order time derivatives of the time series of a macroscopic observable --- the total magnetization of the spin lattice. We exploit the fact that power spectra of the magnetization time series generated by chaotic spin lattices exhibit exponential high-frequency tails, while, for the integrable spin lattices, the power spectra are terminated in a non-exponential way. We have also demonstrated the applicability limits of the above method by investigating the high-frequency tails of the power spectra generated by quantum spin lattices and by the classical Toda lattice."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the key difference in the power spectra of the magnetization time series generated by chaotic and nonchaotic many-particle systems, as identified in the paper \"Signatures of Chaos in Time Series Generated by Many-Spin Systems at High Temperatures\"?\n\nA) Chaotic systems have power spectra with sharp peaks, while nonchaotic systems have power spectra with broad peaks.\nB) Chaotic systems have power spectra with exponential high-frequency tails, while nonchaotic systems have power spectra with non-exponential tails.\nC) Chaotic systems have power spectra with a single dominant frequency, while nonchaotic systems have power spectra with multiple frequencies.\nD) Chaotic systems have power spectra with a high-frequency cutoff, while nonchaotic systems have power spectra with a low-frequency cutoff.\n\n**Correct Answer:** B) Chaotic systems have power spectra with exponential high-frequency tails, while nonchaotic systems have power spectra with non-exponential tails.\n\n**Explanation:** The paper states that the method for discriminating chaotic from nonchaotic motion in many-particle systems is based on analyzing the power spectra of the magnetization time series. Specifically, it is mentioned that \"power spectra of the magnetization time series generated by chaotic spin lattices exhibit exponential high-frequency tails\", while \"power spectra are terminated in a non-exponential way\" for nonchaotic spin lattices. This suggests that the key difference in the power spectra is the presence of exponential high-frequency tails in chaotic systems, which is not observed in nonchaotic systems."}, "5": {"documentation": {"title": "Astronomy with Small Telescopes", "source": "Bohdan Paczynski", "docs_id": "astro-ph/0609161", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Astronomy with Small Telescopes. The All Sky Automated Survey (ASAS) is monitoring all sky to about 14 mag with a cadence of about 1 day; it has discovered about 10^5 variable stars, most of them new. The instrument used for the survey had aperture of 7 cm. A search for planetary transits has lead to the discovery of about a dozen confirmed planets, so called 'hot Jupiters', providing the information of planetary masses and radii. Most discoveries were done with telescopes with aperture of 10 cm. We propose a search for optical transients covering all sky with a cadence of 10 - 30 minutes and the limit of 12 - 14 mag, with an instant verification of all candidate events. The search will be made with a large number of 10 cm instruments, and the verification will be done with 30 cm instruments. We also propose a system to be located at the L_1 point of the Earth - Sun system to detect 'killer asteroids'. With a limiting magnitude of about 18 mag it could detect 10 m boulders several hours prior to their impact, provide warning against Tunguska-like events, as well as to provide news about spectacular but harmless more modest impacts."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the proposed optical transient survey, and how does it differ from previous surveys such as the All Sky Automated Survey (ASAS)?\n\nA) To detect 'killer asteroids' with a limiting magnitude of 18 mag, providing warning against Tunguska-like events, and to provide news about spectacular but harmless impacts.\nB) To search for planetary transits with a cadence of 1 day, using a 7 cm aperture instrument, and to discover new variable stars.\nC) To cover the entire sky with a cadence of 10 - 30 minutes, using a 10 cm aperture instrument, and to verify all candidate events with 30 cm instruments.\nD) To monitor the sky to about 14 mag with a cadence of about 1 day, using a 10 cm aperture instrument, and to discover about a dozen confirmed planets, so-called 'hot Jupiters'.\n\nCorrect Answer: C) To cover the entire sky with a cadence of 10 - 30 minutes, using a 10 cm aperture instrument, and to verify all candidate events with 30 cm instruments.\n\nExplanation: The correct answer is C) because the proposed survey aims to cover the entire sky with a faster cadence (10 - 30 minutes) than the ASAS (about 1 day), and to use a larger aperture instrument (10 cm) to detect optical transients. The verification of candidate events will be done with even larger 30 cm instruments. The other options are incorrect because they describe the ASAS or the search for planetary transits, but not the proposed survey."}, "6": {"documentation": {"title": "Energy Spectra of the Soft X-ray Diffuse Emission in Fourteen Fields\n  Observed with Suzaku", "source": "T. Yoshino, K. Mitsuda, N. Y. Yamasaki, Y. Takei, T. Hagihara, K.\n  Masui, M. Bauer, D. McCammon, R. Fujimoto, Q.D. Wang, and Y. Yao", "docs_id": "0903.2981", "section": ["astro-ph.GA", "astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Energy Spectra of the Soft X-ray Diffuse Emission in Fourteen Fields\n  Observed with Suzaku. The soft diffuse X-ray emission of twelve fields observed with Suzaku are presented together with two additional fields from previous analyses. All have galactic longitudes 65 deg < l < 295 deg to avoid contributions from the very bright diffuse source that extends at least 30 deg from the Galactic center. The surface brightnesses of the Suzaku nine fields for which apparently uncontaminated ROSAT All Sky Survey (RASS) were available were statistically consistent with the RASS values, with an upper limit for differences of 17 x 10^{-6} c s^{-1} amin^{-2} in R45}-band. The Ovii and Oviii intensities are well correlated to each other, and Ovii emission shows an intensity floor at ~2 photons s^{-1} cm^{-2 str^{-1} (LU). The high-latitude Oviii emission shows a tight correlation with excess of Ovii emission above the floor, with (Oviii intensity) = 0.5 x [(Ovii intensity) -2 LU], suggesting that temperatures averaged over different line-of-sight show a narrow distribution around ~0.2 keV. We consider that the offset intensity of Ovii arises from the Heliospheric solar wind charge exchange and perhaps from the local hot bubble, and that the excess Ovii (2-7 LU) is emission from more distant parts of the Galaxy. The total bolometric luminosity of this galactic emission is estimated to be 4 x 10^{39} erg s^{-1}, and its characteristic temperature may be related to the virial temperature of the Galaxy."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the relationship between the Ovii and Oviii intensities in the soft X-ray diffuse emission observed with Suzaku. What is the significance of the correlation between these intensities, and what does it suggest about the temperature distribution of the Galaxy?\n\n**A)** The correlation between Ovii and Oviii intensities suggests that the Galaxy has a uniform temperature distribution, with no significant variations.\n\n**B)** The correlation between Ovii and Oviii intensities indicates that the Galaxy has a narrow temperature distribution around 0.2 keV, with the excess Ovii emission arising from more distant parts of the Galaxy.\n\n**C)** The correlation between Ovii and Oviii intensities is due to the Heliospheric solar wind charge exchange, and the excess Ovii emission is a result of the local hot bubble.\n\n**D)** The correlation between Ovii and Oviii intensities is not significant, and the excess Ovii emission is likely due to instrumental errors.\n\n**Correct Answer:** B) The correlation between Ovii and Oviii intensities indicates that the Galaxy has a narrow temperature distribution around 0.2 keV, with the excess Ovii emission arising from more distant parts of the Galaxy.\n\n**Explanation:** The correct answer is based on the text, which states that the Ovii and Oviii intensities are well correlated, and that the high-latitude Oviii emission shows a tight correlation with excess of Ovii emission above a floor value. This suggests that the temperature distribution of the Galaxy is narrow, with an average temperature around 0.2 keV. The excess Ovii emission is likely due to more distant parts of the Galaxy, as suggested by the text."}, "7": {"documentation": {"title": "Addressing spectroscopic quality of covariant density functional theory", "source": "A. V. Afanasjev", "docs_id": "1409.4853", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Addressing spectroscopic quality of covariant density functional theory. The spectroscopic quality of covariant density functional theory has been accessed by analyzing the accuracy and theoretical uncertainties in the description of spectroscopic observables. Such analysis is first presented for the energies of the single-particle states in spherical and deformed nuclei. It is also shown that the inclusion of particle-vibration coupling improves the description of the energies of predominantly single-particle states in medium and heavy-mass spherical nuclei. However, the remaining differences between theory and experiment clearly indicate missing physics and missing terms in covariant energy density functionals. The uncertainties in the predictions of the position of two-neutron drip line sensitively depend on the uncertainties in the prediction of the energies of the single-particle states. On the other hand, many spectroscopic observables in well deformed nuclei at ground state and finite spin only weakly depend on the choice of covariant energy density functional."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary conclusion regarding the spectroscopic quality of covariant density functional theory, as assessed by analyzing the accuracy and theoretical uncertainties in the description of spectroscopic observables?\n\nA) The inclusion of particle-vibration coupling significantly improves the description of spectroscopic observables in all nuclei.\nB) The uncertainties in the predictions of the position of the two-neutron drip line are independent of the choice of covariant energy density functional.\nC) The spectroscopic quality of covariant density functional theory is limited by missing physics and missing terms in covariant energy density functionals, particularly for medium and heavy-mass spherical nuclei.\nD) The description of spectroscopic observables in well-deformed nuclei at ground state and finite spin is robust and insensitive to the choice of covariant energy density functional.\n\nCorrect Answer: C) The spectroscopic quality of covariant density functional theory is limited by missing physics and missing terms in covariant energy density functionals, particularly for medium and heavy-mass spherical nuclei.\n\nExplanation: The correct answer is C) because the documentation states that the remaining differences between theory and experiment indicate missing physics and missing terms in covariant energy density functionals, particularly for medium and heavy-mass spherical nuclei. This suggests that the spectroscopic quality of covariant density functional theory is limited by these missing components. The other options are incorrect because they either overstate the benefits of particle-vibration coupling (A), misrepresent the relationship between the two-neutron drip line and covariant energy density functionals (B), or incorrectly suggest that spectroscopic observables in well-deformed nuclei are robust and insensitive to the choice of functional (D)."}, "8": {"documentation": {"title": "Reliable inference for complex models by discriminative composite\n  likelihood estimation", "source": "Davide Ferrari and Chao Zheng", "docs_id": "1502.04765", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reliable inference for complex models by discriminative composite\n  likelihood estimation. Composite likelihood estimation has an important role in the analysis of multivariate data for which the full likelihood function is intractable. An important issue in composite likelihood inference is the choice of the weights associated with lower-dimensional data sub-sets, since the presence of incompatible sub-models can deteriorate the accuracy of the resulting estimator. In this paper, we introduce a new approach for simultaneous parameter estimation by tilting, or re-weighting, each sub-likelihood component called discriminative composite likelihood estimation (D-McLE). The data-adaptive weights maximize the composite likelihood function, subject to moving a given distance from uniform weights; then, the resulting weights can be used to rank lower-dimensional likelihoods in terms of their influence in the composite likelihood function. Our analytical findings and numerical examples support the stability of the resulting estimator compared to estimators constructed using standard composition strategies based on uniform weights. The properties of the new method are illustrated through simulated data and real spatial data on multivariate precipitation extremes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of discriminative composite likelihood estimation (D-McLE), what is the primary goal of the data-adaptive weights in maximizing the composite likelihood function, and how do these weights relate to the stability of the resulting estimator?\n\n**A)** The data-adaptive weights aim to minimize the variance of the estimator, while the stability of the estimator is ensured by the use of uniform weights.\n\n**B)** The data-adaptive weights are designed to maximize the composite likelihood function, subject to moving a given distance from uniform weights, thereby ensuring the stability of the estimator.\n\n**C)** The data-adaptive weights are used to rank lower-dimensional likelihoods in terms of their influence in the composite likelihood function, but do not directly impact the stability of the estimator.\n\n**D)** The data-adaptive weights are a byproduct of the estimation process, and their primary function is to ensure that the estimator is unbiased, rather than to maximize the composite likelihood function.\n\n**Correct Answer:** B) The data-adaptive weights are designed to maximize the composite likelihood function, subject to moving a given distance from uniform weights, thereby ensuring the stability of the estimator.\n\n**Explanation:** The correct answer is B) because the data-adaptive weights are used to maximize the composite likelihood function, subject to moving a given distance from uniform weights, which is a key aspect of the D-McLE method. This approach ensures that the estimator is stable and accurate, as demonstrated by the analytical findings and numerical examples in the paper. The other options are incorrect because they either misrepresent the primary goal of the data-adaptive weights (A and D) or incorrectly state the relationship between the weights and the stability of the estimator (C)."}, "9": {"documentation": {"title": "Bibliometric Analysis Of Herding Behavior In Times Of Crisis", "source": "Fenny Marietza, Ridwan Nurazi, Fitri Santi, Saiful", "docs_id": "2106.13598", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bibliometric Analysis Of Herding Behavior In Times Of Crisis. The social and psychological concept of herding behavior provides a suitable solution to give an understanding of the behavioral biases that often occur in the capital market. The aim of this paper is to provide an overview of the broader bibliometric literature on the term and concept of herding behavior. Articles are collected through the help of software consisting of Publish or Perish (PoP), Google Scholar, Mendeley, and VOSViewer through a systematic approach, explicit and reproductive methods. In addition, the articles were scanned by Scimagojr.com (Q1, Q2, Q3, and Q4), analyzing 83 articles of 261 related articles from reputable and non-reputable journals from 1996 to 2021. Mendeley software is used to manage and resume references. To review this database, classification was performed using the VOSviewer software. Four clusters were reviewed; The words that appear most often in each group are the type of stock market, the type of crisis, and the factors that cause herding. Thus these four clusters became the main research themes on the topic of herding in times of crisis. Meanwhile, methodology and strategy are the themes for future research in the future."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the bibliometric analysis of herding behavior in times of crisis, as presented in the paper. What are the four main research themes that emerged from the classification of the 83 articles using VOSviewer software?\n\n**A)** The impact of economic indicators on herding behavior, the role of market sentiment in shaping herding decisions, the effects of crisis on individual investor behavior, and the relationship between herding and market efficiency.\n\n**B)** The types of stock market, the types of crisis, and the factors that cause herding behavior, the methodology and strategy used to study herding, and the implications of herding for financial markets.\n\n**C)** The relationship between herding and market volatility, the impact of crisis on herding behavior, the role of social networks in shaping herding decisions, and the effects of herding on market outcomes.\n\n**D)** The types of crises, the types of stock markets, and the factors that cause herding behavior, the methodology and strategy used to study herding, and the implications of herding for financial markets.\n\n**Correct Answer:** B) The types of stock market, the types of crisis, and the factors that cause herding behavior, the methodology and strategy used to study herding, and the implications of herding for financial markets.\n\n**Explanation:** The correct answer is B) because the paper presents four clusters that emerged from the classification of the articles using VOSviewer software, which are: (1) the types of stock market, (2) the types of crisis, and (3) the factors that cause herding behavior. The fourth cluster is related to methodology and strategy, which is also mentioned in the paper as a theme for future research."}, "10": {"documentation": {"title": "New method to achieve the proper polarization state for a vector vortex\n  coronagraph", "source": "Jorge Llop-Sayson, Cole Kappel, Nemanja Jovanovic, Dimitri Mawet", "docs_id": "2108.07371", "section": ["astro-ph.IM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "New method to achieve the proper polarization state for a vector vortex\n  coronagraph. The vector vortex coronagraph (VVC) performance in the laboratory and in ground-based observatories has earned it a spot on the NASA mission concepts HabEx and LUVOIR. The VVC induces a phase ramp through the manipulation of the polarization state. Left- and right-circular polarizations get imprinted a phase ramp of opposite signs, which prevents model-based focal plane wavefront sensing and control strategies in natural light. We thus have to work with a polarization state than ensures circularly polarized light at the VVC mask. However, achieving this polarization state can be non trivial if there are optics that add phase retardance of any kind between the circular polarizer and the focal plane mask. Here we present the method currently used at the Caltech high contrast spectroscopy testbed (HCST) to achieve the proper circular polarization state for a VVC, which only uses the deformable mirror and appropriate rotation of the circular polarizer and analyzer optics. At HCST we achieve raw contrast levels of \\tentoe~for broadband light with a VVC."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary challenge in achieving the proper polarization state for a vector vortex coronagraph (VVC) when using optics that add phase retardance between the circular polarizer and the focal plane mask?\n\nA) The need for precise control over the deformable mirror to maintain the phase ramp.\nB) The difficulty in ensuring circularly polarized light at the VVC mask due to the presence of optics that add phase retardance.\nC) The requirement for a high-contrast spectroscopy testbed to achieve optimal VVC performance.\nD) The challenge of working with a polarization state that prevents model-based focal plane wavefront sensing and control strategies.\n\nCorrect Answer: B) The difficulty in ensuring circularly polarized light at the VVC mask due to the presence of optics that add phase retardance.\n\nExplanation: The question requires the test-taker to understand the challenges associated with achieving the proper polarization state for a VVC, as described in the documentation. The correct answer highlights the difficulty in ensuring circularly polarized light at the VVC mask due to the presence of optics that add phase retardance, which is a key challenge mentioned in the introduction. The other options are incorrect because they either focus on a different aspect of VVC performance (A and C) or are unrelated to the challenge described in the question (D)."}, "11": {"documentation": {"title": "Chemistry and line emission from evolving Herbig Ae disks", "source": "B. Jonkheid, C.P. Dullemond, M.R. Hogerheijde & E.F. van Dishoeck", "docs_id": "astro-ph/0611223", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chemistry and line emission from evolving Herbig Ae disks. Aims: To calculate chemistry and gas temperature of evolving protoplanetary disks with decreasing mass or dust settling, and to explore the sensitivity of gas-phase tracers. Methods: The density and dust temperature profiles for a range of models of flaring and self-shadowed disks around a typical Herbig Ae star are used together with 2-dimensional ultraviolet (UV) radiative transfer to calculate the chemistry and gas temperature. In each model the line profiles and intensities for the fine structure lines of [O I], [C II] and [C I] and the pure rotational lines of CO, CN, HCN and HCO+ are determined. Results: The chemistry shows a strong correlation with disk mass. Molecules that are easily dissociated, like HCN, require high densities and large extinctions before they can become abundant. The products of photodissociation, like CN and C2H, become abundant in models with lower masses. Dust settling mainly affects the gas temperature, and thus high temperature tracers like the O and C+ fine structure lines. The carbon chemistry is found to be very sensitive to the adopted PAH abundance. The line ratios CO/13CO, CO/HCO+ and [O I] 63 um/146 um can be used to distinguish between disks where dust growth and settling takes place, and disks that undergo overall mass loss."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary effect of dust settling on the gas temperature in evolving protoplanetary disks, and how does this impact the abundance of certain gas-phase tracers?\n\n**A)** Dust settling mainly affects the chemistry, leading to an increase in the abundance of molecules like HCN and CN.\n\n**B)** Dust settling mainly affects the gas temperature, causing it to decrease, and thus high-temperature tracers like [O I] and C+ become more abundant.\n\n**C)** Dust settling has no significant impact on the gas temperature, and therefore the abundance of gas-phase tracers remains unchanged.\n\n**D)** Dust settling mainly affects the line intensities of fine structure lines, making it difficult to distinguish between disks with and without dust growth.\n\n**Correct Answer:** B) Dust settling mainly affects the gas temperature, causing it to decrease, and thus high-temperature tracers like [O I] and C+ become more abundant.\n\n**Explanation:** According to the results section of the Arxiv documentation, dust settling mainly affects the gas temperature, causing it to decrease. This decrease in temperature leads to an increase in the abundance of high-temperature tracers like [O I] and C+, which are sensitive to changes in gas temperature. Therefore, option B is the correct answer."}, "12": {"documentation": {"title": "Anisotropic Multiverse with Varying $c$, $G$ and Study of Thermodynamics", "source": "Ujjal Debnath and Soumak Nag", "docs_id": "2105.02687", "section": ["gr-qc", "astro-ph.CO", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Anisotropic Multiverse with Varying $c$, $G$ and Study of Thermodynamics. We assume the anisotropic model of the Universe in the framework of varying speed of light $c$ and varying gravitational constant $G$ theories and study different types of singularities. For the singularity models, we write the scale factors in terms of cosmic time and found some conditions for possible singularities. For future singularities, we assume the forms of varying speed of light and varying gravitational constant. For regularizing big bang singularity, we assume two forms of scale factors: sine model and tangent model. For both the models, we examine the validity of null energy condition and strong energy condition. Start from the first law of thermodynamics, we study the thermodynamic behaviours of $n$ number of Universes (i.e., Multiverse) for (i) varying $c$, (ii) varying $G$ and (iii) both varying $c$ and $G$ models. We found the total entropies for all the cases in the anisotropic Multiverse model. We also found the nature of the Multiverse if total entropy is constant."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the anisotropic multiverse model with varying speed of light $c$ and varying gravitational constant $G$, examine the thermodynamic behaviors of $n$ number of universes (i.e., multiverse) for the following scenarios:\n\nA) Varying $c$ and constant $G$, where the scale factors are given by $a(t) = \\sin(t)$ and $b(t) = \\cos(t)$.\n\nB) Constant $c$ and varying $G$, where the scale factors are given by $a(t) = e^t$ and $b(t) = \\ln(t)$.\n\nC) Both varying $c$ and $G$, where the scale factors are given by $a(t) = \\sin(t) + \\cos(t)$ and $b(t) = \\sin(t) - \\cos(t)$.\n\nD) Constant $c$ and constant $G$, where the scale factors are given by $a(t) = t^2$ and $b(t) = t^3$.\n\n**Correct Answer:** C) Both varying $c$ and $G$, where the scale factors are given by $a(t) = \\sin(t) + \\cos(t)$ and $b(t) = \\sin(t) - \\cos(t)$.\n\n**Explanation:**\n\nThe correct answer is C) Both varying $c$ and $G$, where the scale factors are given by $a(t) = \\sin(t) + \\cos(t)$ and $b(t) = \\sin(t) - \\cos(t)$. This is because the anisotropic multiverse model with varying $c$ and $G$ requires the scale factors to be functions of both $c$ and $G$. The given scale factors $a(t) = \\sin(t) + \\cos(t)$ and $b(t) = \\sin(t) - \\cos(t)$ satisfy this condition, and the null energy condition and strong energy condition can be examined for these scale factors.\n\nThe other options are incorrect because:\n\nA) Varying $c$ and constant $G$ does not require the scale factors to be functions of both $c$ and $G$.\n\nB) Constant $c$ and varying $G$ does not require the scale factors to be functions of both $c$ and $G$.\n\nD) Constant $c$ and constant $G$ does not require the scale factors to be functions of both $c$ and $G$.\n\nTherefore, option C) Both varying $c$ and $G$, where the scale factors are given by $a(t) = \\sin(t) + \\cos(t)$ and $b(t) = \\sin(t) - \\cos(t)$, is the correct answer."}, "13": {"documentation": {"title": "Suzaku Observations of the X-ray Brightest Fossil Group ESO 3060170", "source": "Yuanyuan Su, Raymond E. White and Eric D. Miller", "docs_id": "1308.0283", "section": ["astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Suzaku Observations of the X-ray Brightest Fossil Group ESO 3060170. \"Fossil\" galaxy groups, each dominated by a relatively isolated giant elliptical galaxy, have many properties intermediate between groups and clusters of galaxies. We used the {\\sl Suzaku} X-ray observatory to observe the X-ray brightest fossil group, ESO 3060170, out to $R_{200}$, in order to better elucidate the relation between fossil groups, normal groups, and clusters. We determined the intragroup gas temperature, density, and metal abundance distributions and derived the entropy, pressure and mass profiles for this group. The entropy and pressure profiles in the outer regions are flatter than in simulated clusters, similar to what is seen in observations of massive clusters. This may indicate that the gas is clumpy and/or the gas has been redistributed. Assuming hydrostatic equilibrium, the total mass is estimated to be $\\sim1.7\\times10^{14}$ $M_{\\odot}$ within a radius $R_{200}$ of $\\sim1.15$ Mpc, with an enclosed baryon mass fraction of 0.14. The integrated iron mass-to-light ratio of this fossil group is larger than in most groups and comparable to those of clusters, indicating that this fossil group has retained the bulk of its metals. A galaxy luminosity density map on a scale of 25 Mpc shows that this fossil group resides in a relatively isolated environment, unlike the filamentary structures in which typical groups and clusters are embedded."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the X-ray observations of the fossil group ESO 3060170 using the Suzaku X-ray observatory. What can be inferred about the gas distribution and properties of this group, and how does it compare to simulations of clusters and observations of massive clusters?\n\n**A)** The gas temperature and density profiles of ESO 3060170 are consistent with a clumpy gas distribution, which is also observed in simulations of clusters. The entropy and pressure profiles in the outer regions are steeper than in simulations of clusters, indicating a more uniform gas distribution.\n\n**B)** The gas temperature and density profiles of ESO 3060170 are consistent with a uniform gas distribution, which is also observed in simulations of clusters. The entropy and pressure profiles in the outer regions are flatter than in simulations of clusters, indicating a clumpy gas distribution.\n\n**C)** The gas temperature and density profiles of ESO 3060170 are consistent with a fossil group, but the entropy and pressure profiles in the outer regions are steeper than in simulations of clusters, indicating a more uniform gas distribution.\n\n**D)** The gas temperature and density profiles of ESO 3060170 are consistent with a fossil group, but the entropy and pressure profiles in the outer regions are flatter than in simulations of clusters, indicating a clumpy gas distribution.\n\n**Correct Answer:** B) The gas temperature and density profiles of ESO 3060170 are consistent with a uniform gas distribution, which is also observed in simulations of clusters. The entropy and pressure profiles in the outer regions are flatter than in simulations of clusters, indicating a clumpy gas distribution.\n\n**Explanation:** The correct answer is B) because the text states that \"The entropy and pressure profiles in the outer regions are flatter than in simulated clusters, similar to what is seen in observations of massive clusters.\" This suggests that the gas distribution is uniform in the outer regions, which is consistent with a fossil group. Additionally, the text states that \"The integrated iron mass-to-light ratio of this fossil group is larger than in most groups and comparable to those of clusters, indicating that this fossil group has retained the bulk of its metals.\" This suggests that the gas distribution is clumpy, as the metal abundance is higher than in most groups."}, "14": {"documentation": {"title": "Defect-Dependent Corrugation in Graphene", "source": "Fabian L. Thiemann, Patrick Rowe, Andrea Zen, Erich A. M\\\"uller,\n  Angelos Michaelides", "docs_id": "2107.01128", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Defect-Dependent Corrugation in Graphene. Graphene's intrinsically corrugated and wrinkled topology fundamentally influences its electronic, mechanical, and chemical properties. Experimental techniques allow the manipulation of pristine graphene and the controlled production of defects which allows to control the atomic out-of-plane fluctuations and, thus, tune graphene's properties. Here, we perform large scale machine learning-driven molecular dynamics simulations to understand the impact of defects on the structure of graphene. We find that defects cause significantly higher corrugation leading to a strongly wrinkled surface. The magnitude of this structural transformation strongly depends on the defect concentration and specific type of defect. Analysing the atomic neighborhood of the defects reveals that the extent of these morphological changes depends on the preferred geometrical orientation and the interactions between defects. While our work highlights that defects can strongly affect graphene's morphology, it also emphasises the differences between distinct types by linking the global structure to the local environment of the defects."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary factor that influences the magnitude of structural transformation in graphene caused by defects, according to the study?\n\nA) The type of defect\nB) The concentration of defects\nC) The interactions between defects\nD) The global structure of graphene\n\n**Correct Answer:** B) The concentration of defects\n\n**Explanation:** The study states that \"the magnitude of this structural transformation strongly depends on the defect concentration and specific type of defect.\" This suggests that the concentration of defects is the primary factor that influences the magnitude of structural transformation in graphene.\n\n**Explanation for incorrect options:**\n\nA) The type of defect: While the study mentions that different types of defects can have different effects on graphene's morphology, it does not suggest that the type of defect is the primary factor influencing the magnitude of structural transformation.\n\nC) The interactions between defects: The study mentions that the interactions between defects play a role in determining the extent of morphological changes, but it does not suggest that they are the primary factor influencing the magnitude of structural transformation.\n\nD) The global structure of graphene: The study emphasizes the importance of considering the local environment of defects, rather than just the global structure of graphene, in understanding the effects of defects on graphene's morphology."}, "15": {"documentation": {"title": "Investigating toroidal flows in the Sun using normal-mode coupling", "source": "Prasad Mani and Shravan Hanasoge", "docs_id": "2108.01426", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Investigating toroidal flows in the Sun using normal-mode coupling. Helioseismic observations have provided valuable datasets with which to pursue the detailed investigation of solar interior dynamics. Among various methods to analyse these data, normal-mode coupling has proven to be a powerful tool, used to study Rossby waves, differential rotation, meridional circulation, and non-axisymmetric multi-scale subsurface flows. Here, we invert mode-coupling measurements from Helioseismic Magnetic Imager (HMI) and Michelson Doppler Imager (MDI) to obtain mass-conserving toroidal convective flow as a function of depth, spatial wavenumber, and temporal frequency. To ensure that the estimates of velocity magnitudes are proper, we also evaluate correlated realization noise, caused by the limited visibility of the Sun. We benchmark the near-surface inversions against results from Local Correlation Tracking (LCT). Convective power likely assumes greater latitudinal isotropy with decrease in spatial scale of the flow. We note an absence of a peak in toroidal-flow power at supergranular scales, in line with observations that show that supergranulation is dominantly poloidal in nature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the nature of convective power in the Sun's interior based on the findings of this study?\n\nA) Convective power is likely to be highly latitudinally anisotropic, with significant variations in power between different latitudes.\nB) Convective power is likely to be dominantly toroidal in nature, with a peak in power at supergranular scales.\nC) Convective power is likely to be isotropic, with equal power distributions across different latitudes and spatial scales.\nD) Convective power is likely to be highly correlated with the presence of magnetic fields, with a significant increase in power in regions with strong magnetic activity.\n\nCorrect Answer: C) Convective power is likely to be isotropic, with equal power distributions across different latitudes and spatial scales.\n\nExplanation: The study notes that \"Convective power likely assumes greater latitudinal isotropy with decrease in spatial scale of the flow.\" This suggests that as the spatial scale of the flow decreases, the power distribution becomes more isotropic, meaning that the power is more evenly distributed across different latitudes. This is in line with the idea that convective power is likely to be isotropic, with equal power distributions across different latitudes and spatial scales."}, "16": {"documentation": {"title": "The Vigilant Eating Rule: A General Approach for Probabilistic Economic\n  Design with Constraints", "source": "Haris Aziz and Florian Brandl", "docs_id": "2008.08991", "section": ["econ.TH", "cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Vigilant Eating Rule: A General Approach for Probabilistic Economic\n  Design with Constraints. We consider the problem of probabilistic allocation of objects under ordinal preferences. We devise an allocation mechanism, called the vigilant eating rule (VER), that applies to nearly arbitrary feasibility constraints. It is constrained ordinally efficient, can be computed efficiently for a large class of constraints, and treats agents equally if they have the same preferences and are subject to the same constraints. When the set of feasible allocations is convex, we also present a characterization of our rule based on ordinal egalitarianism. Our results about VER do not just apply to allocation problems but to all collective choice problems in which agents have ordinal preferences over discrete outcomes. As a case study, we assume objects have priorities for agents and apply VER to sets of probabilistic allocations that are constrained by stability. VER coincides with the (extended) probabilistic serial rule when priorities are flat and the agent proposing deterministic deferred acceptance algorithm when preferences and priorities are strict. While VER always returns a stable and constrained efficient allocation, it fails to be strategyproof, unconstrained efficient, and envy-free. We show, however, that each of these three properties is incompatible with stability and constrained efficiency."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a collective choice problem where agents have ordinal preferences over discrete outcomes, and the set of feasible allocations is convex. Suppose we have three allocation mechanisms: the Vigilant Eating Rule (VER), the Probabilistic Serial Rule (PSR), and the Deterministic Deferred Acceptance Algorithm (DDA). Which of the following statements is true?\n\nA) VER is strategyproof, unconstrained efficient, and envy-free.\nB) PSR is constrained efficient, but fails to be stable and envy-free.\nC) DDA is stable, but fails to be constrained efficient and envy-free.\nD) VER is stable, but fails to be strategyproof, unconstrained efficient, and envy-free.\n\n**Correct Answer:** D) VER is stable, but fails to be strategyproof, unconstrained efficient, and envy-free.\n\n**Explanation:**\n\nThe Vigilant Eating Rule (VER) is a stable and constrained efficient allocation mechanism, but it fails to be strategyproof, unconstrained efficient, and envy-free. This is stated in the provided documentation: \"VER always returns a stable and constrained efficient allocation, it fails to be strategyproof, unconstrained efficient, and envy-free.\"\n\nThe other options are incorrect because:\n\n* A) VER is not strategyproof, unconstrained efficient, and envy-free.\n* B) PSR is not constrained efficient, and its stability and envy-freeness are not guaranteed.\n* C) DDA is not constrained efficient, and its stability and envy-freeness are not guaranteed.\n\nThis question requires the test-taker to carefully read and understand the documentation, and to analyze the properties of each allocation mechanism. It also requires the test-taker to think critically and make connections between the different concepts discussed in the documentation."}, "17": {"documentation": {"title": "Recovery of chaotic tunneling due to destruction of dynamical\n  localization by external noise", "source": "Akiyuki Ishikawa, Atushi Tanaka and Akira Shudo", "docs_id": "0910.1163", "section": ["nlin.CD", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Recovery of chaotic tunneling due to destruction of dynamical\n  localization by external noise. Quantum tunneling in the presence of chaos is analyzed, focusing especially on the interplay between quantum tunneling and dynamical localization. We observed flooding of potentially existing tunneling amplitude by adding noise to the chaotic sea to attenuate the destructive interference generating dynamical localization. This phenomenon is related to the nature of complex orbits describing tunneling between torus and chaotic regions. The tunneling rate is found to obey a perturbative scaling with noise intensity when the noise intensity is sufficiently small and then saturate in a large noise intensity regime. A relation between the tunneling rate and the localization length of the chaotic states is also demonstrated. It is shown that due to the competition between dynamical tunneling and dynamical localization, the tunneling rate is not a monotonically increasing function of Planck's constant. The above results are obtained for a system with a sharp border between torus and chaotic regions. The validity of the results for a system with a smoothed border is also explained."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of quantum tunneling in chaotic systems, what is the relationship between the tunneling rate and the localization length of chaotic states, as demonstrated in the study on the recovery of chaotic tunneling due to destruction of dynamical localization by external noise?\n\nA) The tunneling rate is directly proportional to the localization length.\nB) The tunneling rate is inversely proportional to the localization length.\nC) The tunneling rate is a monotonically increasing function of the localization length.\nD) The tunneling rate is a perturbative scaling with noise intensity, which saturates in a large noise intensity regime, and is related to the localization length.\n\nCorrect Answer: D) The tunneling rate is a perturbative scaling with noise intensity, which saturates in a large noise intensity regime, and is related to the localization length.\n\nExplanation: The correct answer is D) because the study demonstrates that the tunneling rate is a perturbative scaling with noise intensity, which means that it increases with the noise intensity but eventually saturates. Additionally, the study shows that the tunneling rate is related to the localization length of chaotic states, which is a key concept in understanding the interplay between quantum tunneling and dynamical localization. The other options are incorrect because they do not accurately reflect the relationship between the tunneling rate and the localization length as demonstrated in the study."}, "18": {"documentation": {"title": "Neutrinoless Double Beta Decay in Type I+II Seesaw Models", "source": "Debasish Borah and Arnab Dasgupta", "docs_id": "1509.01800", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neutrinoless Double Beta Decay in Type I+II Seesaw Models. We study neutrinoless double beta decay in left-right symmetric extension of the standard model with type I and type II seesaw origin of neutrino masses. Due to the enhanced gauge symmetry as well as extended scalar sector, there are several new physics sources of neutrinoless double beta decay in this model. Ignoring the left-right gauge boson mixing and heavy-light neutrino mixing, we first compute the contributions to neutrinoless double beta decay for type I and type II dominant seesaw separately and compare with the standard light neutrino contributions. We then repeat the exercise by considering the presence of both type I and type II seesaw, having non-negligible contributions to light neutrino masses and show the difference in results from individual seesaw cases. Assuming the new gauge bosons and scalars to be around a TeV, we constrain different parameters of the model including both heavy and light neutrino masses from the requirement of keeping the new physics contribution to neutrinoless double beta decay amplitude below the upper limit set by the GERDA experiment and also satisfying bounds from lepton flavor violation, cosmology and colliders."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the left-right symmetric extension of the standard model with type I and type II seesaw origin of neutrino masses, what is the primary source of neutrinoless double beta decay that is not accounted for by the standard light neutrino contributions?\n\nA) Heavy-light neutrino mixing\nB) Left-right gauge boson mixing\nC) Type I seesaw contribution\nD) Type II seesaw contribution\n\nCorrect Answer: A) Heavy-light neutrino mixing\n\nExplanation: The question requires the test-taker to understand the context of the documentation and identify the primary source of neutrinoless double beta decay that is not accounted for by the standard light neutrino contributions. The correct answer, heavy-light neutrino mixing, is mentioned in the introduction as a new physics source of neutrinoless double beta decay in the left-right symmetric extension of the standard model. The other options are incorrect because left-right gauge boson mixing is ignored, type I and type II seesaw contributions are considered separately, and type II seesaw contribution is not the primary source of neutrinoless double beta decay that is not accounted for by the standard light neutrino contributions."}, "19": {"documentation": {"title": "On conditions of negativity of friction resistance for non-stationary\n  modes of blood flow and possible mechanism of affecting of environmental\n  factors on energy effectiveness of cardio-vascular system functioning", "source": "S.G. Chefranov", "docs_id": "1301.6603", "section": ["physics.flu-dyn", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On conditions of negativity of friction resistance for non-stationary\n  modes of blood flow and possible mechanism of affecting of environmental\n  factors on energy effectiveness of cardio-vascular system functioning. It is shown that initiated by action of molecular viscosity impulse flow, directed usually from the moving fluid to limiting it solid surface, can, under certain conditions, turn to zero and get negative values in the case of non-stationary flow caused by alternating in time longitudinal (along the pipe axis) pressure gradient. It is noted that this non-equilibrium mechanism of negative friction resistance in the similar case of pulsating blood flow in the blood vessels, in addition to the stable to turbulent disturbances swirled blood flow structure providing, can also constitute hydro-mechanical basis of the observed but not explained yet paradoxically high energy effectiveness of the normal functioning of the cardio-vascular system (CVS). We consider respective mechanism of affecting on the stability of the normal work of CVS by environmental variable factors using shifting of hydro-dynamic mode with negative resistance realization range boundaries and variation of linear hydro-dynamic instability leading to the structurally stable swirled blood flow organization."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the proposed mechanism by which environmental factors can affect the energy effectiveness of the cardiovascular system (CVS) functioning, and how does it relate to the stability of normal blood flow?\n\n**A)** The mechanism involves the action of molecular viscosity to create a negative pressure gradient, leading to a decrease in energy consumption by the CVS.\n\n**B)** The mechanism involves the shifting of hydrodynamic modes, resulting in a range of negative resistance realization, which can lead to structurally stable swirled blood flow organization.\n\n**C)** The mechanism involves the variation of linear hydrodynamic instability, causing a decrease in energy consumption by the CVS, and is related to the stability of normal blood flow.\n\n**D)** The mechanism involves the action of environmental factors to increase the molecular viscosity of blood, leading to a decrease in energy consumption by the CVS.\n\n**Correct Answer:** B) The mechanism involves the shifting of hydrodynamic modes, resulting in a range of negative resistance realization, which can lead to structurally stable swirled blood flow organization.\n\n**Explanation:** The correct answer is B) because the documentation states that the proposed mechanism involves the shifting of hydrodynamic modes, resulting in a range of negative resistance realization, which can lead to structurally stable swirled blood flow organization. This is related to the stability of normal blood flow and the energy effectiveness of the CVS functioning. The other options are incorrect because they do not accurately reflect the proposed mechanism described in the documentation."}, "20": {"documentation": {"title": "Corresponding states for mesostructure and dynamics of supercooled water", "source": "David T. Limmer and David Chandler", "docs_id": "1305.1382", "section": ["cond-mat.stat-mech", "cond-mat.soft", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Corresponding states for mesostructure and dynamics of supercooled water. Water famously expands upon freezing, foreshadowed by a negative coefficient of expansion of the liquid at temperatures close to its freezing temperature. These behaviors, and many others, reflect the energetic preference for local tetrahedral arrangements of water molecules and entropic effects that oppose it. Here, we provide theoretical analysis of mesoscopic implications of this competition, both equilibrium and non-equilibrium, including mediation by interfaces. With general scaling arguments bolstered by simulation results, and with reduced units that elucidate corresponding states, we derive a phase diagram for bulk and confined water and water-like materials. For water itself, the corresponding states cover the temperature range of 150 K to 300 K and the pressure range of 1 bar to 2 kbar. In this regime, there are two reversible condensed phases - ice and liquid. Out of equilibrium, there is irreversible polyamorphism, i.e., more than one glass phase, reflecting dynamical arrest of coarsening ice. Temperature-time plots are derived to characterize time scales of the different phases and explain contrasting dynamical behaviors of different water-like systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary driving force behind the expansion of water upon freezing, and how does it relate to the energetic preference for local tetrahedral arrangements of water molecules?\n\nA) Entropic effects that favor a more open structure, leading to a decrease in volume upon freezing.\nB) A negative coefficient of expansion of the liquid at temperatures close to its freezing temperature, which reflects the energetic preference for local tetrahedral arrangements of water molecules.\nC) The formation of hydrogen bonds between water molecules, which leads to a more compact and dense structure in the solid state.\nD) The presence of impurities or defects in the water crystal lattice, which disrupts the regular arrangement of water molecules.\n\n**Correct Answer:** B) A negative coefficient of expansion of the liquid at temperatures close to its freezing temperature, which reflects the energetic preference for local tetrahedral arrangements of water molecules.\n\n**Explanation:** The correct answer is based on the text, which states that the negative coefficient of expansion of the liquid at temperatures close to its freezing temperature \"foreshadowed\" the expansion of water upon freezing. This suggests that the negative coefficient of expansion is a consequence of the energetic preference for local tetrahedral arrangements of water molecules, which is a key aspect of the competition between equilibrium and non-equilibrium behaviors in supercooled water."}, "21": {"documentation": {"title": "Heterogeneous length of stay of hosts' movements and spatial epidemic\n  spread", "source": "Chiara Poletto, Michele Tizzoni, Vittoria Colizza", "docs_id": "1207.4746", "section": ["physics.soc-ph", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Heterogeneous length of stay of hosts' movements and spatial epidemic\n  spread. Infectious diseases outbreaks are often characterized by a spatial component induced by hosts' distribution, mobility, and interactions. Spatial models that incorporate hosts' movements are being used to describe these processes, to investigate the conditions for propagation, and to predict the spatial spread. Several assumptions are being considered to model hosts' movements, ranging from permanent movements to daily commuting, where the time spent at destination is either infinite or assumes a homogeneous fixed value, respectively. Prompted by empirical evidence, here we introduce a general metapopulation approach to model the disease dynamics in a spatially structured population where the mobility process is characterized by a heterogeneous length of stay. We show that large fluctuations of the length of stay, as observed in reality, can have a significant impact on the threshold conditions for the global epidemic invasion, thus altering model predictions based on simple assumptions, and displaying important public health implications."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of modeling the spread of infectious diseases, what is the primary assumption underlying the traditional approach to modeling hosts' movements, and how does the introduction of a heterogeneous length of stay in hosts' movements affect the threshold conditions for global epidemic invasion?\n\n**A)** The traditional approach assumes a homogeneous fixed length of stay, while the introduction of a heterogeneous length of stay can lead to significant changes in the model's predictions.\n\n**B)** The traditional approach assumes a permanent length of stay, while the introduction of a heterogeneous length of stay can lead to a decrease in the threshold conditions for global epidemic invasion.\n\n**C)** The traditional approach assumes a daily commuting pattern with an infinite length of stay, while the introduction of a heterogeneous length of stay can lead to an increase in the threshold conditions for global epidemic invasion.\n\n**D)** The traditional approach assumes a daily commuting pattern with a fixed length of stay, while the introduction of a heterogeneous length of stay can lead to a decrease in the threshold conditions for global epidemic invasion.\n\n**Correct Answer:** A) The traditional approach assumes a homogeneous fixed length of stay, while the introduction of a heterogeneous length of stay can lead to significant changes in the model's predictions.\n\n**Explanation:** The traditional approach to modeling hosts' movements assumes a homogeneous fixed length of stay, where the time spent at destination is either infinite or assumes a fixed value. However, empirical evidence suggests that the length of stay can be heterogeneous, leading to significant fluctuations. The introduction of a heterogeneous length of stay can alter the model's predictions, including the threshold conditions for global epidemic invasion. This has important public health implications, as it can lead to changes in the spread of infectious diseases."}, "22": {"documentation": {"title": "Predicting acoustic relaxation absorption in gas mixtures for extraction\n  of composition relaxation contributions", "source": "Tingting Liu, Shu Wang, Ming Zhu", "docs_id": "1709.03795", "section": ["physics.chem-ph", "physics.app-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Predicting acoustic relaxation absorption in gas mixtures for extraction\n  of composition relaxation contributions. The existing molecular relaxation models based on both parallel relaxation theory and series relaxation theory cannot extract the contributions of gas compositions to acoustic relaxation absorption in mixtures. In this paper, we propose an analytical model to predict acoustic relaxation absorption and clarify composition relaxation contributions based on the rate-determining energy transfer processes in molecular relaxation in excitable gases. By combining parallel and series relaxation theory, the proposed model suggests that the vibration-translation process of the lowest vibrational mode in each composition provides the primary deexcitation path of the relaxation energy, and the rate-determining vibration-vibration processes between the lowest mode and others dominate the coupling energy transfer between different modes. Thus, each gas composition contributes directly one single relaxation process to the molecular relaxation in mixture, which can be illustrated by the decomposed acoustic relaxation absorption spectrum of the single relaxation process. The proposed model is validated by simulation results in good agreement with experimental data such as $\\mathrm{N_2}$, $\\mathrm{O_2}$, $\\mathrm{CO_2}$, $\\mathrm{CH_4}$ and their mixtures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary deexcitation path of the relaxation energy in gas mixtures, according to the proposed analytical model?\n\nA) The vibration-translation process of the highest vibrational mode in each composition\nB) The vibration-translation process of the lowest vibrational mode in each composition\nC) The translation-rotation process of the lowest rotational mode in each composition\nD) The vibration-vibration process between the highest and lowest modes in each composition\n\n**Correct Answer:** B) The vibration-translation process of the lowest vibrational mode in each composition\n\n**Explanation:** The proposed model suggests that the vibration-translation process of the lowest vibrational mode in each composition provides the primary deexcitation path of the relaxation energy. This is because the lowest vibrational mode is the most stable and has the lowest energy, making it the most likely to undergo relaxation. The other options are incorrect because they do not accurately describe the primary deexcitation path of the relaxation energy in gas mixtures.\n\n**Additional explanation:** The model combines parallel and series relaxation theory to explain the relaxation processes in gas mixtures. The vibration-translation process of the lowest vibrational mode is identified as the primary deexcitation path, and the rate-determining vibration-vibration processes between the lowest mode and others dominate the coupling energy transfer between different modes. This understanding is crucial for predicting acoustic relaxation absorption and clarifying composition relaxation contributions in gas mixtures."}, "23": {"documentation": {"title": "Quantum-Inspired Support Vector Machine", "source": "Chen Ding, Tian-Yi Bao, He-Liang Huang", "docs_id": "1906.08902", "section": ["cs.LG", "cs.CC", "quant-ph", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantum-Inspired Support Vector Machine. Support vector machine (SVM) is a particularly powerful and flexible supervised learning model that analyzes data for both classification and regression, whose usual algorithm complexity scales polynomially with the dimension of data space and the number of data points. To tackle the big data challenge, a quantum SVM algorithm was proposed, which is claimed to achieve exponential speedup for least squares SVM (LS-SVM). Here, inspired by the quantum SVM algorithm, we present a quantum-inspired classical algorithm for LS-SVM. In our approach, a improved fast sampling technique, namely indirect sampling, is proposed for sampling the kernel matrix and classifying. We first consider the LS-SVM with a linear kernel, and then discuss the generalization of our method to non-linear kernels. Theoretical analysis shows our algorithm can make classification with arbitrary success probability in logarithmic runtime of both the dimension of data space and the number of data points for low rank, low condition number and high dimensional data matrix, matching the runtime of the quantum SVM."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed quantum-inspired classical algorithm for Least Squares Support Vector Machine (LS-SVM) over traditional algorithms, and how does it achieve this advantage?\n\n**A)** The algorithm uses a non-linear kernel, which allows for more complex data relationships to be modeled, resulting in improved classification accuracy.\n**B)** The algorithm employs an improved fast sampling technique, namely indirect sampling, which reduces the computational complexity of sampling the kernel matrix and classification, leading to exponential speedup for LS-SVM.\n**C)** The algorithm uses a parallel processing approach, which allows for the simultaneous processing of multiple data points, resulting in significant speedup for large datasets.\n**D)** The algorithm incorporates a regularization term, which helps to prevent overfitting and improves the generalization performance of the model.\n\n**Correct Answer:** B) The algorithm employs an improved fast sampling technique, namely indirect sampling, which reduces the computational complexity of sampling the kernel matrix and classification, leading to exponential speedup for LS-SVM.\n\n**Explanation:** The correct answer is B) because the proposed algorithm uses an improved fast sampling technique, namely indirect sampling, which is claimed to achieve exponential speedup for LS-SVM. This is a key advantage of the algorithm, as it allows for faster computation of the kernel matrix and classification, making it more efficient for large datasets. The other options are incorrect because they do not accurately describe the primary advantage of the proposed algorithm. Option A is incorrect because the algorithm does not use a non-linear kernel, option C is incorrect because the algorithm does not employ parallel processing, and option D is incorrect because the algorithm does not incorporate a regularization term."}, "24": {"documentation": {"title": "Evaluation of Accounting and Market Performance: A Study on Listed\n  Islamic Banks of Bangladesh", "source": "Nusrat Jahan and M. Ayub Islam", "docs_id": "2005.08734", "section": ["q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evaluation of Accounting and Market Performance: A Study on Listed\n  Islamic Banks of Bangladesh. This study compared accounting performance of Islamic banks with their market performance and also assessed the effect of firm-specific determinants and cross-sectional effect on accounting and market performance. This study selected all six listed Islamic banks of Chittagong Stock Exchange and the data were collected for the period of 2009 to 2013. This study reported that Social Islamic Bank Limited exhibits superior accounting performance whereas Islami Bank Bangladesh Limited holds better market performance. However, banks exhibiting superior accounting performance reported to have inferior market performance. Further, random-effect model for ROA reports that there exist significant entity or crosssectional effect on ROA; and operational efficiency and bank size are significantly negatively associated with ROA. However, random-effect model for Tobins Q failed to ascertain entity or cross-sectional effect on Tobins Q and also reveals that firm-specific determinants have no significant impact on Tobins Q."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Evaluate the relationship between accounting performance and market performance of listed Islamic banks in Bangladesh, using the study's findings as a basis for your analysis.\n\n**A)** The study suggests that superior accounting performance is positively correlated with market performance, and that firms with better accounting performance tend to outperform their peers in the market.\n\n**B)** The study reports that Social Islamic Bank Limited exhibits superior accounting performance, but its market performance is inferior compared to Islami Bank Bangladesh Limited, which has better market performance.\n\n**C)** The random-effect model for ROA (Return on Assets) reveals that operational efficiency and bank size are significantly negatively associated with ROA, indicating that these factors have a negative impact on accounting performance.\n\n**D)** The study's findings indicate that firm-specific determinants have no significant impact on Tobin's Q (a measure of firm value), and that the entity or cross-sectional effect on ROA is significant, suggesting that there are underlying factors that influence accounting performance.\n\n**Correct Answer:** D) The study's findings indicate that firm-specific determinants have no significant impact on Tobin's Q, and that the entity or cross-sectional effect on ROA is significant, suggesting that there are underlying factors that influence accounting performance.\n\n**Explanation:** This question requires the candidate to carefully read and understand the study's findings, and to analyze the relationships between accounting performance and market performance. The correct answer (D) highlights the study's key findings, including the lack of significant impact of firm-specific determinants on Tobin's Q and the presence of an entity or cross-sectional effect on ROA. The incorrect answers (A, B, and C) either misinterpret or oversimplify the study's findings, demonstrating a lack of understanding of the complex relationships between accounting performance and market performance."}, "25": {"documentation": {"title": "Heavy-hadron molecules from light-meson-exchange saturation", "source": "Fang-Zheng Peng, Ming-Zhu Liu, Mario S\\'anchez S\\'anchez, Manuel Pavon\n  Valderrama", "docs_id": "2004.05658", "section": ["hep-ph", "hep-ex", "hep-lat", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Heavy-hadron molecules from light-meson-exchange saturation. In the effective field theory framework the interaction between two heavy hadrons can be decomposed into a long- and a short-range piece. The long-range piece corresponds to the one-pion-exchange potential and is relatively well-known. The short-range piece is given by a series of contact-range interactions with unknown couplings, which substitute the less well-known short-range dynamics. While the general structure of the short-range potential between heavy hadrons is heavily constrained from heavy-quark symmetry, the couplings are still free parameters. Here we argue that the relative strength and the sign of these couplings can be estimated from the hypothesis that they are saturated by the exchange of light mesons, in particular the vector mesons $\\rho$ and $\\omega$, i.e. from resonance saturation. However, we propose a novel saturation procedure that effectively removes form-factor artifacts. From this we can determine in which spin and isospin configurations the low-energy constants are most attractive for specific two-heavy-hadron systems. In general the molecular states with lower isospins and higher spins will be more attractive and thus more probable candidates to form heavy-hadron molecules. This pattern is compatible with the interpretation of the $X(3872)$ and $P_c(4312/4440/4457)$ as molecular states, but it is not applicable to states with maximum isospin like the $Z_c(3900/4020)$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the effective field theory framework, the interaction between two heavy hadrons can be decomposed into a long- and a short-range piece. The long-range piece corresponds to the one-pion-exchange potential, which is relatively well-known. However, the short-range piece is given by a series of contact-range interactions with unknown couplings. The authors of the paper propose a novel saturation procedure to estimate the relative strength and sign of these couplings, which is based on the hypothesis that they are saturated by the exchange of light mesons, particularly the vector mesons \u03c1 and \u03c9. According to this procedure, which of the following spin and isospin configurations are most attractive for forming heavy-hadron molecules, and why?\n\nA) High spin and high isospin configurations, as they would lead to a stronger attractive potential.\nB) Low spin and low isospin configurations, as they would lead to a weaker attractive potential.\nC) High spin and low isospin configurations, as they would lead to a stronger attractive potential due to the exchange of vector mesons.\nD) Low spin and high isospin configurations, as they would lead to a weaker attractive potential due to the exchange of vector mesons.\n\nCorrect Answer: C) High spin and low isospin configurations, as they would lead to a stronger attractive potential due to the exchange of vector mesons.\n\nExplanation: The authors of the paper argue that the low-energy constants are most attractive for specific two-heavy-hadron systems when the spin and isospin configurations are such that the exchange of vector mesons (\u03c1 and \u03c9) is most effective. This is because the exchange of vector mesons leads to a stronger attractive potential, which is in line with the interpretation of the X(3872) and P_c(4312/4440/4457) as molecular states. Therefore, high spin and low isospin configurations are most attractive for forming heavy-hadron molecules."}, "26": {"documentation": {"title": "${\\bar\\nu}_l$ induced pion production from nuclei at $\\sim$1 GeV", "source": "M. Rafi Alam, S. Chauhan, M. Sajjad Athar and S. K. Singh", "docs_id": "1310.7704", "section": ["nucl-th", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "${\\bar\\nu}_l$ induced pion production from nuclei at $\\sim$1 GeV. We have studied charged current $\\bar\\nu_l$ induced one pion production from $^{12}$C and $^{16}$O nuclear targets at MiniBooNE and atmospheric antineutrino energies. The calculations have been done for the incoherent pion production process as well as for the pions coming from the hyperons in the quasielastic production of $\\Lambda$ and $\\Sigma$. The calculations are done in the local density approximation. For the inelastic processes the calculations have been done in the $\\Delta$ dominance model and we take into account the effect of Pauli blocking, Fermi motion of the nucleon and renormalization of $\\Delta$ properties in the nuclear medium. The effect of final state interaction(FSI) of pions is also taken into account. For the hyperon production, the nuclear medium effects due to Fermi motion and FSI effects due to hyperon-nucleon scattering have been taken into account. These results may be quite useful in the analysis of SciBooNE, MicroBooNE, MINER$\\nu$A, and ArgoNeuT experiments when the pion analysis is done by using antineutrino beams."}, "answer": "Here is the difficult multi-form question:\n\nWhat are the key features of the calculations for ${\\bar\\nu}_l$ induced pion production from nuclei at $\\sim$1 GeV, as described in the provided Arxiv documentation?\n\nA) The calculations are done in the local density approximation, but neglect the effect of Pauli blocking and Fermi motion of the nucleon.\nB) The calculations are done in the $\\Delta$ dominance model, but do not take into account the effect of final state interaction (FSI) of pions.\nC) The calculations are done in the local density approximation, and take into account the effect of Pauli blocking, Fermi motion of the nucleon, and renormalization of $\\Delta$ properties in the nuclear medium, as well as FSI effects due to hyperon-nucleon scattering.\nD) The calculations are done in the $\\Delta$ dominance model, but neglect the effect of Fermi motion of the nucleon and renormalization of $\\Delta$ properties in the nuclear medium.\n\nCorrect Answer: C) The calculations are done in the local density approximation, and take into account the effect of Pauli blocking, Fermi motion of the nucleon, and renormalization of $\\Delta$ properties in the nuclear medium, as well as FSI effects due to hyperon-nucleon scattering.\n\nExplanation: The correct answer is C) because the documentation states that the calculations are done in the local density approximation, and take into account the effect of Pauli blocking, Fermi motion of the nucleon, and renormalization of $\\Delta$ properties in the nuclear medium, as well as FSI effects due to hyperon-nucleon scattering. This is a key feature of the calculations, and is mentioned explicitly in the documentation. The other options are incorrect because they either omit or misrepresent one or more of these key features."}, "27": {"documentation": {"title": "A framework for the local information dynamics of distributed\n  computation in complex systems", "source": "Joseph T. Lizier, Mikhail Prokopenko, Albert Y. Zomaya", "docs_id": "0811.2690", "section": ["nlin.CG", "cs.IT", "math.IT", "nlin.AO", "nlin.PS", "physics.data-an"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A framework for the local information dynamics of distributed\n  computation in complex systems. The nature of distributed computation has often been described in terms of the component operations of universal computation: information storage, transfer and modification. We review the first complete framework that quantifies each of these individual information dynamics on a local scale within a system, and describes the manner in which they interact to create non-trivial computation where \"the whole is greater than the sum of the parts\". We describe the application of the framework to cellular automata, a simple yet powerful model of distributed computation. This is an important application, because the framework is the first to provide quantitative evidence for several important conjectures about distributed computation in cellular automata: that blinkers embody information storage, particles are information transfer agents, and particle collisions are information modification events. The framework is also shown to contrast the computations conducted by several well-known cellular automata, highlighting the importance of information coherence in complex computation. The results reviewed here provide important quantitative insights into the fundamental nature of distributed computation and the dynamics of complex systems, as well as impetus for the framework to be applied to the analysis and design of other systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary contribution of the framework described in the Arxiv documentation, and how does it relate to the analysis of complex systems?\n\nA) The framework provides a new mathematical model for describing the behavior of complex systems, but does not provide any quantitative evidence for the nature of distributed computation.\nB) The framework provides a framework for analyzing the information dynamics of distributed computation in complex systems, and provides quantitative evidence for several important conjectures about distributed computation in cellular automata.\nC) The framework is a new type of cellular automaton that is capable of performing complex computations, but does not provide any insights into the fundamental nature of distributed computation.\nD) The framework is a tool for designing new complex systems, but does not provide any quantitative evidence for the nature of distributed computation.\n\nCorrect Answer: B) The framework provides a framework for analyzing the information dynamics of distributed computation in complex systems, and provides quantitative evidence for several important conjectures about distributed computation in cellular automata.\n\nExplanation: The correct answer is B) because the framework is described as providing a complete framework for quantifying the individual information dynamics of distributed computation on a local scale within a system, and providing quantitative evidence for several important conjectures about distributed computation in cellular automata. The framework is also shown to contrast the computations conducted by several well-known cellular automata, highlighting the importance of information coherence in complex computation. This suggests that the primary contribution of the framework is to provide a framework for analyzing the information dynamics of distributed computation in complex systems, and providing quantitative evidence for the nature of distributed computation."}, "28": {"documentation": {"title": "The Development of Equilibrium After Preheating", "source": "Gary Felder & Lev Kofman", "docs_id": "hep-ph/0011160", "section": ["hep-ph", "astro-ph", "gr-qc", "hep-lat", "hep-th", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Development of Equilibrium After Preheating. We present a fully nonlinear study of the development of equilibrium after preheating. Preheating is the exponentially rapid transfer of energy from the nearly homogeneous inflaton field to fluctuations of other fields and/or the inflaton itself. This rapid transfer leaves these fields in a highly nonthermal state with energy concentrated in infrared modes. We have performed lattice simulations of the evolution of interacting scalar fields during and after preheating for a variety of inflationary models. We have formulated a set of generic rules that govern the thermalization process in all of these models. Notably, we see that once one of the fields is amplified through parametric resonance or other mechanisms it rapidly excites other coupled fields to exponentially large occupation numbers. These fields quickly acquire nearly thermal spectra in the infrared, which gradually propagates into higher momenta. Prior to the formation of total equilibrium, the excited fields group into subsets with almost identical characteristics (e.g. group effective temperature). The way fields form into these groups and the properties of the groups depend on the couplings between them. We also studied the onset of chaos after preheating by calculating the Lyapunov exponent of the scalar fields."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the development of equilibrium after preheating, what is the primary mechanism by which the excited fields acquire nearly thermal spectra in the infrared, and how does this process relate to the formation of groups with almost identical characteristics?\n\n**A)** Parametric resonance amplifies one field, which then rapidly excites other coupled fields to exponentially large occupation numbers, leading to the formation of groups with distinct properties.\n\n**B)** The thermalization process is driven by the coupling between fields, resulting in the formation of groups with similar characteristics, such as group effective temperature, through a process of statistical equilibrium.\n\n**C)** The Lyapunov exponent of the scalar fields is used to study the onset of chaos after preheating, and the excited fields acquire nearly thermal spectra in the infrared through a process of parametric resonance.\n\n**D)** The formation of groups with similar characteristics is a result of the parametric resonance amplification of one field, which then rapidly excites other coupled fields to exponentially large occupation numbers, leading to the formation of groups with distinct properties.\n\n**Correct Answer:** B) The thermalization process is driven by the coupling between fields, resulting in the formation of groups with similar characteristics, such as group effective temperature, through a process of statistical equilibrium.\n\n**Explanation:** The correct answer is B) because the documentation states that \"once one of the fields is amplified through parametric resonance or other mechanisms it rapidly excites other coupled fields to exponentially large occupation numbers.\" This process leads to the formation of groups with similar characteristics, such as group effective temperature, through a process of statistical equilibrium. The other options are incorrect because they either misrepresent the primary mechanism of thermalization (A and D) or focus on a different aspect of the process (C)."}, "29": {"documentation": {"title": "Low-loss single-mode hybrid-lattice hollow-core photonic crystal fiber", "source": "Foued Amrani, Jonas H. Os\\'orio, Fr\\'ed\\'eric Delahaye, Fabio\n  Giovanardi, Luca Vincetti, Beno\\^it Debord, Fr\\'ed\\'eric G\\'er\\^ome, Fetah\n  Benabid", "docs_id": "2006.06375", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Low-loss single-mode hybrid-lattice hollow-core photonic crystal fiber. The remarkable recent demonstrations in ultralow loss Inhibited-Coupling (IC) hollow-core photonic crystal fibers (HCPCFs) place them as serious candidates for the next-generation of long-haul fiber optics systems. A hindrance to this prospect, but also to short-haul applications such as micromachining, where stable and high-quality beam delivery is needed, is the challenge to design and fabricate an IC-guiding fiber that combines ultra-low loss, truly and robust single-modeness, and polarization-maintaining operation. Design solutions proposed up to now require a trade-off between low loss and truly single modeness. Here, we propose a novel concept of IC HCPCF for obtaining low-loss and effective single-mode operation. The fiber is endowed with a hybrid cladding composed of a Kagome-tubular lattice (HKT). This new concept of microstructured cladding allows to significantly reduce confinement loss and, at the same time, preserving a truly and robust single-mode operation. Experimental results show a HKT-IC-HCPCF with a minimum loss figure of 1.6 dB/km at 1050 nm and a higher-order modes extinction ratio as high as 47.0 dB for a 10 m long fiber. The robustness of the fiber single-modeness was tested by moving the fiber and varying the coupling conditions. The design proposed herein opens a new route for the accomplishment of HCPCFs that combine robust ultralow loss transmission and single-mode beam delivery and provides new insight into the understanding of IC guidance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary challenge in designing and fabricating Inhibited-Coupling (IC) hollow-core photonic crystal fibers (HCPCFs) for long-haul fiber optics systems and short-haul applications such as micromachining?\n\nA) Achieving ultra-low loss while maintaining polarization-maintaining operation\nB) Ensuring robust single-mode operation while minimizing confinement loss\nC) Overcoming the trade-off between low loss and truly single modeness\nD) Scaling up the fiber's diameter to increase its transmission capacity\n\nCorrect Answer: C) Overcoming the trade-off between low loss and truly single modeness\n\nExplanation: The question requires the test-taker to understand the main challenge in designing and fabricating IC HCPCFs. The correct answer, C) Overcoming the trade-off between low loss and truly single modeness, highlights the difficulty in achieving both ultra-low loss and robust single-mode operation simultaneously. The other options, while related to the topic, do not accurately capture the primary challenge. Option A is incorrect because polarization-maintaining operation is not the primary challenge. Option B is incorrect because confinement loss is not the primary concern. Option D is incorrect because scaling up the fiber's diameter is not the main issue."}, "30": {"documentation": {"title": "Wikipedia and Digital Currencies: Interplay Between Collective Attention\n  and Market Performance", "source": "Abeer ElBahrawy, Laura Alessandretti, Andrea Baronchelli", "docs_id": "1902.04517", "section": ["physics.soc-ph", "cs.SI", "q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Wikipedia and Digital Currencies: Interplay Between Collective Attention\n  and Market Performance. The production and consumption of information about Bitcoin and other digital-, or 'crypto'-, currencies have grown together with their market capitalisation. However, a systematic investigation of the relationship between online attention and market dynamics, across multiple digital currencies, is still lacking. Here, we quantify the interplay between the attention towards digital currencies in Wikipedia and their market performance. We consider the entire edit history of currency-related pages, and their view history from July 2015. First, we quantify the evolution of the cryptocurrency presence in Wikipedia by analysing the editorial activity and the network of co-edited pages. We find that a small community of tightly connected editors is responsible for most of the production of information about cryptocurrencies in Wikipedia. Then, we show that a simple trading strategy informed by Wikipedia views performs better, in terms of returns on investment, than classic baseline strategies for most of the covered period. Our results contribute to the recent literature on the interplay between online information and investment markets, and we anticipate it will be of interest for researchers as well as investors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary finding of the study on the interplay between online attention and market performance of digital currencies, as reported in the provided documentation?\n\nA) A simple trading strategy informed by Wikipedia views performs worse than classic baseline strategies for most of the covered period.\nB) The production and consumption of information about digital currencies have no significant impact on their market capitalization.\nC) A small community of tightly connected editors is responsible for most of the production of information about cryptocurrencies in Wikipedia.\nD) The study found no correlation between online attention and market performance of digital currencies.\n\nCorrect Answer: C) A small community of tightly connected editors is responsible for most of the production of information about cryptocurrencies in Wikipedia.\n\nExplanation: The study quantifies the evolution of the cryptocurrency presence in Wikipedia by analyzing the editorial activity and the network of co-edited pages, and finds that a small community of tightly connected editors is responsible for most of the production of information about cryptocurrencies in Wikipedia. This suggests that a small group of influential individuals or organizations are driving the creation and dissemination of information about digital currencies, which can have an impact on market performance."}, "31": {"documentation": {"title": "Nuclear Dynamics and Reactions in the Ab Initio Symmetry-Adapted\n  Framework", "source": "Kristina D. Launey, Alexis Mercenne, and Tomas Dytrych", "docs_id": "2108.04894", "section": ["nucl-th", "astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nuclear Dynamics and Reactions in the Ab Initio Symmetry-Adapted\n  Framework. We review the ab initio symmetry-adapted (SA) framework for determining the structure of stable and unstable nuclei, along with related electroweak, decay and reaction processes. This framework utilizes the dominant symmetry of nuclear dynamics, the shape-related symplectic Sp(3,R) symmetry, which has been shown to emerge from first principles and to expose dominant degrees of freedom that are collective in nature, even in the lightest species or seemingly spherical states. This feature is illustrated for a broad scope of nuclei ranging from helium to titanium isotopes, enabled by recent developments of the ab initio symmetry-adapted no-core shell model expanded to the continuum through the use of the SA basis and that of the resonating group method. The review focuses on energies, electromagnetic transitions, quadrupole and magnetic moments, radii, form factors, and response function moments, for ground-state rotational bands and giant resonances. The method also determines the structure of reaction fragments that is used to calculate decay widths and alpha-capture reactions for simulated x-ray burst abundance patterns, as well as nucleon-nucleus interactions for cross sections and other reaction observables."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary symmetry that emerges from first principles in the ab initio symmetry-adapted (SA) framework for determining the structure of stable and unstable nuclei, and how does it expose dominant degrees of freedom that are collective in nature?\n\nA) The orthogonal symmetry Sp(2,R)\nB) The shape-related symplectic Sp(3,R) symmetry\nC) The unitary symmetry SU(2)\nD) The Lorentz symmetry\n\nCorrect Answer: B) The shape-related symplectic Sp(3,R) symmetry\n\nExplanation: The ab initio symmetry-adapted (SA) framework utilizes the dominant symmetry of nuclear dynamics, which is the shape-related symplectic Sp(3,R) symmetry. This symmetry has been shown to emerge from first principles and exposes dominant degrees of freedom that are collective in nature, even in the lightest species or seemingly spherical states. This symmetry is a key feature of the SA framework and is used to determine various nuclear properties, such as energies, electromagnetic transitions, and radii."}, "32": {"documentation": {"title": "Field-theory calculation of the electric dipole moment of the neutron\n  and paramagnetic atoms", "source": "S.A. Blundell, J. Griffith, and J. Sapirstein", "docs_id": "1205.2341", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Field-theory calculation of the electric dipole moment of the neutron\n  and paramagnetic atoms. Electric dipole moments (edms) of bound states that arise from the constituents having edms are studied with field-theoretic techniques. The systems treated are the neutron and a set of paramagnetic atoms. In the latter case it is well known that the atomic edm differs greatly from the electron edm when the internal electric fields of the atom are taken into account. In the nonrelativistic limit these fields lead to a complete suppression, but for heavy atoms large enhancement factors are present. A general bound-state field theory approach applicable to both the neutron and paramagnetic atoms is set up. It is applied first to the neutron, treating the quarks as moving freely in a confining spherical well. It is shown that the effect of internal electric fields is small in this case. The atomic problem is then revisited using field-theory techniques in place of the usual Hamiltonian methods, and the atomic enhancement factor is shown to be consistent with previous calculations. Possible application of bound-state techniques to other sources of the neutron edm is discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the electric dipole moment (EDM) calculation of the neutron and paramagnetic atoms, what is the general bound-state field theory approach that is set up to treat both systems, and how does it account for the internal electric fields of the atom?\n\n**A)** The approach is based on the Dirac equation, which includes the effects of internal electric fields, and is applied to both the neutron and paramagnetic atoms using a confining spherical well potential.\n\n**B)** The approach is based on the non-relativistic Schr\u00f6dinger equation, which neglects the effects of internal electric fields, and is applied to the neutron using a confining spherical well potential.\n\n**C)** The approach is based on the Feynman path integral, which includes the effects of internal electric fields, and is applied to both the neutron and paramagnetic atoms using a non-relativistic Hamiltonian.\n\n**D)** The approach is based on the Hartree-Fock method, which neglects the effects of internal electric fields, and is applied to the neutron using a relativistic Dirac equation.\n\n**Correct Answer:** A) The approach is based on the Dirac equation, which includes the effects of internal electric fields, and is applied to both the neutron and paramagnetic atoms using a confining spherical well potential.\n\n**Explanation:** The correct answer is A) because the documentation states that a general bound-state field theory approach is set up to treat both the neutron and paramagnetic atoms, and that this approach includes the effects of internal electric fields. The confining spherical well potential is also mentioned as being used to treat the neutron. The other options are incorrect because they either neglect the effects of internal electric fields (B and D) or use a non-relativistic Hamiltonian (C), which is not consistent with the documentation."}, "33": {"documentation": {"title": "Modelling an Ammonium Transporter with SCLS", "source": "Mario Coppo (Dipartimento di Informatica, Universit\\'a di Torino),\n  Ferruccio Damiani (Dipartimento di Informatica, Universit\\'a di Torino),\n  Elena Grassi (Molecular Biotechnology Center, Dipartimento di Genetica,\n  Biologia e Biochimica and Dipartimento di Informatica, Universit\\'a di\n  Torino), Mike Guether (Dipartimento di Biologia Vegetale, Universit\\`a di\n  Torino), Angelo Troina (Dipartimento di Informatica, Universit\\'a di Torino)", "docs_id": "0910.1418", "section": ["q-bio.QM", "cs.CE", "q-bio.CB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modelling an Ammonium Transporter with SCLS. The Stochastic Calculus of Looping Sequences (SCLS) is a recently proposed modelling language for the representation and simulation of biological systems behaviour. It has been designed with the aim of combining the simplicity of notation of rewrite systems with the advantage of compositionality. It also allows a rather simple and accurate description of biological membranes and their interactions with the environment. In this work we apply SCLS to model a newly discovered ammonium transporter. This transporter is believed to play a fundamental role for plant mineral acquisition, which takes place in the arbuscular mycorrhiza, the most wide-spread plant-fungus symbiosis on earth. Due to its potential application in agriculture this kind of symbiosis is one of the main focuses of the BioBITs project. In our experiments the passage of NH3 / NH4+ from the fungus to the plant has been dissected in known and hypothetical mechanisms; with the model so far we have been able to simulate the behaviour of the system under different conditions. Our simulations confirmed some of the latest experimental results about the LjAMT2;2 transporter. The initial simulation results of the modelling of the symbiosis process are promising and indicate new directions for biological investigations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Modelling an Ammonium Transporter with SCLS\" propose a novel approach to modelling biological systems using the Stochastic Calculus of Looping Sequences (SCLS). What are the two main advantages of SCLS mentioned in the paper, and how do they relate to the representation and simulation of biological systems behavior?\n\n**A)** SCLS combines the simplicity of notation of rewrite systems with the advantage of compositionality, allowing for a simple and accurate description of biological membranes and their interactions with the environment.\n\n**B)** SCLS is a deterministic modelling language that relies on complex mathematical equations to simulate biological systems, whereas the authors propose a probabilistic approach to model the behaviour of biological systems.\n\n**C)** SCLS is a graphical modelling language that uses visual representations to describe the interactions between biological molecules, whereas the authors propose a text-based approach to model the behaviour of biological systems.\n\n**D)** SCLS is a deterministic modelling language that relies on complex mathematical equations to simulate biological systems, whereas the authors propose a simplified approach to model the behaviour of biological systems using a set of predefined rules.\n\n**Correct Answer:** A) SCLS combines the simplicity of notation of rewrite systems with the advantage of compositionality, allowing for a simple and accurate description of biological membranes and their interactions with the environment.\n\n**Explanation:** The correct answer is A) because the paper explicitly states that SCLS combines the simplicity of notation of rewrite systems with the advantage of compositionality, allowing for a simple and accurate description of biological membranes and their interactions with the environment. This is mentioned in the introduction of the paper as one of the key design goals of SCLS. The other options are incorrect because they either misrepresent the nature of SCLS (options B and C) or are not mentioned in the paper (option D)."}, "34": {"documentation": {"title": "On a Low-Frequency and Contrast Stabilized Full-Wave Volume Integral\n  Equation Solver for Lossy Media", "source": "Cl\\'ement Henry, Adrien Merlini, Lyes Rahmouni and Francesco P.\n  Andriulli", "docs_id": "2108.10690", "section": ["eess.IV", "physics.med-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On a Low-Frequency and Contrast Stabilized Full-Wave Volume Integral\n  Equation Solver for Lossy Media. In this paper we present a new regularized electric flux volume integral equation (D-VIE) for modeling high-contrast conductive dielectric objects in a broad frequency range. This new formulation is particularly suitable for modeling biological tissues at low frequencies, as it is required by brain epileptogenic area imaging, but also at higher ones, as it is required by several applications including, but not limited to, transcranial magnetic and deep brain stimulation (TMS and DBS, respectively). When modeling inhomogeneous objects with high complex permittivities at low frequencies, the traditional D-VIE is ill-conditioned and suffers from numerical instabilities that result in slower convergence and in less accurate solutions. In this work we address these shortcomings by leveraging a new set of volume quasi-Helmholtz projectors. Their scaling by the material permittivity matrix allows for the re-balancing of the equation when applied to inhomogeneous scatterers and thereby makes the proposed method accurate and stable even for high complex permittivity objects until arbitrarily low frequencies. Numerical results, canonical and realistic, corroborate the theory and confirm the stability and the accuracy of this new method both in the quasi-static regime and at higher frequencies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a new regularized electric flux volume integral equation (D-VIE) for modeling high-contrast conductive dielectric objects in a broad frequency range, as presented in the paper \"On a Low-Frequency and Contrast Stabilized Full-Wave Volume Integral Equation Solver for Lossy Media\"?\n\n**A)** To improve the accuracy of solutions for inhomogeneous objects with high complex permittivities at low frequencies.\n**B)** To enhance the stability of the traditional D-VIE for modeling biological tissues at low frequencies.\n**C)** To develop a method for modeling high-contrast conductive dielectric objects in a broad frequency range, particularly suitable for applications such as transcranial magnetic and deep brain stimulation (TMS and DBS, respectively).\n**D)** To reduce the computational complexity of the D-VIE for lossy media.\n\n**Correct Answer:** C) To develop a method for modeling high-contrast conductive dielectric objects in a broad frequency range, particularly suitable for applications such as transcranial magnetic and deep brain stimulation (TMS and DBS, respectively).\n\n**Explanation:** The correct answer is C) because the paper explicitly states that the new D-VIE is \"particularly suitable for modeling biological tissues at low frequencies, as it is required by brain epileptogenic area imaging, but also at higher ones, as it is required by several applications including, but not limited to, transcranial magnetic and deep brain stimulation (TMS and DBS, respectively)\". This indicates that the primary motivation behind the development of the new D-VIE is to create a method for modeling high-contrast conductive dielectric objects in a broad frequency range, with a focus on applications such as TMS and DBS."}, "35": {"documentation": {"title": "Dual representations for systemic risk measures", "source": "\\c{C}a\\u{g}{\\i}n Ararat, Birgit Rudloff", "docs_id": "1607.03430", "section": ["q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dual representations for systemic risk measures. The financial crisis showed the importance of measuring, allocating and regulating systemic risk. Recently, the systemic risk measures that can be decomposed into an aggregation function and a scalar measure of risk, received a lot of attention. In this framework, capital allocations are added after aggregation and can represent bailout costs. More recently, a framework has been introduced, where institutions are supplied with capital allocations before aggregation. This yields an interpretation that is particularly useful for regulatory purposes. In each framework, the set of all feasible capital allocations leads to a multivariate risk measure. In this paper, we present dual representations for scalar systemic risk measures as well as for the corresponding multivariate risk measures concerning capital allocations. Our results cover both frameworks: aggregating after allocating and allocating after aggregation. As examples, we consider the aggregation mechanisms of the Eisenberg-Noe model as well as those of the resource allocation and network flow models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the framework where institutions are supplied with capital allocations before aggregation, as opposed to the framework where capital allocations are added after aggregation?\n\n**A)** It allows for more accurate risk assessment of individual institutions.\n**B)** It enables regulatory bodies to better manage bailout costs.\n**C)** It provides a more comprehensive view of systemic risk by considering the interactions between institutions.\n**D)** It simplifies the calculation of capital allocations.\n\n**Correct Answer:** B) It enables regulatory bodies to better manage bailout costs.\n\n**Explanation:** The correct answer is B) It enables regulatory bodies to better manage bailout costs. This is because, in the framework where institutions are supplied with capital allocations before aggregation, the set of all feasible capital allocations leads to a multivariate risk measure that can represent bailout costs. This interpretation is particularly useful for regulatory purposes, as it allows regulatory bodies to better manage the costs associated with bailouts. The other options are incorrect because they do not accurately capture the primary advantage of this framework."}, "36": {"documentation": {"title": "Non-blind catalogue of extragalactic point sources from the Wilkinson\n  Microwave Anisotropy Probe (WMAP) first 3--year survey data", "source": "M. Lopez-Caniego, J. Gonzalez-Nuevo, D. Herranz, M. Massardi, J.L.\n  Sanz, G. De Zotti, L. Toffolatti, F. Argueso", "docs_id": "astro-ph/0701473", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-blind catalogue of extragalactic point sources from the Wilkinson\n  Microwave Anisotropy Probe (WMAP) first 3--year survey data. We have used the MHW2 filter to obtain estimates of the flux densities at the WMAP frequencies of a complete sample of 2491 sources, mostly brighter than 500 mJy at 5 GHz, distributed over the whole sky excluding a strip around the Galactic equator (b < 5 degrees). After having detected 933 sources above the 3 sigma level in the MHW2 filtered maps - our New Extragalactic WMAP Point Source (NEWPS_3sigma) Catalogue - we are left with 381 sources above 5 sigma in at least one WMAP channel, 369 of which constitute our NEWPS_5sigma catalogue. It is remarkable to note that 98 (i.e. 26%) sources detected above 5 sigma are `new', they are not present in the WMAP catalogue. Source fluxes have been corrected for the Eddington bias. Our flux density estimates before such correction are generally in good agreement with the WMAP ones at 23 GHz. At higher frequencies WMAP fluxes tend to be slightly higher than ours, probably because WMAP estimates neglect the deviations of the point spread function from a Gaussian shape. On the whole, above the estimated completeness limit of 1.1 Jy at 23 GHz we detected 43 sources missed by the blind method adopted by the WMAP team. On the other hand, our low-frequency selection threshold left out 25 WMAP sources, only 12 of which, however, are 5 sigma detections and only 3 have fluxes S at 23 GHz > 1.1 Jy. Thus, our approach proved to be competitive with, and complementary to the WMAP one."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The authors of the paper used the MHW2 filter to obtain estimates of the flux densities of extragalactic point sources from the WMAP first 3-year survey data. What is the significance of the fact that 98 (26%) of the sources detected above 5 sigma in at least one WMAP channel are \"new\" and not present in the original WMAP catalogue?\n\nA) The authors were able to detect more sources than the WMAP team due to a more sensitive filter.\nB) The authors were able to detect sources that were missed by the WMAP team due to a different point spread function.\nC) The authors were able to detect sources that were present in the WMAP catalogue but were not detected by the blind method adopted by the WMAP team.\nD) The authors were able to detect sources that were present in the WMAP catalogue but were not detected by the WMAP team due to the Eddington bias correction.\n\n**Correct Answer:** D) The authors were able to detect sources that were present in the WMAP catalogue but were not detected by the WMAP team due to the Eddington bias correction.\n\n**Explanation:** The correct answer is D) because the text states that \"98 (i.e. 26%) sources detected above 5 sigma are 'new', they are not present in the WMAP catalogue.\" This implies that the Eddington bias correction allowed the authors to detect sources that were present in the WMAP catalogue but were not detected by the WMAP team. The other options are incorrect because they do not accurately reflect the information provided in the text."}, "37": {"documentation": {"title": "Preferences Yielding the \"Precautionary Effect\"", "source": "Michel De Lara (CERMICS)", "docs_id": "0907.4093", "section": ["q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Preferences Yielding the \"Precautionary Effect\". Consider an agent taking two successive decisions to maximize his expected utility under uncertainty. After his first decision, a signal is revealed that provides information about the state of nature. The observation of the signal allows the decision-maker to revise his prior and the second decision is taken accordingly. Assuming that the first decision is a scalar representing consumption, the \\emph{precautionary effect} holds when initial consumption is less in the prospect of future information than without (no signal). \\citeauthor{Epstein1980:decision} in \\citep*{Epstein1980:decision} has provided the most operative tool to exhibit the precautionary effect. Epstein's Theorem holds true when the difference of two convex functions is either convex or concave, which is not a straightforward property, and which is difficult to connect to the primitives of the economic model. Our main contribution consists in giving a geometric characterization of when the difference of two convex functions is convex, then in relating this to the primitive utility model. With this tool, we are able to study and unite a large body of the literature on the precautionary effect."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider an agent taking two successive decisions to maximize his expected utility under uncertainty. After his first decision, a signal is revealed that provides information about the state of nature. The observation of the signal allows the decision-maker to revise his prior and the second decision is taken accordingly. Assuming that the first decision is a scalar representing consumption, the precautionary effect holds when initial consumption is less in the prospect of future information than without (no signal). According to Epstein's Theorem, what is the necessary and sufficient condition for the precautionary effect to hold?\n\nA) The difference of two concave functions is convex.\nB) The difference of two convex functions is concave.\nC) The difference of two convex functions is convex, and the second decision is taken with respect to the revised prior.\nD) The difference of two convex functions is convex, and the first decision is taken with respect to the initial prior.\n\n**Correct Answer:** B) The difference of two convex functions is concave.\n\n**Explanation:** Epstein's Theorem states that the precautionary effect holds when the difference of two convex functions is either convex or concave. In this case, we are interested in the condition for the precautionary effect to hold, which is when the difference of two convex functions is concave. This is because the precautionary effect holds when initial consumption is less in the prospect of future information than without (no signal), which is equivalent to saying that the difference of two convex functions is concave. The other options are incorrect because they either describe a different condition (A) or are incomplete (C and D)."}, "38": {"documentation": {"title": "Forecasting of Jump Arrivals in Stock Prices: New Attention-based\n  Network Architecture using Limit Order Book Data", "source": "Ymir M\\\"akinen, Juho Kanniainen, Moncef Gabbouj, Alexandros Iosifidis", "docs_id": "1810.10845", "section": ["q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Forecasting of Jump Arrivals in Stock Prices: New Attention-based\n  Network Architecture using Limit Order Book Data. The existing literature provides evidence that limit order book data can be used to predict short-term price movements in stock markets. This paper proposes a new neural network architecture for predicting return jump arrivals in equity markets with high-frequency limit order book data. This new architecture, based on Convolutional Long Short-Term Memory with Attention, is introduced to apply time series representation learning with memory and to focus the prediction attention on the most important features to improve performance. The data set consists of order book data on five liquid U.S. stocks. The use of the attention mechanism makes it possible to analyze the importance of the inclusion limit order book data and other input variables. By using this mechanism, we provide evidence that the use of limit order book data was found to improve the performance of the proposed model in jump prediction, either clearly or marginally, depending on the underlying stock. This suggests that path-dependence in limit order book markets is a stock specific feature. Moreover, we find that the proposed approach with an attention mechanism outperforms the multi-layer perceptron network as well as the convolutional neural network and Long Short-Term memory model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the proposed neural network architecture in the paper \"Forecasting of Jump Arrivals in Stock Prices: New Attention-based Network Architecture using Limit Order Book Data\"?\n\nA) The use of a multi-layer perceptron network to predict short-term price movements in stock markets.\nB) The application of a convolutional neural network to analyze the importance of limit order book data and other input variables.\nC) The introduction of a new architecture that combines Convolutional Long Short-Term Memory with Attention to improve time series representation learning and focus prediction attention on the most important features.\nD) The development of a new dataset consisting of order book data on five liquid U.S. stocks.\n\n**Correct Answer:** C) The introduction of a new architecture that combines Convolutional Long Short-Term Memory with Attention to improve time series representation learning and focus prediction attention on the most important features.\n\n**Explanation:** The correct answer is C) because the paper proposes a new neural network architecture that combines Convolutional Long Short-Term Memory (LSTM) with Attention to improve time series representation learning and focus prediction attention on the most important features. This architecture is the primary contribution of the paper, as it allows for more accurate prediction of return jump arrivals in equity markets. The other options are incorrect because they either describe existing approaches (A and B) or a different aspect of the paper (D)."}, "39": {"documentation": {"title": "Evaluation of the duty ratio of bacterial flagellar motor by a dynamic\n  load control", "source": "Kento Sato, Shuichi Nakamura, Seishi Kudo, Shoichi Toyabe", "docs_id": "1806.06470", "section": ["physics.bio-ph", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evaluation of the duty ratio of bacterial flagellar motor by a dynamic\n  load control. Bacterial flagellar motor is one of the most complex and sophisticated nano machineries in nature. A duty ratio $D$ is a fraction of time that the stator and the rotor interact and is a fundamental property to characterize the motor but remains to be determined. It is known that the stator units of the motor bind to and dissociate from the motor dynamically to control the motor torque depending on the load on the motor. At low load where the kinetics such as a proton translocation speed limits the rotation rate, the dependency of the rotation rate on the number of stator units $N$ infers $D$; the dependency becomes larger for smaller $D$. Contradicting observations supporting both the small and large $D$ have been reported. A dilemma is that it is difficult to explore a broad range of $N$ at low load because the stator units easily dissociate, and $N$ is limited to one or two at vanishing load. Here, we develop an electrorotation method to dynamically control the load on the flagellar motor of {\\it Salmonella} with a calibrated magnitude of the torque. By instantly reducing the load for keeping $N$ high, we observed that the speed at low load depends on $N$, implying a small duty ratio. We recovered the torque-speed curves of individual motors and evaluated the duty ratio to be $0.14 \\pm 0.04$ from the correlation between the torque at high load and the rotation rate at low load."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study on the bacterial flagellar motor used an electrorotation method to dynamically control the load on the motor. What is the main conclusion drawn from their experiment regarding the duty ratio of the motor?\n\nA) The duty ratio is independent of the number of stator units and is constant for all conditions.\nB) The duty ratio is directly proportional to the number of stator units and increases with the load on the motor.\nC) The duty ratio is small and depends on the number of stator units, with a value of 0.14 \u00b1 0.04.\nD) The duty ratio is large and remains constant for all conditions, regardless of the number of stator units.\n\n**Correct Answer:** C) The duty ratio is small and depends on the number of stator units, with a value of 0.14 \u00b1 0.04.\n\n**Explanation:** The correct answer is based on the text, which states that the authors \"observed that the speed at low load depends on N, implying a small duty ratio\" and that they \"recovered the torque-speed curves of individual motors and evaluated the duty ratio to be 0.14 \u00b1 0.04\". This indicates that the duty ratio is indeed small and depends on the number of stator units."}, "40": {"documentation": {"title": "Graph Node-Feature Convolution for Representation Learning", "source": "Li Zhang, Heda Song, Haiping Lu", "docs_id": "1812.00086", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Graph Node-Feature Convolution for Representation Learning. Graph convolutional network (GCN) is an emerging neural network approach. It learns new representation of a node by aggregating feature vectors of all neighbors in the aggregation process without considering whether the neighbors or features are useful or not. Recent methods have improved solutions by sampling a fixed size set of neighbors, or assigning different weights to different neighbors in the aggregation process, but features within a feature vector are still treated equally in the aggregation process. In this paper, we introduce a new convolution operation on regular size feature maps constructed from features of a fixed node bandwidth via sampling to get the first-level node representation, which is then passed to a standard GCN to learn the second-level node representation. Experiments show that our method outperforms competing methods in semi-supervised node classification tasks. Furthermore, our method opens new doors for exploring new GCN architectures, particularly deeper GCN models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary limitation of traditional Graph Convolutional Networks (GCNs) in terms of representation learning, and how does the proposed method address this limitation?\n\nA) Traditional GCNs are limited by the inability to handle non-linear relationships between nodes and their neighbors, and the proposed method addresses this by using a different activation function. \nB) Traditional GCNs are limited by the fact that they treat all features within a feature vector equally, and the proposed method addresses this by sampling a fixed size set of neighbors and assigning different weights to different neighbors. \nC) Traditional GCNs are limited by the inability to handle multi-modal data, and the proposed method addresses this by using a different type of convolutional operation. \nD) Traditional GCNs are limited by the fact that they do not consider the utility of neighbors or features in the aggregation process, and the proposed method addresses this by introducing a new convolution operation that aggregates features of a fixed node bandwidth.\n\nCorrect Answer: D) Traditional GCNs are limited by the fact that they do not consider the utility of neighbors or features in the aggregation process, and the proposed method addresses this by introducing a new convolution operation that aggregates features of a fixed node bandwidth.\n\nExplanation: The correct answer is D) because the documentation states that traditional GCNs learn new representations of nodes by aggregating feature vectors of all neighbors without considering whether the neighbors or features are useful or not. The proposed method addresses this limitation by introducing a new convolution operation that aggregates features of a fixed node bandwidth, which allows for more efficient and effective representation learning."}, "41": {"documentation": {"title": "Relativistic Dynamics of Point Magnetic Moment", "source": "Johann Rafelski, Martin Formanek, and Andrew Steinmetz", "docs_id": "1712.01825", "section": ["physics.class-ph", "hep-ph", "physics.acc-ph", "physics.plasm-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Relativistic Dynamics of Point Magnetic Moment. The covariant motion of a classical point particle with magnetic moment in the presence of (external) electromagnetic fields is revisited. We are interested in understanding Lorentz force extension involving point particle magnetic moment (Stern-Gerlach force) and how the spin precession dynamics is modified for consistency. We introduce spin as a classical particle property inherent to Poincare\\'e symmetry of space-time. We propose a covariant formulation of the magnetic force based on a \\lq magnetic\\rq\\ 4-potential and show how the point particle magnetic moment relates to the Amperian (current loop) and Gilbertian (magnetic monopole) description. We show that covariant spin precession lacks a unique form and discuss connection to $g-2$ anomaly. We consider variational action principle and find that a consistent extension of Lorentz force to include magnetic spin force is not straightforward. We look at non-covariant particle dynamics, and present a short introduction to dynamics of (neutral) particles hit by a laser pulse of arbitrary shape."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the main challenge in extending the Lorentz force to include the magnetic spin force, as proposed in the Relativistic Dynamics of Point Magnetic Moment paper?\n\nA) The introduction of a new 4-potential that breaks Poincare symmetry\nB) The lack of a unique form for covariant spin precession dynamics\nC) The difficulty in reconciling the Amperian and Gilbertian descriptions of the magnetic force\nD) The need for a non-covariant particle dynamics approach to handle the spin precession\n\nCorrect Answer: B) The lack of a unique form for covariant spin precession dynamics\n\nExplanation: The paper highlights the challenge of finding a consistent extension of the Lorentz force to include the magnetic spin force, and specifically mentions that covariant spin precession lacks a unique form. This is a key point in the paper, and the correct answer requires an understanding of the underlying physics and the challenges discussed in the paper.\n\nNote: The other options are plausible but incorrect. Option A is related to the introduction of a new 4-potential, but it is not the main challenge. Option C is related to the reconciliation of different descriptions of the magnetic force, but it is not the main challenge. Option D is related to the need for a non-covariant approach, but it is not the main challenge."}, "42": {"documentation": {"title": "Modelling and predicting the effect of social distancing and travel\n  restrictions on COVID-19 spreading", "source": "Francesco Parino, Lorenzo Zino, Maurizio Porfiri, Alessandro Rizzo", "docs_id": "2010.05968", "section": ["physics.soc-ph", "math.DS", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modelling and predicting the effect of social distancing and travel\n  restrictions on COVID-19 spreading. To date, the only effective means to respond to the spreading of COVID-19 pandemic are non-pharmaceutical interventions (NPIs), which entail policies to reduce social activity and mobility restrictions. Quantifying their effect is difficult, but it is key to reduce their social and economical consequences. Here, we introduce a meta-population model based on temporal networks, calibrated on the COVID-19 outbreak data in Italy and apt to evaluate the outcomes of these two types of NPIs. Our approach combines the advantages of granular spatial modelling of meta-population models with the ability to realistically describe social contacts via activity-driven networks. We provide a valuable framework to assess the viability of different NPIs, varying with respect to their timing and severity. Results suggest that the effects of mobility restrictions largely depend on the possibility to implement timely NPIs in the early phases of the outbreak, whereas activity reduction policies should be prioritised afterwards."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of non-pharmaceutical interventions (NPIs) in responding to the COVID-19 pandemic, according to the study, and how does the proposed meta-population model address this limitation?\n\n**A)** NPIs are ineffective in reducing the spread of COVID-19 due to their limited spatial coverage, but the model provides a granular spatial approach to mitigate this issue.\n**B)** NPIs are difficult to quantify due to the complexity of social interactions, but the model uses activity-driven networks to realistically describe social contacts.\n**C)** NPIs are limited by their inability to target specific high-risk populations, but the model allows for the evaluation of different NPIs varying in timing and severity.\n**D)** NPIs are ineffective in reducing the spread of COVID-19 due to their limited ability to reduce mobility, but the model provides a framework to assess the viability of different NPIs.\n\n**Correct Answer:** B) NPIs are difficult to quantify due to the complexity of social interactions, but the model uses activity-driven networks to realistically describe social contacts.\n\n**Explanation:** The study highlights the difficulty in quantifying the effect of NPIs on COVID-19 spreading due to the complexity of social interactions. However, the proposed meta-population model addresses this limitation by using activity-driven networks to realistically describe social contacts, allowing for a more accurate evaluation of the effects of NPIs."}, "43": {"documentation": {"title": "Computational Socioeconomics", "source": "Jian Gao, Yi-Cheng Zhang, Tao Zhou", "docs_id": "1905.06166", "section": ["physics.soc-ph", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Computational Socioeconomics. Uncovering the structure of socioeconomic systems and timely estimation of socioeconomic status are significant for economic development. The understanding of socioeconomic processes provides foundations to quantify global economic development, to map regional industrial structure, and to infer individual socioeconomic status. In this review, we will make a brief manifesto about a new interdisciplinary research field named Computational Socioeconomics, followed by detailed introduction about data resources, computational tools, data-driven methods, theoretical models and novel applications at multiple resolutions, including the quantification of global economic inequality and complexity, the map of regional industrial structure and urban perception, the estimation of individual socioeconomic status and demographic, and the real-time monitoring of emergent events. This review, together with pioneering works we have highlighted, will draw increasing interdisciplinary attentions and induce a methodological shift in future socioeconomic studies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the primary goals of the new interdisciplinary research field of Computational Socioeconomics, as outlined in the review article?\n\nA) To develop new economic models and theories\nB) To quantify global economic development, map regional industrial structure, and estimate individual socioeconomic status\nC) To analyze the impact of technological advancements on socioeconomic systems\nD) To study the effects of climate change on economic inequality\n\n**Correct Answer:** B) To quantify global economic development, map regional industrial structure, and estimate individual socioeconomic status\n\n**Explanation:** The review article outlines the goals of Computational Socioeconomics as including the quantification of global economic inequality and complexity, the mapping of regional industrial structure and urban perception, the estimation of individual socioeconomic status and demographic, and the real-time monitoring of emergent events. Option B accurately captures these primary goals. Options A, C, and D are related to socioeconomic topics, but they are not the primary focus of Computational Socioeconomics as outlined in the review article."}, "44": {"documentation": {"title": "A quasi steady-state measurement of exciton diffusion lengths in organic\n  semiconductors", "source": "Drew B. Riley, Oskar J. Sandberg, Wei Li, Paul Meredith, and Ardalan\n  Armin", "docs_id": "2109.00839", "section": ["physics.app-ph", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A quasi steady-state measurement of exciton diffusion lengths in organic\n  semiconductors. Understanding the role that exciton diffusion plays in organic solar cells is a crucial to understanding the recent rise in power conversion effciencies brought about by non-fullerene acceptors (NFA). Established methods for measuring exciton diffusion lengths in organic solar cells require specialized equipment designed for measuring high-resolution time-resolved photoluminescence (TRPL). Here we introduce a technique, coined pulsed-PLQY, to measure the diffusion length of organic solar cells without any temporal measurements. Using a Monte-Carlo model we simulate the dynamics within a thin film semiconductor and analyse the results using both pulsed-PLQY and TRPL methods. We find that pulsed-PLQY has a larger operational region and depends less on the excitation fuence than the TRPL approach. We validate these simulated results by preforming both measurements on organic thin films and reproduce the predicted trends. Pulsed-PLQY is then used to evaluate the diffusion length in a variety of technologically relevant organic semiconductors. It is found that the diffusion lengths in NFA's are much larger than in the benchmark fullerene and that this increase is driven by an increase in diffusivity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the pulsed-PLQY technique over the traditional TRPL method for measuring exciton diffusion lengths in organic solar cells?\n\nA) Pulsed-PLQY requires less specialized equipment\nB) Pulsed-PLQY has a larger operational region and depends less on the excitation fluence\nC) Pulsed-PLQY is more suitable for measuring thin film semiconductors\nD) Pulsed-PLQY provides more accurate results in high-temperature conditions\n\nCorrect Answer: B) Pulsed-PLQY has a larger operational region and depends less on the excitation fluence\n\nExplanation: The question requires the test-taker to understand the key benefits of the pulsed-PLQY technique, which is highlighted in the provided documentation as having a larger operational region and being less dependent on the excitation fluence. This is a critical aspect of the technique, as it allows for more flexibility and accuracy in measuring exciton diffusion lengths. The incorrect options (A, C, and D) are plausible but incorrect, requiring the test-taker to carefully evaluate the information and choose the correct answer."}, "45": {"documentation": {"title": "Wrinkles as a relaxation of compressive stresses in an annular thin film", "source": "Peter Bella and Robert V. Kohn", "docs_id": "1202.3160", "section": ["math-ph", "cond-mat.mtrl-sci", "math.AP", "math.MP", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Wrinkles as a relaxation of compressive stresses in an annular thin film. It is well known that an elastic sheet loaded in tension will wrinkle and that the length scale of the wrinkles tends to zero with vanishing thickness of the sheet [Cerda and Mahadevan, Phys. Rev. Lett. 90, 074302 (2003)]. We give the first mathematically rigorous analysis of such a problem. Since our methods require an explicit understanding of the underlying (convex) relaxed problem, we focus on the wrinkling of an annular sheet loaded in the radial direction [Davidovitch et al., PNAS 108 (2011), no. 45]. Our main achievement is identification of the scaling law of the minimum energy as the thickness of the sheet tends to zero. This requires proving an upper bound and a lower bound that scale the same way. We prove both bounds first in a simplified Kirchhoff-Love setting and then in the nonlinear three-dimensional setting. To obtain the optimal upper bound, we need to adjust a naive construction (one family of wrinkles superimposed on a planar deformation) by introducing a cascade of wrinkles. The lower bound is more subtle, since it must be ansatz-free."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the wrinkle formation in an annular thin film, what is the scaling law of the minimum energy as the thickness of the sheet tends to zero, and how do the upper and lower bounds for this energy scale?\n\n**A)** The minimum energy scales as the square of the thickness, with both upper and lower bounds scaling in the same way.\n\n**B)** The minimum energy scales as the cube of the thickness, with the upper bound scaling faster than the lower bound.\n\n**C)** The minimum energy scales as the square root of the thickness, with the upper bound scaling faster than the lower bound due to the introduction of a cascade of wrinkles.\n\n**D)** The minimum energy scales as the inverse of the thickness, with both upper and lower bounds scaling in the same way.\n\n**Correct Answer:** C) The minimum energy scales as the square root of the thickness, with the upper bound scaling faster than the lower bound due to the introduction of a cascade of wrinkles.\n\n**Explanation:** The correct answer is based on the fact that the problem requires proving both upper and lower bounds that scale the same way. The introduction of a cascade of wrinkles in the upper bound is necessary to obtain the optimal scaling law, which is the square root of the thickness. The lower bound is more subtle and requires an ansatz-free approach, which is not explicitly mentioned in the provided documentation. However, based on the context, it can be inferred that the lower bound scales in the same way as the upper bound, but with a different scaling factor."}, "46": {"documentation": {"title": "Modeling impurity concentrations in liquid argon detectors", "source": "Aiwu Zhang, Yichen Li, Craig Thorn, Carl Bromberg, Milind V. Diwan,\n  Steve Kettell, Vittorio Paolone, Xin Qian, James Stewart, Wei Tang, Chao\n  Zhang", "docs_id": "2009.10906", "section": ["physics.ins-det", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modeling impurity concentrations in liquid argon detectors. Impurities in noble liquid detectors used for neutrino and dark matter experiments can significantly impact the quality of data. We present an experimentally verified model for describing the dynamics of impurity distributions in liquid argon (LAr) detectors. The model considers sources, sinks, and transport of impurities within and between the gas and liquid argon phases. Measurements of oxygen concentrations in a 20-L LAr multi-purpose test stand are compared to calculations made with this model to show that an accurate description of the concentrations under various operational conditions can be obtained. A result of this analysis is a determination of Henry's coefficient for oxygen in LAr. These calculations also show that some processes have small effects on the impurity dynamics and excluding them yields a solution as a sum of two exponential terms. This solution provides a simple way to extract Henry's coefficient with negligible approximation error. It is applied to the data and the Henry's coefficient for oxygen in LAr is obtained as 0.84$^{+0.09}_{-0.05}$, consistent with literature results. Based on the analysis of the data with the model, we further suggest that, for a large liquid argon detector, barriers to flow (\"baffles\") installed in the gas phase to restrict flow can help reduce the ultimate impurity concentration in the LAr."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A liquid argon detector is used to measure neutrino and dark matter interactions. The detector is filled with a 20-L volume of liquid argon, and the oxygen concentration is measured to be 0.5 ppm. The detector is operated at a temperature of 90 K and a pressure of 1 atm. Based on the model presented in the paper, what is the most likely value of Henry's coefficient for oxygen in liquid argon?\n\nA) 0.8\nB) 0.84\nC) 1.0\nD) 1.2\n\n**Correct Answer:** B) 0.84\n\n**Explanation:** The correct answer is based on the analysis presented in the paper, which shows that the Henry's coefficient for oxygen in liquid argon is obtained as 0.84 \u00b1 0.09. This value is consistent with literature results. The other options are incorrect because they do not match the value obtained from the analysis.\n\n**Additional Question:** The detector is operated for a long time, and the oxygen concentration increases to 1.0 ppm. Based on the model, what is the most likely explanation for this increase in oxygen concentration?\n\nA) The detector is not operating at the optimal temperature and pressure.\nB) The detector is experiencing a leak in the gas phase.\nC) The impurities are being transported from the gas phase to the liquid phase through the \"baffles\" installed in the gas phase.\nD) The detector is not being properly maintained.\n\n**Correct Answer:** C) The impurities are being transported from the gas phase to the liquid phase through the \"baffles\" installed in the gas phase.\n\n**Explanation:** The correct answer is based on the analysis presented in the paper, which shows that excluding some processes yields a solution as a sum of two exponential terms. This solution provides a simple way to extract Henry's coefficient with negligible approximation error. The paper also suggests that barriers to flow (\"baffles\") installed in the gas phase can help reduce the ultimate impurity concentration in the LAr. Therefore, the most likely explanation for the increase in oxygen concentration is the transport of impurities from the gas phase to the liquid phase through the \"baffles\"."}, "47": {"documentation": {"title": "Quasi-one-dimensional Bose-Einstein condensates in nonlinear lattices", "source": "L. Salasnich and B. A. Malomed", "docs_id": "1201.4578", "section": ["cond-mat.quant-gas", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quasi-one-dimensional Bose-Einstein condensates in nonlinear lattices. We consider the three-dimensional (3D) mean-field model for the Bose-Einstein condensate (BEC), with a 1D nonlinear lattice (NL), which periodically changes the sign of the nonlinearity along the axial direction, and the harmonic-oscillator trapping potential applied in the transverse plane. The lattice can be created as an optical or magnetic one, by means of available experimental techniques. The objective is to identify stable 3D solitons supported by the setting. Two methods are developed for this purpose: The variational approximation, formulated in the framework of the 3D Gross-Pitaevskii equation, and the 1D nonpolynomial Schr\\\"{o}dinger equation (NPSE) in the axial direction, which allows one to predict the collapse in the framework of the 1D description. Results are summarized in the form of a stability region for the solitons in the plane of the NL strength and wavenumber. Both methods produce a similar form of the stability region. Unlike their counterparts supported by the NL in the 1D model with the cubic nonlinearity, kicked solitons of the NPSE cannot be set in motion, but the kick may help to stabilize them against the collapse, by causing the solitons to shed excess norm. A dynamical effect specific to the NL is found in the form of freely propagating small-amplitude wave packets emitted by perturbed solitons."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of quasi-one-dimensional Bose-Einstein condensates in nonlinear lattices, what is the primary effect of the kick in the 1D nonpolynomial Schr\u00f6dinger equation (NPSE) on the stability of the solitons?\n\nA) The kick causes the solitons to collapse immediately.\nB) The kick helps to stabilize the solitons against collapse by causing them to shed excess norm.\nC) The kick has no effect on the stability of the solitons.\nD) The kick causes the solitons to become more unstable and prone to collapse.\n\n**Correct Answer:** B) The kick helps to stabilize the solitons against collapse by causing them to shed excess norm.\n\n**Explanation:** The correct answer is based on the text, which states that \"the kick may help to stabilize them against the collapse, by causing the solitons to shed excess norm.\" This indicates that the kick has a stabilizing effect on the solitons, which is a key concept in the study of quasi-one-dimensional Bose-Einstein condensates in nonlinear lattices.\n\n**Candidate A:** This option is incorrect because the text does not suggest that the kick causes the solitons to collapse immediately. In fact, the text states that the kick helps to stabilize the solitons.\n\n**Candidate B:** This option is correct, as it accurately reflects the text's statement about the effect of the kick on the stability of the solitons.\n\n**Candidate C:** This option is incorrect because the text does suggest that the kick has an effect on the stability of the solitons, specifically that it helps to stabilize them.\n\n**Candidate D:** This option is incorrect because the text states that the kick helps to stabilize the solitons, not that it causes them to become more unstable."}, "48": {"documentation": {"title": "Activity of the first interstellar comet 2I/Borisov around perihelion:\n  Results from Indian observatories", "source": "Aravind Krishnakumar (1 and 2), Shashikiran Ganesh (1), Kumar\n  Venkataramani (3), Devendra Sahu (4), Dorje Angchuk (4), Thirupathi Sivarani\n  (4), Athira Unni (4) ((1) Physical Research Laboratory, Ahmedabad, India,(2)\n  Institute of Technology Gandhinagar, Gandhinagar, India,(3) Auburn\n  University, Auburn, USA,(4) Indian Institute of Astrophysics, Bangalore,\n  India)", "docs_id": "2101.02752", "section": ["astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Activity of the first interstellar comet 2I/Borisov around perihelion:\n  Results from Indian observatories. Comet 2I/Borisov is the first true interstellar comet discovered. Here we present results from observational programs at two Indian observatories, 2 m Himalayan Chandra Telescope at the Indian Astronomical Observatory, Hanle (HCT) and 1.2 m telescope at the Mount Abu Infrared Observatory (MIRO). Two epochs of imaging and spectroscopy were carried out at the HCT and three epochs of imaging at MIRO. We found CN to be the dominant molecular emission on both epochs, 31/11/2019 and 22/12/2019, at distances of r$_H$ = 2.013 and 2.031 AU respectively. The comet was inferred to be relatively depleted in Carbon bearing molecules on the basis of low $C_2$ and $C_3$ abundances. We find the production rate ratio, Q($C_2$)/Q(CN) = 0.54 $\\pm$ 0.18, pre-perihelion and Q($C_2$)/Q(CN) = 0.34 $\\pm$ 0.12 post-perihelion. This classifies the comet as being moderately depleted in carbon chain molecules. Using the results from spectroscopic observations, we believe the comet to have a chemically heterogeneous surface having variation in abundance of carbon chain molecules. From imaging observations we infer a dust-to-gas ratio similar to carbon chain depleted comets of the Solar system. We also compute the nucleus size to be in the range $0.18\\leq r \\leq 3.1$ Km. Our observations show that 2I/Borisov's behaviour is analogous to that of the Solar system comets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the chemical composition of comet 2I/Borisov, specifically its molecular emissions and abundance of carbon chain molecules. How does the comet's behavior compare to that of Solar system comets?\n\n**A)** The comet's dominant molecular emission is CO, indicating a high abundance of carbon dioxide. The low abundance of carbon chain molecules suggests a highly depleted surface.\n\n**B)** The comet's dominant molecular emission is CN, indicating a moderate abundance of carbon-bearing molecules. The production rate ratio of C2 to CN suggests a chemically heterogeneous surface with varying abundance of carbon chain molecules.\n\n**C)** The comet's dominant molecular emission is C2, indicating a highly depleted surface. The high abundance of carbon chain molecules suggests a highly active comet with a large nucleus.\n\n**D)** The comet's dominant molecular emission is CH3OH, indicating a high abundance of water ice. The low abundance of carbon chain molecules suggests a highly depleted surface.\n\n**Correct Answer:** B) The comet's dominant molecular emission is CN, indicating a moderate abundance of carbon-bearing molecules. The production rate ratio of C2 to CN suggests a chemically heterogeneous surface with varying abundance of carbon chain molecules.\n\n**Explanation:** The correct answer is B) because the documentation states that CN is the dominant molecular emission on both epochs, indicating a moderate abundance of carbon-bearing molecules. Additionally, the production rate ratio of C2 to CN suggests a chemically heterogeneous surface with varying abundance of carbon chain molecules, which is consistent with the comet's behavior being analogous to that of Solar system comets."}, "49": {"documentation": {"title": "Influence of Electron-Phonon Interaction on Spin Fluctuation Induced\n  Superconductivity", "source": "T. S. Nunner, J. Schmalian, and K. H. Bennemann", "docs_id": "cond-mat/9804088", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Influence of Electron-Phonon Interaction on Spin Fluctuation Induced\n  Superconductivity. We investigate the interplay of the electron-phonon and the spin fluctuation interaction for the superconducting state of YBa$_2$Cu$_3$O$_{7}$. The spin fluctuations are described within the nearly antiferromagnetic Fermi liquid theory, whereas the phonons are treated using a shell model calculation of all phonon branches. The electron-phonon coupling is calculated using rigidly displaced ionic potentials screened by a background dielectric constant $\\epsilon_\\infty$ and by holes within the CuO$_2$ planes. Taking into account both interactions we get a superconducting state with $d_{x^2-y^2}$-symmetry, whose origin are antiferromagnetic spin fluctuations. The investigation of all phonon modes of the system shows that the phononic contribution to the d-wave pairing interaction is attractive. This is a necessary prerequisite for a positive isotope effect. The size of the isotope exponent depends strongly on the relative strength of the electron-phonon and spin fluctuation coupling. Due to the strong electronic correlations no phononic induced superconducting state, which is always of s-wave character, is possible."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism responsible for the superconducting state of YBa$_2$Cu$_3$O$_{7}$, as investigated in the study \"Influence of Electron-Phonon Interaction on Spin Fluctuation Induced Superconductivity\"?\n\nA){The electron-phonon interaction is the primary mechanism, as it leads to an attractive phononic contribution to the d-wave pairing interaction.}\nB){The spin fluctuation interaction is the primary mechanism, as it gives rise to antiferromagnetic spin fluctuations that drive the superconducting state.}\nC){The electron-phonon interaction and spin fluctuation interaction are both necessary, but the electron-phonon interaction is the primary mechanism, as it provides an attractive phononic contribution to the pairing interaction.}\nD){The study suggests that the superconducting state of YBa$_2$Cu$_3$O$_{7}$ is due to the absence of phononic induced superconducting states, which are always of s-wave character.}\n\n**Correct Answer:** C){The electron-phonon interaction and spin fluctuation interaction are both necessary, but the electron-phonon interaction is the primary mechanism, as it provides an attractive phononic contribution to the pairing interaction.}\n\n**Explanation:** The study investigates the interplay between the electron-phonon interaction and spin fluctuation interaction in YBa$_2$Cu$_3$O$_{7}$. While both interactions are important, the electron-phonon interaction is found to be the primary mechanism responsible for the superconducting state. This is because the electron-phonon interaction provides an attractive phononic contribution to the d-wave pairing interaction, which is necessary for the superconducting state to occur. The spin fluctuation interaction, on the other hand, gives rise to antiferromagnetic spin fluctuations that drive the superconducting state, but it is not the primary mechanism."}, "50": {"documentation": {"title": "Semi-abelian analogues of Schanuel Conjecture and applications", "source": "Cristiana Bertolin, Patrice Philippon, Biswajyoti Saha, Ekata Saha", "docs_id": "2010.15170", "section": ["math.NT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semi-abelian analogues of Schanuel Conjecture and applications. In this article we study Semi-abelian analogues of Schanuel conjecture. As showed by the first author, Schanuel Conjecture is equivalent to the Generalized Period Conjecture applied to 1-motives without abelian part. Extending her methods, the second, the third and the fourth authors have introduced the Abelian analogue of Schanuel Conjecture as the Generalized Period Conjecture applied to 1-motives without toric part. As a first result of this paper, we define the Semi-abelian analogue of Schanuel Conjecture as the Generalized Period Conjecture applied to 1-motives. C. Cheng et al. proved that Schanuel conjecture implies the algebraic independence of the values of the iterated exponential and the values of the iterated logarithm, answering a question of M. Waldschmidt. The second, the third and the fourth authors have investigated a similar question in the setup of abelian varieties: the Weak Abelian Schanuel conjecture implies the algebraic independence of the values of the iterated abelian exponential and the values of an iterated generalized abelian logarithm. The main result of this paper is that a Relative Semi-abelian conjecture implies the algebraic independence of the values of the iterated semi-abelian exponential and the values of an iterated generalized semi-abelian logarithm."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let $K$ be a number field of degree $n$ over $\\mathbb{Q}$, and let $L$ be a finite extension of $K$ of degree $m$. Suppose that $L$ is a 1-motive, and let $\\phi: K \\to \\mathbb{C}$ be a non-trivial algebraic map. Define the semi-abelian exponential and logarithm functions as follows:\n\n* The semi-abelian exponential function is defined as $\\exp_{sa}(\\phi) = \\exp(\\phi) \\cdot \\exp_{ab}(\\phi)$, where $\\exp_{ab}(\\phi)$ is the abelian exponential function.\n* The semi-abelian logarithm function is defined as $\\log_{sa}(\\phi) = \\log(\\phi) \\cdot \\log_{ab}(\\phi)$, where $\\log_{ab}(\\phi)$ is the abelian logarithm function.\n\nProve that if the Relative Semi-abelian Conjecture holds for $K$ and $L$, then the values of the iterated semi-abelian exponential and the values of the iterated generalized semi-abelian logarithm are algebraically independent over $K$.\n\nA) The values of the iterated semi-abelian exponential are algebraically independent over $K$ if and only if the values of the iterated abelian exponential are algebraically independent over $K$.\n\nB) The values of the iterated generalized semi-abelian logarithm are algebraically independent over $K$ if and only if the values of the iterated generalized abelian logarithm are algebraically independent over $K$.\n\nC) The values of the iterated semi-abelian exponential and the values of the iterated generalized semi-abelian logarithm are algebraically independent over $K$ if and only if the values of the iterated semi-abelian exponential and the values of the iterated generalized abelian logarithm are algebraically independent over $K$.\n\nD) The values of the iterated semi-abelian exponential and the values of the iterated generalized semi-abelian logarithm are algebraically independent over $K$ if and only if the values of the iterated semi-abelian exponential and the values of the iterated generalized abelian logarithm are algebraically independent over $K$, and the values of the iterated abelian exponential and the values of the iterated generalized abelian logarithm are algebraically independent over $K$.\n\nCorrect Answer: D"}, "51": {"documentation": {"title": "\"Quantum Equilibrium-Disequilibrium\": Asset Price Dynamics, Symmetry\n  Breaking, and Defaults as Dissipative Instantons", "source": "Igor Halperin and Matthew Dixon", "docs_id": "1808.03607", "section": ["q-fin.ST", "q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "\"Quantum Equilibrium-Disequilibrium\": Asset Price Dynamics, Symmetry\n  Breaking, and Defaults as Dissipative Instantons. We propose a simple non-equilibrium model of a financial market as an open system with a possible exchange of money with an outside world and market frictions (trade impacts) incorporated into asset price dynamics via a feedback mechanism. Using a linear market impact model, this produces a non-linear two-parametric extension of the classical Geometric Brownian Motion (GBM) model, that we call the \"Quantum Equilibrium-Disequilibrium\" (QED) model. The QED model gives rise to non-linear mean-reverting dynamics, broken scale invariance, and corporate defaults. In the simplest one-stock (1D) formulation, our parsimonious model has only one degree of freedom, yet calibrates to both equity returns and credit default swap spreads. Defaults and market crashes are associated with dissipative tunneling events, and correspond to instanton (saddle-point) solutions of the model. When market frictions and inflows/outflows of money are neglected altogether, \"classical\" GBM scale-invariant dynamics with an exponential asset growth and without defaults are formally recovered from the QED dynamics. However, we argue that this is only a formal mathematical limit, and in reality the GBM limit is non-analytic due to non-linear effects that produce both defaults and divergence of perturbation theory in a small market friction parameter."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of the \"Quantum Equilibrium-Disequilibrium\" (QED) model, what is the relationship between the dissipative tunneling events (corresponding to instanton solutions) and the market crashes, and how do these events relate to the scale-invariance of the model?\n\n**A)** The dissipative tunneling events are the primary drivers of market crashes, and the QED model's non-linear dynamics are responsible for the breakdown of scale-invariance.\n\n**B)** The dissipative tunneling events are a consequence of the market frictions and inflows/outflows of money, and the QED model's non-linear dynamics are not directly related to the scale-invariance of the model.\n\n**C)** The dissipative tunneling events are associated with the \"classical\" GBM limit, where market frictions and inflows/outflows of money are neglected, and the QED model's non-linear dynamics are responsible for the breakdown of scale-invariance.\n\n**D)** The dissipative tunneling events are a formal mathematical limit of the QED model, where the market frictions and inflows/outflows of money are neglected, and the QED model's non-linear dynamics are not directly related to the scale-invariance of the model.\n\n**Correct Answer:** A) The dissipative tunneling events are the primary drivers of market crashes, and the QED model's non-linear dynamics are responsible for the breakdown of scale-invariance.\n\n**Explanation:** The QED model introduces dissipative tunneling events, which correspond to instanton solutions of the model. These events are associated with market crashes, and the non-linear dynamics of the QED model are responsible for the breakdown of scale-invariance. The correct answer, A, highlights the direct relationship between dissipative tunneling events and market crashes, as well as the role of non-linear dynamics in breaking scale-invariance. The other options are incorrect because they either misrepresent the relationship between dissipative tunneling events and market crashes, or incorrectly attribute the breakdown of scale-invariance to the \"classical\" GBM limit."}, "52": {"documentation": {"title": "Analise Demografica e Socioeconomica do Uso e do Acesso a Medicamentos\n  Antidepressivos no Brasil", "source": "Karinna Moura Boaviagem and Jos\\'e Ricardo Bezerra Nogueira", "docs_id": "2111.15618", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analise Demografica e Socioeconomica do Uso e do Acesso a Medicamentos\n  Antidepressivos no Brasil. Depressive disorders, in addition to causing direct negative impacts on health, are also responsible for imposing substantial costs on society. In relation to the treatment of depression, antidepressants have proven effective, and, to the World Health Organization, access to psychotropic drugs for people with mental illnesses offers a chance of improved health and an opportunity for reengagement in society. The aim of this study is to analyze the use of and access to antidepressants in Brazil, according to macro-regions and to demographic, social and economic conditions of the population, using the National Survey on Access, Use and Promotion of Rational Use of Medicines (PNAUM 2013/2014). The results show that there is a high prevalence of antidepressant use in individuals with depression in Brazil. The main profile of use of these drugs is: female individuals, between 20 and 59 years old, white, from the Southeast region, of the economic class D/E, with a high schooling level, in a marital situation, without health insurance coverage, without limitations derived from depression, and who self-evaluated health as regular."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the relationship between demographic, social, and economic conditions and the use of antidepressants in Brazil, as revealed by the PNAUM 2013/2014 survey. How do the characteristics of individuals with depression in Brazil, such as age, gender, ethnicity, socioeconomic status, marital status, and health insurance coverage, influence their access to and use of antidepressants?\n\n**A)** The use of antidepressants in Brazil is influenced by the socioeconomic status of individuals, with those from lower socioeconomic classes being more likely to use these medications. However, the survey also reveals that individuals with higher schooling levels and those without health insurance coverage are more likely to use antidepressants.\n\n**B)** The demographic characteristics of individuals with depression in Brazil, such as age and ethnicity, play a significant role in determining their access to and use of antidepressants. Specifically, the survey shows that individuals between 20 and 59 years old, who are white, are more likely to use antidepressants.\n\n**C)** The survey suggests that the use of antidepressants in Brazil is influenced by the marital status of individuals, with those who are married being more likely to use these medications. Additionally, the survey reveals that individuals with higher schooling levels and those without health insurance coverage are more likely to use antidepressants.\n\n**D)** The PNAUM 2013/2014 survey reveals that the use of antidepressants in Brazil is influenced by the economic class of individuals, with those from the economic classes D/E being more likely to use these medications. However, the survey also shows that individuals with higher schooling levels and those without health insurance coverage are more likely to use antidepressants.\n\n**Correct Answer:** A) The use of antidepressants in Brazil is influenced by the socioeconomic status of individuals, with those from lower socioeconomic classes being more likely to use these medications. However, the survey also reveals that individuals with higher schooling levels and those without health insurance coverage are more likely to use antidepressants.\n\n**Explanation:** The correct answer is A) because the survey reveals that the use of antidepressants in Brazil is influenced by the socioeconomic status of individuals, with those from lower socioeconomic classes being more likely to use these medications. However, the survey also shows that individuals with higher schooling levels and those without health insurance coverage are more likely to use antidepressants, which is consistent with the main profile of use of antidepressants in Brazil."}, "53": {"documentation": {"title": "Contrastive Hebbian Learning with Random Feedback Weights", "source": "Georgios Detorakis, Travis Bartley, Emre Neftci", "docs_id": "1806.07406", "section": ["cs.LG", "q-bio.NC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Contrastive Hebbian Learning with Random Feedback Weights. Neural networks are commonly trained to make predictions through learning algorithms. Contrastive Hebbian learning, which is a powerful rule inspired by gradient backpropagation, is based on Hebb's rule and the contrastive divergence algorithm. It operates in two phases, the forward (or free) phase, where the data are fed to the network, and a backward (or clamped) phase, where the target signals are clamped to the output layer of the network and the feedback signals are transformed through the transpose synaptic weight matrices. This implies symmetries at the synaptic level, for which there is no evidence in the brain. In this work, we propose a new variant of the algorithm, called random contrastive Hebbian learning, which does not rely on any synaptic weights symmetries. Instead, it uses random matrices to transform the feedback signals during the clamped phase, and the neural dynamics are described by first order non-linear differential equations. The algorithm is experimentally verified by solving a Boolean logic task, classification tasks (handwritten digits and letters), and an autoencoding task. This article also shows how the parameters affect learning, especially the random matrices. We use the pseudospectra analysis to investigate further how random matrices impact the learning process. Finally, we discuss the biological plausibility of the proposed algorithm, and how it can give rise to better computational models for learning."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Contrastive Hebbian Learning with Random Feedback Weights proposes a novel variant of the algorithm that does not rely on synaptic weights symmetries. What is the primary difference between this proposed algorithm and traditional contrastive Hebbian learning, and how does it impact the learning process?\n\n**A)** The proposed algorithm uses a different optimization objective, whereas traditional contrastive Hebbian learning relies on gradient backpropagation. This difference leads to a more efficient learning process.\n\n**B)** The proposed algorithm uses random matrices to transform feedback signals during the clamped phase, whereas traditional contrastive Hebbian learning relies on synaptic weights. This difference eliminates the need for synaptic weights symmetries.\n\n**C)** The proposed algorithm is limited to binary classification tasks, whereas traditional contrastive Hebbian learning can be applied to a wide range of tasks. The random matrices used in the proposed algorithm do not affect the classification performance.\n\n**D)** The proposed algorithm is more computationally expensive than traditional contrastive Hebbian learning due to the need to solve first-order non-linear differential equations. This increased computational cost leads to slower learning rates.\n\n**Correct Answer:** B) The proposed algorithm uses random matrices to transform feedback signals during the clamped phase, whereas traditional contrastive Hebbian learning relies on synaptic weights. This difference eliminates the need for synaptic weights symmetries.\n\n**Explanation:** The correct answer, B, highlights the key difference between the proposed algorithm and traditional contrastive Hebbian learning. The use of random matrices to transform feedback signals during the clamped phase eliminates the need for synaptic weights symmetries, which is a major advantage of the proposed algorithm. This difference has significant implications for the learning process, as it allows for more flexibility and adaptability in the neural network."}, "54": {"documentation": {"title": "Identifiability and Estimation of Possibly Non-Invertible SVARMA Models:\n  A New Parametrisation", "source": "Bernd Funovits", "docs_id": "2002.04346", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Identifiability and Estimation of Possibly Non-Invertible SVARMA Models:\n  A New Parametrisation. This article deals with parameterisation, identifiability, and maximum likelihood (ML) estimation of possibly non-invertible structural vector autoregressive moving average (SVARMA) models driven by independent and non-Gaussian shocks. In contrast to previous literature, the novel representation of the MA polynomial matrix using the Wiener-Hopf factorisation (WHF) focuses on the multivariate nature of the model, generates insights into its structure, and uses this structure for devising optimisation algorithms. In particular, it allows to parameterise the location of determinantal zeros inside and outside the unit circle, and it allows for MA zeros at zero, which can be interpreted as informational delays. This is highly relevant for data-driven evaluation of Dynamic Stochastic General Equilibrium (DSGE) models. Typically imposed identifying restrictions on the shock transmission matrix as well as on the determinantal root location are made testable. Furthermore, we provide low level conditions for asymptotic normality of the ML estimator and analytic expressions for the score and the information matrix. As application, we estimate the Blanchard and Quah model and show that our method provides further insights regarding non-invertibility using a standard macroeconometric model. These and further analyses are implemented in a well documented R-package."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a possibly non-invertible SVARMA model driven by independent and non-Gaussian shocks, where the MA polynomial matrix is represented using the Wiener-Hopf factorisation (WHF). Suppose we want to estimate the model using maximum likelihood (ML) estimation, and we are given the following information:\n\n* The shock transmission matrix has a non-zero determinant at the origin.\n* The determinantal zeros of the MA polynomial matrix are located at the origin and on the unit circle.\n* The model is estimated using a standard macroeconometric model, and the results are implemented in a well-documented R-package.\n\nWhich of the following statements is true about the ML estimator of the SVARMA model?\n\nA) The ML estimator is asymptotically normal, but its variance is unbounded.\nB) The ML estimator is asymptotically normal, and its variance is bounded by a constant.\nC) The ML estimator is not asymptotically normal, but its bias is zero.\nD) The ML estimator is not asymptotically normal, and its bias is non-zero.\n\n**Correct answer:** B) The ML estimator is asymptotically normal, and its variance is bounded by a constant.\n\n**Explanation:**\n\nThe article provides low-level conditions for asymptotic normality of the ML estimator, which states that the estimator is asymptotically normal if certain conditions are met. One of these conditions is that the variance of the estimator is bounded by a constant. This is because the Wiener-Hopf factorisation (WHF) representation of the MA polynomial matrix allows for the estimation of the model's parameters using a standard macroeconometric model, which provides a well-defined and bounded variance for the ML estimator.\n\nThe other options are incorrect because:\n\n* A) The ML estimator is asymptotically normal, but its variance is unbounded, is not supported by the article, which provides conditions for asymptotic normality with bounded variance.\n* C) The ML estimator is not asymptotically normal, but its bias is zero, is not supported by the article, which provides conditions for asymptotic normality with bounded variance.\n* D) The ML estimator is not asymptotically normal, and its bias is non-zero, is not supported by the article, which provides conditions for asymptotic normality with bounded variance."}, "55": {"documentation": {"title": "The M Dwarf Problem in the Galaxy", "source": "Vincent M. Woolf and Andrew A. West", "docs_id": "1202.3078", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The M Dwarf Problem in the Galaxy. We present evidence that there is an M dwarf problem similar to the previously identified G dwarf and K dwarf problems: the number of low-metallicity M dwarfs is not sufficient to match simple closed-box models of local Galactic chemical evolution. We estimated the metallicity of 4141 M dwarf stars with spectra from the Sloan Digital Sky Survey (SDSS) using a molecular band strength versus metallicity calibration developed using high resolution spectra of nearby M dwarfs. Using a sample of M dwarfs with measured magnitudes, parallaxes, and metallicities, we derived a relation that describes the absolute magnitude variation as a function of metallicity. When we examined the metallicity distribution of SDSS stars, after correcting for the different volumes sampled by the magnitude-limited survey, we found that there is an M dwarf problem, with the number of M dwarfs at [Fe/H] ~ -0.5 less than 1% the number at [Fe/H] = 0, where a simple model of Galactic chemical evolution predicts a more gradual drop in star numbers with decreasing metallicity."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The authors of the study \"The M Dwarf Problem in the Galaxy\" found that the number of low-metallicity M dwarfs is not sufficient to match simple closed-box models of local Galactic chemical evolution. What is the approximate metallicity range where the number of M dwarfs is less than 1% of the number at [Fe/H] = 0?\n\nA) [Fe/H] = -0.8 to -1.0\nB) [Fe/H] = -0.5 to -0.7\nC) [Fe/H] = -1.2 to -1.5\nD) [Fe/H] = -0.2 to -0.3\n\n**Correct Answer:** B) [Fe/H] = -0.5 to -0.7\n\n**Explanation:** According to the text, the authors found that the number of M dwarfs at [Fe/H] ~ -0.5 is less than 1% the number at [Fe/H] = 0. This corresponds to the metallicity range [Fe/H] = -0.5 to -0.7."}, "56": {"documentation": {"title": "Experimental study of three-wave interactions among capillary-gravity\n  surface waves", "source": "Florence Haudin (MSC), Annette Cazaubiel (MSC), Luc Deike, Timoth\\'ee\n  Jamin (MSC), Eric Falcon (MSC), Michael Berhanu (MSC)", "docs_id": "1603.02654", "section": ["physics.flu-dyn", "nlin.CD", "physics.class-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Experimental study of three-wave interactions among capillary-gravity\n  surface waves. In propagating wave systems, three or four-wave resonant interactions constitute a classical non-linear mechanism exchanging energy between the different scales. Here we investigate three-wave interactions for gravity-capillary surface waves in a closed laboratory tank. We generate two crossing wave-trains and we study their interaction. Using two optical methods, a local one (Laser Doppler Vibrometry) and a spatio-temporal one (Diffusive Light Photography), a third wave of smaller amplitude is detected, verifying the three-wave resonance conditions in frequency and in wavenumber. Furthermore, by focusing on the stationary regime and by taking into account viscous dissipation, we directly estimate the growth rate of the resonant mode. The latter is then compared to the predictions of the weakly non-linear triadic resonance interaction theory. The obtained results confirm qualitatively and extend previous experimental results obtained only for collinear wave-trains. Finally, we discuss the relevance of three-wave interaction mechanisms in recent experiments studying gravity-capillary turbulence."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the primary mechanism by which three-wave interactions among gravity-capillary surface waves exchange energy, and how does this mechanism relate to the growth rate of the resonant mode?\n\nA) The primary mechanism is through the exchange of energy between the different scales through a classical non-linear mechanism, and the growth rate of the resonant mode is directly proportional to the amplitude of the interacting waves.\n\nB) The primary mechanism is through the exchange of energy between the different scales through a classical non-linear mechanism, and the growth rate of the resonant mode is directly proportional to the frequency of the interacting waves.\n\nC) The primary mechanism is through the exchange of energy between the different scales through a classical non-linear mechanism, and the growth rate of the resonant mode is directly proportional to the wavenumber of the interacting waves.\n\nD) The primary mechanism is through the exchange of energy between the different scales through a classical non-linear mechanism, and the growth rate of the resonant mode is directly proportional to the viscous dissipation of the interacting waves.\n\nCorrect Answer: C) The primary mechanism is through the exchange of energy between the different scales through a classical non-linear mechanism, and the growth rate of the resonant mode is directly proportional to the wavenumber of the interacting waves.\n\nExplanation: The correct answer is C) because the question states that the three-wave interactions constitute a classical non-linear mechanism exchanging energy between the different scales, and the growth rate of the resonant mode is directly estimated by taking into account viscous dissipation. The correct answer also states that the growth rate is directly proportional to the wavenumber of the interacting waves, which is consistent with the experimental results. The other options are incorrect because they do not accurately reflect the experimental results or the theoretical framework of the problem."}, "57": {"documentation": {"title": "Spatially distributed social complex networks", "source": "Gerald F. Frasco, Jie Sun, Hernan D. Rozenfeld, Daniel ben-Avraham", "docs_id": "1306.0257", "section": ["physics.soc-ph", "cond-mat.stat-mech", "cs.SI", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spatially distributed social complex networks. We propose a bare-bones stochastic model that takes into account both the geographical distribution of people within a country and their complex network of connections. The model, which is designed to give rise to a scale-free network of social connections and to visually resemble the geographical spread seen in satellite pictures of the Earth at night, gives rise to a power-law distribution for the ranking of cities by population size (but for the largest cities) and reflects the notion that highly connected individuals tend to live in highly populated areas. It also yields some interesting insights regarding Gibrat's law for the rates of city growth (by population size), in partial support of the findings in a recent analysis of real data [Rozenfeld et al., Proc. Natl. Acad. Sci. U.S.A. 105, 18702 (2008)]. The model produces a nontrivial relation between city population and city population density and a superlinear relationship between social connectivity and city population, both of which seem quite in line with real data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The proposed stochastic model in \"Spatially distributed social complex networks\" aims to capture the geographical distribution of people within a country and their complex network of connections. What is the primary characteristic of the power-law distribution that arises from this model, and how does it relate to the geographical spread of cities?\n\n**A)** The power-law distribution reflects the notion that highly connected individuals tend to live in highly populated areas, but it does not capture the geographical distribution of cities.\n\n**B)** The power-law distribution is a direct result of the geographical distribution of cities, and it reflects the notion that highly connected individuals tend to live in highly populated areas.\n\n**C)** The power-law distribution is a characteristic of the social network itself, and it does not provide any information about the geographical distribution of cities.\n\n**D)** The power-law distribution is a result of the interplay between the geographical distribution of cities and the social network, and it reflects the notion that highly connected individuals tend to live in highly populated areas.\n\n**Correct Answer:** B) The power-law distribution is a direct result of the geographical distribution of cities, and it reflects the notion that highly connected individuals tend to live in highly populated areas.\n\n**Explanation:** The model is designed to give rise to a scale-free network of social connections that visually resembles the geographical spread seen in satellite pictures of the Earth at night. This suggests that the power-law distribution is a direct result of the geographical distribution of cities. Additionally, the model reflects the notion that highly connected individuals tend to live in highly populated areas, which is also a key characteristic of the power-law distribution."}, "58": {"documentation": {"title": "On the Selection of Loss Severity Distributions to Model Operational\n  Risk", "source": "Daniel Hadley, Harry Joe, Natalia Nolde", "docs_id": "2107.03979", "section": ["q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Selection of Loss Severity Distributions to Model Operational\n  Risk. Accurate modeling of operational risk is important for a bank and the finance industry as a whole to prepare for potentially catastrophic losses. One approach to modeling operational is the loss distribution approach, which requires a bank to group operational losses into risk categories and select a loss frequency and severity distribution for each category. This approach estimates the annual operational loss distribution, and a bank must set aside capital, called regulatory capital, equal to the 0.999 quantile of this estimated distribution. In practice, this approach may produce unstable regulatory capital calculations from year-to-year as selected loss severity distribution families change. This paper presents truncation probability estimates for loss severity data and a consistent quantile scoring function on annual loss data as useful severity distribution selection criteria that may lead to more stable regulatory capital. Additionally, the Sinh-arcSinh distribution is another flexible candidate family for modeling loss severities that can be easily estimated using the maximum likelihood approach. Finally, we recommend that loss frequencies below the minimum reporting threshold be collected so that loss severity data can be treated as censored data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A bank is required to estimate the annual operational loss distribution to determine its regulatory capital. The bank has collected data on operational losses for the past 5 years, but the data is not normally distributed. Which of the following approaches would be most suitable for modeling the loss severity distribution, and why?\n\nA) Maximum Likelihood Estimation (MLE) using the Sinh-arcSinh distribution\nB) Quantile Scoring Function (QSF) with truncation probability estimates\nC) Empirical Distribution Function (EDF) with a uniform distribution\nD) Bayesian Estimation using a normal distribution\n\n**Correct Answer:** B) Quantile Scoring Function (QSF) with truncation probability estimates\n\n**Explanation:** The Quantile Scoring Function (QSF) with truncation probability estimates is a suitable approach for modeling the loss severity distribution because it can handle non-normal data and provides a consistent quantile scoring function. The truncation probability estimates can help to stabilize the regulatory capital calculations by accounting for the minimum reporting threshold. In contrast, MLE using the Sinh-arcSinh distribution (A) may not be suitable if the data is not normally distributed, and Bayesian Estimation using a normal distribution (D) may not be suitable if the data is not normally distributed. The Empirical Distribution Function (EDF) with a uniform distribution (C) is not a suitable approach for modeling the loss severity distribution as it does not account for the truncation probability estimates.\n\nNote: This question requires the test-taker to have a good understanding of the concepts discussed in the documentation, including the loss distribution approach, truncation probability estimates, and the Sinh-arcSinh distribution. The correct answer requires the test-taker to apply their knowledge of the subject to a specific scenario and choose the most suitable approach."}, "59": {"documentation": {"title": "Photon Assisted Tunneling of Zero Modes in a Majorana Wire", "source": "David M.T. van Zanten, Deividas Sabonis, Judith Suter, Jukka I.\n  V\\\"ayrynen, Torsten Karzig, Dmitry I. Pikulin, Eoin C. T. O'Farrell, Davydas\n  Razmadze, Karl D. Petersson, Peter Krogstrup, Charles M. Marcus", "docs_id": "1902.00797", "section": ["cond-mat.mes-hall", "cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Photon Assisted Tunneling of Zero Modes in a Majorana Wire. Hybrid nanowires with proximity-induced superconductivity in the topological regime host Majorana zero modes (MZMs) at their ends, and networks of such structures can produce topologically protected qubits. In a double-island geometry where each segment hosts a pair of MZMs, inter-pair coupling mixes the charge parity of the islands and opens an energy gap between the even and odd charge states at the inter-island charge degeneracy. Here, we report on the spectroscopic measurement of such an energy gap in an InAs/Al double-island device by tracking the position of the microwave-induced quasiparticle (qp) transitions using a radio-frequency (rf) charge sensor. In zero magnetic field, photon assisted tunneling (PAT) of Cooper pairs gives rise to resonant lines in the 2e-2e periodic charge stability diagram. In the presence of a magnetic field aligned along the nanowire, resonance lines are observed parallel to the inter-island charge degeneracy of the 1e-1e periodic charge stability diagram, where the 1e periodicity results from a zero-energy sub-gap state that emerges in magnetic field. Resonant lines in the charge stability diagram indicate coherent photon assisted tunneling of single-electron states, changing the parity of the two islands. The dependence of resonant frequency on detuning indicates a sizable (GHz-scale) hybridization of zero modes across the junction separating islands."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Photon Assisted Tunneling of Zero Modes in a Majorana Wire paper, what is the primary effect of inter-pair coupling in a double-island geometry on the energy gap between the even and odd charge states at the inter-island charge degeneracy?\n\nA) It opens a gap between the 0e and 1e periodic charge stability diagrams.\nB) It closes the energy gap between the even and odd charge states, resulting in a single 2e periodicity.\nC) It creates a zero-energy sub-gap state that emerges in the 1e periodicity, parallel to the inter-island charge degeneracy.\nD) It hybridizes the zero modes across the junction separating islands, resulting in a sizable (GHz-scale) hybridization.\n\n**Correct Answer:** D) It hybridizes the zero modes across the junction separating islands, resulting in a sizable (GHz-scale) hybridization.\n\n**Explanation:** The correct answer is D) because the paper states that the dependence of resonant frequency on detuning indicates a sizable (GHz-scale) hybridization of zero modes across the junction separating islands. This is a key finding in the paper, and it highlights the importance of inter-pair coupling in the double-island geometry.\n\n**Candidate A (Incorrect)**: This option is incorrect because the paper does not mention the 0e and 1e periodic charge stability diagrams being opened or closed by inter-pair coupling.\n\n**Candidate B (Incorrect)**: This option is incorrect because the paper states that inter-pair coupling mixes the charge parity of the islands and opens an energy gap between the even and odd charge states, not closes it.\n\n**Candidate C (Incorrect)**: This option is incorrect because the paper does not mention the creation of a zero-energy sub-gap state that emerges in the 1e periodicity, parallel to the inter-island charge degeneracy. The zero-energy sub-gap state is actually mentioned as a result of the magnetic field alignment, not inter-pair coupling."}}